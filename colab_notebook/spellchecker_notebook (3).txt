{"cells":[{"cell_type":"markdown","metadata":{"id":"fB0F6JZZ69nU"},"source":["# Neural Spellchecker for Medical and Legal Domains\n","\n","This notebook implements a neural spellchecker with a web interface (Step 1) and back-end logic (Step 2) for medical (PubMed) and legal (case files) domains. It uses BERT (BioBERT, LegalBERT) and a placeholder LSTM model, with a JSON config file for domain-specific terms.\n","\n","## Step 1: Web Interface\n","- HTML/CSS/JavaScript interface with text/file input, domain (Medical, Legal) and model (BERT, LSTM) dropdowns, and error display.\n","\n","## Step 2: Spellchecker Logic\n","- Flask back-end with BioBERT (medical), LegalBERT (legal), and placeholder LSTM.\n","- Uses `domain_config.json` for domain-specific terms.\n","- Classifies errors as general (NLTK dictionary) or domain-specific (config-based).\n","- Prioritizes corrections using semantic and syntactic patterns.\n","\n","## Setup Instructions\n","1. Run cells sequentially to install dependencies, save files, and start the Flask server.\n","2. Use the `ngrok` URL to access the web interface.\n","3. Test with sample text (e.g., 'Patient has hypertention.') or .txt files.\n","\n","**Troubleshooting**:\n","- If `connection refused` error occurs, check Flask logs in the 'Run Flask Server' cell.\n","- Ensure GPU is enabled (Edit > Notebook Settings > T4 GPU).\n","- Replace 'YOUR_NGROK_AUTH_TOKEN' with your ngrok token (from ngrok.com dashboard).\n","\n","**Note**: .docx parsing and LSTM corrections are placeholders (pending Step 3)."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MPvpmwiO69nX","executionInfo":{"status":"ok","timestamp":1750437060032,"user_tz":-330,"elapsed":104340,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"d9cf90d3-1930-45d6-a0a1-ed66cb890b2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m692.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m522.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\n","tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.16.1 which is incompatible.\n","ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n","grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n","tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.16.1 which is incompatible.\n","jax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.3.2 which is incompatible.\n","tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.16.1 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mPython 3.11.13\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"]}],"source":["# Install dependencies\n","!pip install flask==2.3.2 flask-cors==4.0.0 transformers==4.44.2 tensorflow==2.16.1 nltk==3.8.1 numpy==1.26.4 pyngrok==7.1.6 -q\n","\n","# Download NLTK data\n","import nltk\n","nltk.download('words')\n","\n","# Verify Python version\n","!python --version"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UTEYAq_j69nY","executionInfo":{"status":"ok","timestamp":1750437060085,"user_tz":-330,"elapsed":49,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"bafd31ca-d1ad-4683-d724-5b3f3addfed2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing index.html\n"]}],"source":["# Save front-end files\n","%%writefile index.html\n","<!DOCTYPE html>\n","<html lang=\"en\">\n","<head>\n","    <meta charset=\"UTF-8\">\n","    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","    <title>Neural Spellchecker</title>\n","    <link rel=\"stylesheet\" href=\"styles.css\">\n","</head>\n","<body>\n","    <div class=\"container\">\n","        <h1>Neural Spellchecker</h1>\n","        <div class=\"input-section\">\n","            <label for=\"text-input\">Enter Text:</label>\n","            <textarea id=\"text-input\" rows=\"5\" placeholder=\"Enter text to check...\"></textarea>\n","            <label for=\"file-input\">Or Upload File (.txt, .docx):</label>\n","            <input type=\"file\" id=\"file-input\" accept=\".txt,.docx\" multiple>\n","            <div class=\"options\">\n","                <label for=\"domain-select\">Domain:</label>\n","                <select id=\"domain-select\">\n","                    <option value=\"medical\" selected>Medical</option>\n","                    <option value=\"legal\">Legal</option>\n","                </select>\n","                <label for=\"model-select\">Model:</label>\n","                <select id=\"model-select\">\n","                    <option value=\"bert\">BERT</option>\n","                    <option value=\"lstm\">LSTM</option>\n","                </select>\n","            </div>\n","            <button id=\"analyze-btn\">Analyze</button>\n","        </div>\n","        <div class=\"output-section\">\n","            <h2>Results</h2>\n","            <div id=\"progress\" class=\"hidden\">Processing...</div>\n","            <div id=\"general-corrections\">\n","                <h3>General Corrections</h3>\n","                <ul id=\"general-list\"></ul>\n","            </div>\n","            <div id=\"domain-corrections\">\n","                <h3>Domain-Specific Corrections</h3>\n","                <ul id=\"domain-list\"></ul>\n","            </div>\n","            <div id=\"highlighted-text\">\n","                <h3>Highlighted Text</h3>\n","                <p id=\"highlighted-output\"></p>\n","            </div>\n","        </div>\n","    </div>\n","    <script src=\"script.js\"></script>\n","</body>\n","</html>"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BhnwYRl069nY","executionInfo":{"status":"ok","timestamp":1750437060099,"user_tz":-330,"elapsed":12,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"406f6395-ad92-47c4-a608-2ab6970892a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing styles.css\n"]}],"source":["%%writefile styles.css\n","body {\n","    font-family: Arial, sans-serif;\n","    margin: 0;\n","    padding: 20px;\n","    background-color: #f4f4f9;\n","}\n",".container {\n","    max-width: 800px;\n","    margin: 0 auto;\n","    background: white;\n","    padding: 20px;\n","    border-radius: 8px;\n","    box-shadow: 0 0 10px rgba(0,0,0,0.1);\n","}\n","h1 {\n","    text-align: center;\n","    color: #333;\n","}\n",".input-section, .output-section {\n","    margin-bottom: 20px;\n","}\n","textarea {\n","    width: 100%;\n","    padding: 10px;\n","    margin-bottom: 10px;\n","    border: 1px solid #ddd;\n","    border-radius: 4px;\n","}\n","input[type=\"file\"] {\n","    margin-bottom: 10px;\n","}\n",".options {\n","    display: flex;\n","    gap: 10px;\n","    margin-bottom: 10px;\n","}\n","button {\n","    background-color: #007bff;\n","    color: white;\n","    padding: 10px 20px;\n","    border: none;\n","    border-radius: 4px;\n","    cursor: pointer;\n","}\n","button:hover {\n","    background-color: #0056b3;\n","}\n",".output-section h3 {\n","    color: #444;\n","}\n","ul {\n","    list-style: none;\n","    padding: 0;\n","}\n","li {\n","    padding: 5px 0;\n","}\n",".error {\n","    background-color: #ffe6e6;\n","    text-decoration: line-through;\n","}\n",".correction {\n","    background-color: #e6ffe6;\n","}\n",".hidden {\n","    display: none;\n","}\n","#progress {\n","    text-align: center;\n","    color: #007bff;\n","}"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"otrzn5q769nZ","executionInfo":{"status":"ok","timestamp":1750437060111,"user_tz":-330,"elapsed":10,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"e96707e0-345d-494e-f823-6d4f55131f62"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing script.js\n"]}],"source":["%%writefile script.js\n","document.getElementById('analyze-btn').addEventListener('click', async () => {\n","    const textInput = document.getElementById('text-input').value;\n","    const fileInput = document.getElementById('file-input').files;\n","    const domain = document.getElementById('domain-select').value;\n","    const model = document.getElementById('model-select').value;\n","    const generalList = document.getElementById('general-list');\n","    const domainList = document.getElementById('domain-list');\n","    const highlightedOutput = document.getElementById('highlighted-output');\n","    const progress = document.getElementById('progress');\n","\n","    generalList.innerHTML = '';\n","    domainList.innerHTML = '';\n","    highlightedOutput.innerHTML = '';\n","    progress.classList.remove('hidden');\n","\n","    try {\n","        const baseUrl = window.location.origin.includes('ngrok') ? window.location.origin : 'http://localhost:5000';\n","        if (textInput) {\n","            const response = await fetch(`${baseUrl}/analyze`, {\n","                method: 'POST',\n","                headers: { 'Content-Type': 'application/json' },\n","                body: JSON.stringify({ text: textInput, domain, model })\n","            });\n","            const data = await response.json();\n","            displayResults(data, textInput);\n","        } else if (fileInput.length) {\n","            for (let file of fileInput) {\n","                const text = await readFile(file);\n","                const response = await fetch(`${baseUrl}/analyze`, {\n","                    method: 'POST',\n","                    headers: { 'Content-Type': 'application/json' },\n","                    body: JSON.stringify({ text, domain, model })\n","                });\n","                const data = await response.json();\n","                displayResults(data, text, file.name);\n","            }\n","        } else {\n","            alert('Please enter text or upload a file.');\n","        }\n","    } catch (error) {\n","        console.error('Error:', error);\n","        alert('An error occurred while analyzing.');\n","    } finally {\n","        progress.classList.add('hidden');\n","    }\n","});\n","\n","function readFile(file) {\n","    return new Promise((resolve, reject) => {\n","        const reader = new FileReader();\n","        reader.onload = () => resolve(reader.result);\n","        reader.onerror = reject;\n","        if (file.name.endsWith('.docx')) {\n","            reject('DOCX parsing not implemented.');\n","        } else {\n","            reader.readAsText(file);\n","        }\n","    });\n","}\n","\n","function displayResults(data, text, filename = '') {\n","    const generalList = document.getElementById('general-list');\n","    const domainList = document.getElementById('domain-list');\n","    const highlightedOutput = document.getElementById('highlighted-output');\n","\n","    data.general_corrections.forEach(c => {\n","        const li = document.createElement('li');\n","        li.textContent = `${c.original} → ${c.corrected} [${(c.confidence * 100).toFixed(0)}%]`;\n","        generalList.appendChild(li);\n","    });\n","\n","    data.domain_corrections.forEach(c => {\n","        const li = document.createElement('li');\n","        li.textContent = `${c.original} → ${c.corrected} [${(c.confidence * 100).toFixed(0)}%]`;\n","        domainList.appendChild(li);\n","    });\n","\n","    let highlighted = text;\n","    data.general_corrections.concat(data.domain_corrections).forEach(c => {\n","        const regex = new RegExp(`\\\\b${c.original}\\\\b`, 'gi');\n","        highlighted = highlighted.replace(regex, `<span class=\"error\">${c.original}</span> <span class=\"correction\">${c.corrected}</span>`);\n","    });\n","    highlightedOutput.innerHTML = filename ? `<strong>${filename}</strong>: ${highlighted}` : highlighted;\n","}"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_U8kcGK69na","executionInfo":{"status":"ok","timestamp":1750437060118,"user_tz":-330,"elapsed":6,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"d84a1fd7-94cf-4399-bcf2-812e43f0a1e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing domain_config.json\n"]}],"source":["%%writefile domain_config.json\n","{\n","    \"medical\": {\n","        \"terms\": [\n","            \"hypertension\",\n","            \"cardiology\",\n","            \"arrhythmia\",\n","            \"oncology\"\n","        ]\n","    },\n","    \"legal\": {\n","        \"terms\": [\n","            \"defendant\",\n","            \"plaintiff\",\n","            \"litigation\",\n","            \"arbitration\"\n","        ]\n","    }\n","}"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DBzuSGC769na","executionInfo":{"status":"ok","timestamp":1750437060130,"user_tz":-330,"elapsed":11,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"00f3fb11-42a7-452b-b5e6-334c72e9db97"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing app.py\n"]}],"source":["%%writefile app.py\n","from flask import Flask, request, jsonify\n","from flask_cors import CORS\n","import nltk\n","from nltk.corpus import words\n","from transformers import TFBertTokenizer, TFBertForMaskedLM\n","import tensorflow as tf\n","import numpy as np\n","import json\n","import logging\n","import re\n","\n","# Setup logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","# Initialize Flask app\n","app = Flask(__name__)\n","CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n","\n","# Download NLTK data\n","try:\n","    nltk.download('words', quiet=True)\n","    word_list = set(words.words())\n","except Exception as e:\n","    logger.error(f\"Error downloading NLTK data: {e}\")\n","    word_list = set()\n","\n","# Load domain config\n","try:\n","    with open('domain_config.json', 'r') as f:\n","        domain_config = json.load(f)\n","except Exception as e:\n","    logger.error(f\"Error loading domain config: {e}\")\n","    domain_config = {\"medical\": {\"terms\": []}, \"legal\": {\"terms\": []}}\n","\n","# Load BERT model (only BioBERT for testing)\n","try:\n","    logger.info(\"Loading BioBERT for medical domain...\")\n","    medical_tokenizer = TFBertTokenizer.from_pretrained('allenai/biobert_v1.1_pubmed')\n","    medical_model = TFBertForMaskedLM.from_pretrained('allenai/biobert_v1.1_pubmed')\n","    legal_tokenizer = None\n","    legal_model = None\n","except Exception as e:\n","    logger.error(f\"Error loading BioBERT: {e}\")\n","    raise\n","\n","# Define LSTM model\n","def create_lstm_model(vocab_size, max_length=128):\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Embedding(vocab_size, 128, input_length=max_length),\n","        tf.keras.layers.LSTM(256, return_sequences=True),\n","        tf.keras.layers.LSTM(256),\n","        tf.keras.layers.Dense(vocab_size, activation='softmax')\n","    ])\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","    return model\n","\n","# Initialize LSTM model (placeholder)\n","vocab_size = 30000\n","lstm_model = create_lstm_model(vocab_size)\n","\n","@app.route('/analyze', methods=['POST'])\n","def analyze_text():\n","    try:\n","        data = request.get_json()\n","        text = data.get('text', '')\n","        domain = data.get('domain', 'medical')\n","        model_type = data.get('model', 'bert')\n","\n","        if not text:\n","            return jsonify({\"error\": \"No text provided\"}), 400\n","\n","        words = re.findall(r'\\b\\w+\\b', text.lower())\n","        general_corrections = []\n","        domain_corrections = []\n","\n","        for word in words:\n","            if word not in word_list:\n","                correction, confidence = suggest_correction(word, domain, model_type)\n","                if correction and correction != word:\n","                    if word in domain_config.get(domain, {}).get('terms', []):\n","                        domain_corrections.append({\n","                            \"original\": word,\n","                            \"corrected\": correction,\n","                            \"confidence\": confidence\n","                        })\n","                    else:\n","                        general_corrections.append({\n","                            \"original\": word,\n","                            \"corrected\": correction,\n","                            \"confidence\": confidence\n","                        })\n","\n","        return jsonify({\n","            \"general_corrections\": general_corrections,\n","            \"domain_corrections\": domain_corrections\n","        })\n","    except Exception as e:\n","        logger.error(f\"Error analyzing text: {e}\")\n","        return jsonify({\"error\": \"Internal server error\"}), 500\n","\n","def suggest_correction(word, domain, model_type):\n","    try:\n","        if model_type == 'bert':\n","            if domain == 'legal':\n","                return word, 0.9  # Placeholder for LegalBERT\n","            tokenizer = medical_tokenizer\n","            model = medical_model\n","            sentence = f\"The {'patient' if domain == 'medical' else 'case'} involves {word}.\"\n","            inputs = tokenizer(sentence, return_tensors='tf')\n","            mask_idx = inputs['input_ids'][0].numpy().tolist().index(tokenizer.mask_token_id)\n","            outputs = model(inputs)\n","            logits = outputs.logits[0, mask_idx]\n","            probs = tf.nn.softmax(logits).numpy()\n","            top_idx = np.argmax(probs)\n","            correction = tokenizer.decode([top_idx]).strip()\n","            confidence = float(probs[top_idx])\n","            return correction, confidence\n","        else:\n","            return word, 0.9  # Placeholder for LSTM\n","    except Exception as e:\n","        logger.error(f\"Error suggesting correction for {word}: {e}\")\n","        return None, 0.0\n","\n","if __name__ == '__main__':\n","    app.run(host='0.0.0.0', port=5000)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YrN-cOtp69nb","executionInfo":{"status":"ok","timestamp":1750437081852,"user_tz":-330,"elapsed":21720,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"0201b557-4079-42cf-ca73-eaa3f673da24"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-06-20 16:31:13.283553: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-06-20 16:31:15.926732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1603, in _get_module\n","    return importlib.import_module(\".\" + module_name, self.__name__)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n","    return _bootstrap._gcd_import(name[level:], package, level)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n","  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n","  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n","  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/tokenization_bert_tf.py\", line 5, in <module>\n","    from tensorflow_text import BertTokenizer as BertTokenizerLayer\n","  File \"/usr/local/lib/python3.11/dist-packages/tensorflow_text/__init__.py\", line 21, in <module>\n","    from tensorflow_text.python import keras\n","  File \"/usr/local/lib/python3.11/dist-packages/tensorflow_text/python/keras/__init__.py\", line 21, in <module>\n","    from tensorflow_text.python.keras.layers import *\n","  File \"/usr/local/lib/python3.11/dist-packages/tensorflow_text/python/keras/layers/__init__.py\", line 22, in <module>\n","    from tensorflow_text.python.keras.layers.tokenization_layers import *\n","  File \"/usr/local/lib/python3.11/dist-packages/tensorflow_text/python/keras/layers/tokenization_layers.py\", line 24, in <module>\n","    from tensorflow_text.python.ops import unicode_script_tokenizer\n","  File \"/usr/local/lib/python3.11/dist-packages/tensorflow_text/python/ops/__init__.py\", line 23, in <module>\n","    from tensorflow_text.core.pybinds.pywrap_fast_bert_normalizer_model_builder import build_fast_bert_normalizer_model\n","ImportError: /usr/local/lib/python3.11/dist-packages/tensorflow_text/core/pybinds/pywrap_fast_bert_normalizer_model_builder.so: undefined symbol: _ZN3tsl8internal10LogMessageC1EPKciN4absl12lts_2023080211LogSeverityE\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/content/app.py\", line 5, in <module>\n","    from transformers import TFBertTokenizer, TFBertForMaskedLM\n","  File \"<frozen importlib._bootstrap>\", line 1229, in _handle_fromlist\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1594, in __getattr__\n","    value = getattr(module, name)\n","            ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1593, in __getattr__\n","    module = self._get_module(self._class_to_module[name])\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1605, in _get_module\n","    raise RuntimeError(\n","RuntimeError: Failed to import transformers.models.bert.tokenization_bert_tf because of the following error (look up to see its traceback):\n","/usr/local/lib/python3.11/dist-packages/tensorflow_text/core/pybinds/pywrap_fast_bert_normalizer_model_builder.so: undefined symbol: _ZN3tsl8internal10LogMessageC1EPKciN4absl12lts_2023080211LogSeverityE\n"]}],"source":["# Run Flask server (run this cell and check logs)\n","!python app.py"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QH7FUCpC69nb","executionInfo":{"status":"ok","timestamp":1750437083419,"user_tz":-330,"elapsed":1564,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"79eeb0db-6273-406f-bdd8-c1cbd487548d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Public URL: https://2e15-34-145-75-81.ngrok-free.app\n","Open in browser: https://2e15-34-145-75-81.ngrok-free.app/index.html\n","Test the API with the cell below.\n"]}],"source":["# Setup ngrok (run after Flask server is running)\n","from pyngrok import ngrok\n","\n","# Start ngrok tunnel\n","ngrok.set_auth_token('2yk1wCi1k5WBn0Ubet6spRSf6W2_2Sa9R1sM8EEqXemZUXz6k')  # Replace with your ngrok auth token\n","public_url = ngrok.connect(5000).public_url\n","print(f'Public URL: {public_url}')\n","\n","# Access the web interface\n","print(f'Open in browser: {public_url}/index.html')\n","print('Test the API with the cell below.')"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zeOEXMa_69nb","executionInfo":{"status":"ok","timestamp":1750437084017,"user_tz":-330,"elapsed":595,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"b8bbec51-aa99-4c0b-fe70-6f9811eeec12"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pyngrok.process.ngrok:t=2025-06-20T16:31:23+0000 lvl=warn msg=\"failed to open private leg\" id=64fab47678c5 privaddr=localhost:5000 err=\"dial tcp 127.0.0.1:5000: connect: connection refused\"\n"]}],"source":["# Test the /analyze endpoint\n","import requests\n","\n","payload = {\n","    'text': 'Patient has hypertention.',\n","    'domain': 'medical',\n","    'model': 'bert'\n","}\n","response = requests.post(f'{public_url}/analyze', json=payload)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}