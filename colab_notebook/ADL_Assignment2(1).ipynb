{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNWvnggOtKusqPpnJYEheHH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fp0aRbK_8Sgq","executionInfo":{"status":"ok","timestamp":1754242903828,"user_tz":-330,"elapsed":182464,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"4a05da7a-ce20-4e36-87d5-414c9acc52f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","Generator Loss: 4.0734, Discriminator Loss: 0.0671\n","Epoch 2/50\n","Generator Loss: 4.1152, Discriminator Loss: 0.0861\n","Epoch 3/50\n","Generator Loss: 3.4163, Discriminator Loss: 0.1908\n","Epoch 4/50\n","Generator Loss: 3.9177, Discriminator Loss: 0.3519\n","Epoch 5/50\n","Generator Loss: 2.9517, Discriminator Loss: 0.6817\n","Epoch 6/50\n","Generator Loss: 2.9772, Discriminator Loss: 0.6232\n","Epoch 7/50\n","Generator Loss: 2.4804, Discriminator Loss: 0.4628\n","Epoch 8/50\n","Generator Loss: 2.9621, Discriminator Loss: 0.7233\n","Epoch 9/50\n","Generator Loss: 1.9550, Discriminator Loss: 0.6662\n","Epoch 10/50\n","Generator Loss: 2.6956, Discriminator Loss: 0.8019\n","Epoch 11/50\n","Generator Loss: 2.2964, Discriminator Loss: 0.5583\n","Epoch 12/50\n","Generator Loss: 2.7128, Discriminator Loss: 0.9399\n","Epoch 13/50\n","Generator Loss: 1.8036, Discriminator Loss: 0.7408\n","Epoch 14/50\n","Generator Loss: 1.5189, Discriminator Loss: 0.8829\n","Epoch 15/50\n","Generator Loss: 1.8139, Discriminator Loss: 0.9026\n","Epoch 16/50\n","Generator Loss: 1.7583, Discriminator Loss: 0.8583\n","Epoch 17/50\n","Generator Loss: 2.0059, Discriminator Loss: 0.8603\n","Epoch 18/50\n","Generator Loss: 1.7709, Discriminator Loss: 1.0549\n","Epoch 19/50\n","Generator Loss: 1.8186, Discriminator Loss: 0.9816\n","Epoch 20/50\n","Generator Loss: 1.6945, Discriminator Loss: 0.9110\n","Epoch 21/50\n","Generator Loss: 1.7746, Discriminator Loss: 0.8950\n","Epoch 22/50\n","Generator Loss: 1.3138, Discriminator Loss: 0.9505\n","Epoch 23/50\n","Generator Loss: 2.1529, Discriminator Loss: 0.8872\n","Epoch 24/50\n","Generator Loss: 2.0601, Discriminator Loss: 0.9134\n","Epoch 25/50\n","Generator Loss: 2.0273, Discriminator Loss: 1.0353\n","Epoch 26/50\n","Generator Loss: 2.1712, Discriminator Loss: 0.6472\n","Epoch 27/50\n","Generator Loss: 1.7511, Discriminator Loss: 0.7274\n","Epoch 28/50\n","Generator Loss: 1.6708, Discriminator Loss: 0.9044\n","Epoch 29/50\n","Generator Loss: 1.6527, Discriminator Loss: 0.9322\n","Epoch 30/50\n","Generator Loss: 1.9031, Discriminator Loss: 0.7168\n","Epoch 31/50\n","Generator Loss: 2.0597, Discriminator Loss: 0.7343\n","Epoch 32/50\n","Generator Loss: 2.0607, Discriminator Loss: 0.8664\n","Epoch 33/50\n","Generator Loss: 2.0732, Discriminator Loss: 0.6629\n","Epoch 34/50\n","Generator Loss: 1.8115, Discriminator Loss: 0.9766\n","Epoch 35/50\n","Generator Loss: 1.9542, Discriminator Loss: 0.8569\n","Epoch 36/50\n","Generator Loss: 2.0515, Discriminator Loss: 0.8368\n","Epoch 37/50\n","Generator Loss: 1.7674, Discriminator Loss: 0.7033\n","Epoch 38/50\n","Generator Loss: 1.6560, Discriminator Loss: 0.8270\n","Epoch 39/50\n","Generator Loss: 1.6171, Discriminator Loss: 0.8188\n","Epoch 40/50\n","Generator Loss: 1.5825, Discriminator Loss: 0.8304\n","Epoch 41/50\n","Generator Loss: 1.7324, Discriminator Loss: 0.8446\n","Epoch 42/50\n","Generator Loss: 1.4976, Discriminator Loss: 0.8153\n","Epoch 43/50\n","Generator Loss: 2.0355, Discriminator Loss: 0.9833\n","Epoch 44/50\n","Generator Loss: 1.8381, Discriminator Loss: 0.8618\n","Epoch 45/50\n","Generator Loss: 1.4012, Discriminator Loss: 0.9197\n","Epoch 46/50\n","Generator Loss: 2.3734, Discriminator Loss: 0.9079\n","Epoch 47/50\n","Generator Loss: 1.4609, Discriminator Loss: 0.8600\n","Epoch 48/50\n","Generator Loss: 1.7602, Discriminator Loss: 0.8296\n","Epoch 49/50\n","Generator Loss: 1.5994, Discriminator Loss: 0.8133\n","Epoch 50/50\n","Generator Loss: 2.0020, Discriminator Loss: 0.8188\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Set random seed for reproducibility\n","tf.random.set_seed(42)\n","np.random.seed(42)\n","\n","# Hyperparameters\n","LATENT_DIM = 100\n","IMG_SHAPE = (28, 28, 1)\n","BATCH_SIZE = 128\n","EPOCHS = 50\n","STEPS_PER_EPOCH = 500\n","\n","# Build the Generator\n","def build_generator():\n","    model = tf.keras.Sequential([\n","        layers.Dense(256, input_dim=LATENT_DIM),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.BatchNormalization(momentum=0.8),\n","        layers.Dense(512),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.BatchNormalization(momentum=0.8),\n","        layers.Dense(1024),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.BatchNormalization(momentum=0.8),\n","        layers.Dense(np.prod(IMG_SHAPE), activation='tanh'),\n","        layers.Reshape(IMG_SHAPE)\n","    ])\n","    return model\n","\n","# Build the Discriminator\n","def build_discriminator():\n","    model = tf.keras.Sequential([\n","        layers.Flatten(input_shape=IMG_SHAPE),\n","        layers.Dense(512),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Dense(256),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Dense(1, activation='sigmoid')\n","    ])\n","    return model\n","\n","# Combined GAN model\n","def build_gan(generator, discriminator):\n","    discriminator.trainable = False\n","    model = tf.keras.Sequential([generator, discriminator])\n","    return model\n","\n","# Loss functions\n","cross_entropy = tf.keras.losses.BinaryCrossentropy()\n","\n","def discriminator_loss(real_output, fake_output):\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    return real_loss + fake_loss\n","\n","def generator_loss(fake_output):\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n","# Optimizers\n","generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","\n","# Training step\n","@tf.function\n","def train_step(images, generator, discriminator, batch_size):\n","    noise = tf.random.normal([batch_size, LATENT_DIM])\n","\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        generated_images = generator(noise, training=True)\n","\n","        real_output = discriminator(images, training=True)\n","        fake_output = discriminator(generated_images, training=True)\n","\n","        gen_loss = generator_loss(fake_output)\n","        disc_loss = discriminator_loss(real_output, fake_output)\n","\n","    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","\n","    return gen_loss, disc_loss\n","\n","# Function to generate and save images\n","def generate_and_save_images(generator, epoch, test_noise):\n","    predictions = generator(test_noise, training=False)\n","    fig = plt.figure(figsize=(4, 4))\n","\n","    for i in range(16):\n","        plt.subplot(4, 4, i+1)\n","        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n","        plt.axis('off')\n","\n","    plt.savefig(f'image_at_epoch_{epoch:04d}.png')\n","    plt.close()\n","\n","# Main training loop\n","def train_gan(dataset, generator, discriminator):\n","    test_noise = tf.random.normal([16, LATENT_DIM])\n","\n","    for epoch in range(EPOCHS):\n","        print(f'Epoch {epoch + 1}/{EPOCHS}')\n","        for batch in dataset:\n","            gen_loss, disc_loss = train_step(batch, generator, discriminator, BATCH_SIZE)\n","\n","        # Generate and save sample images every 10 epochs\n","        if (epoch + 1) % 10 == 0:\n","            generate_and_save_images(generator, epoch + 1, test_noise)\n","\n","        print(f'Generator Loss: {gen_loss:.4f}, Discriminator Loss: {disc_loss:.4f}')\n","\n","def main():\n","    # Load and preprocess MNIST dataset\n","    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n","    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32')\n","    x_train = (x_train - 127.5) / 127.5  # Normalize to [-1, 1]\n","\n","    # Create dataset\n","    dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(60000).batch(BATCH_SIZE)\n","\n","    # Initialize models\n","    generator = build_generator()\n","    discriminator = build_discriminator()\n","\n","    # Train the GAN\n","    train_gan(dataset, generator, discriminator)\n","\n","if __name__ == '__main__':\n","    main()"]}]}