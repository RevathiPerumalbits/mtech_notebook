{"cells":[{"cell_type":"markdown","metadata":{"id":"Co2wm3FJVrX8"},"source":["# Financial Document Analysis and Q&A Pipeline\n","\n","This Jupyter Notebook consolidates a series of Python scripts into a cohesive pipeline for extracting, cleaning, segmenting, and analyzing financial data from PDF documents. The ultimate goal is to create a retrieval system capable of answering questions based on the document's content.\n","\n","The pipeline is organized into the following logical steps:\n","1.  **Data Extraction**: Reading text from PDF files.\n","2.  **Text Cleaning**: Removing noise and standardizing the extracted text.\n","3.  **Document Segmentation**: Isolating specific financial statements (e.g., Balance Sheet, Income Statement).\n","4.  **Validation**: Ensuring the segmented sections are correct and complete.\n","5.  **Q&A Generation**: Creating question-answer pairs from the financial data (for fine-tuning or evaluation).\n","6.  **Chunking**: Breaking down the text into small, meaningful sentences for embedding.\n","7.  **Embedding & Indexing**: Converting text chunks into numerical vectors and building search indexes (FAISS and BM25).\n","8.  **Data Loading**: Utility functions to load the generated indexes and chunks for the final application."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install -r \"/content/drive/My Drive/requirements.txt\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CO2dFkrAdgIZ","executionInfo":{"status":"ok","timestamp":1755542466735,"user_tz":-330,"elapsed":135739,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"dc963922-caed-40d5-d8c9-045a44b0a884"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting pytesseract (from -r /content/drive/My Drive/requirements.txt (line 1))\n","  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n","Collecting PyMuPDF (from -r /content/drive/My Drive/requirements.txt (line 2))\n","  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n","Collecting pdf2image (from -r /content/drive/My Drive/requirements.txt (line 3))\n","  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/requirements.txt (line 4)) (2.2.2)\n","Collecting pdfplumber (from -r /content/drive/My Drive/requirements.txt (line 5))\n","  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/requirements.txt (line 6)) (5.1.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/requirements.txt (line 7)) (3.9.1)\n","Collecting faiss-cpu (from -r /content/drive/My Drive/requirements.txt (line 8))\n","  Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n","Collecting rank_bm25 (from -r /content/drive/My Drive/requirements.txt (line 9))\n","  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n","Collecting streamlit (from -r /content/drive/My Drive/requirements.txt (line 10))\n","  Downloading streamlit-1.48.1-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/requirements.txt (line 11)) (1.10.0)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/requirements.txt (line 12)) (0.17.0)\n","Collecting bitsandbytes (from -r /content/drive/My Drive/requirements.txt (line 14))\n","  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/My Drive/requirements.txt (line 15)) (2.6.0+cu124)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract->-r /content/drive/My Drive/requirements.txt (line 1)) (25.0)\n","Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract->-r /content/drive/My Drive/requirements.txt (line 1)) (11.3.0)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /content/drive/My Drive/requirements.txt (line 4)) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /content/drive/My Drive/requirements.txt (line 4)) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /content/drive/My Drive/requirements.txt (line 4)) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /content/drive/My Drive/requirements.txt (line 4)) (2025.2)\n","Collecting pdfminer.six==20250506 (from pdfplumber->-r /content/drive/My Drive/requirements.txt (line 5))\n","  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n","Collecting pypdfium2>=4.18.0 (from pdfplumber->-r /content/drive/My Drive/requirements.txt (line 5))\n","  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber->-r /content/drive/My Drive/requirements.txt (line 5)) (3.4.3)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber->-r /content/drive/My Drive/requirements.txt (line 5)) (43.0.3)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /content/drive/My Drive/requirements.txt (line 6)) (4.55.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /content/drive/My Drive/requirements.txt (line 6)) (4.67.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /content/drive/My Drive/requirements.txt (line 6)) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /content/drive/My Drive/requirements.txt (line 6)) (1.16.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /content/drive/My Drive/requirements.txt (line 6)) (0.34.4)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /content/drive/My Drive/requirements.txt (line 6)) (4.14.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->-r /content/drive/My Drive/requirements.txt (line 7)) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->-r /content/drive/My Drive/requirements.txt (line 7)) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->-r /content/drive/My Drive/requirements.txt (line 7)) (2024.11.6)\n","Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (1.9.0)\n","Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (5.5.2)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (5.29.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (18.1.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (2.32.3)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (9.1.2)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (0.10.2)\n","Collecting watchdog<7,>=2.1.5 (from streamlit->-r /content/drive/My Drive/requirements.txt (line 10))\n","  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (3.1.45)\n","Collecting pydeck<1,>=0.8.0b4 (from streamlit->-r /content/drive/My Drive/requirements.txt (line 10))\n","  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (6.4.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->-r /content/drive/My Drive/requirements.txt (line 11)) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate->-r /content/drive/My Drive/requirements.txt (line 11)) (6.0.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate->-r /content/drive/My Drive/requirements.txt (line 11)) (0.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/My Drive/requirements.txt (line 15)) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/My Drive/requirements.txt (line 15)) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/My Drive/requirements.txt (line 15)) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/My Drive/requirements.txt (line 15)) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->-r /content/drive/My Drive/requirements.txt (line 15))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->-r /content/drive/My Drive/requirements.txt (line 15))\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->-r /content/drive/My Drive/requirements.txt (line 15))\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r /content/drive/My Drive/requirements.txt (line 15))\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r /content/drive/My Drive/requirements.txt (line 15))\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r /content/drive/My Drive/requirements.txt (line 15))\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->-r /content/drive/My Drive/requirements.txt (line 15))\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r /content/drive/My Drive/requirements.txt (line 15))\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r /content/drive/My Drive/requirements.txt (line 15))\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/My Drive/requirements.txt (line 15)) (0.6.2)\n","Collecting nvidia-nccl-cu12==2.21.5 (from torch->-r /content/drive/My Drive/requirements.txt (line 15))\n","  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/My Drive/requirements.txt (line 15)) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r /content/drive/My Drive/requirements.txt (line 15))\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/My Drive/requirements.txt (line 15)) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/My Drive/requirements.txt (line 15)) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r /content/drive/My Drive/requirements.txt (line 15)) (1.3.0)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (4.25.0)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (2.1.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (4.0.12)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r /content/drive/My Drive/requirements.txt (line 6)) (1.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r /content/drive/My Drive/requirements.txt (line 15)) (3.0.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r /content/drive/My Drive/requirements.txt (line 4)) (1.17.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (2025.8.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r /content/drive/My Drive/requirements.txt (line 6)) (0.21.4)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->-r /content/drive/My Drive/requirements.txt (line 6)) (3.6.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber->-r /content/drive/My Drive/requirements.txt (line 5)) (1.17.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (5.0.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r /content/drive/My Drive/requirements.txt (line 10)) (0.27.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber->-r /content/drive/My Drive/requirements.txt (line 5)) (2.22)\n","Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n","Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n","Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n","Downloading streamlit-1.48.1-py3-none-any.whl (9.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: watchdog, rank_bm25, pytesseract, pypdfium2, PyMuPDF, pdf2image, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, pydeck, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pdfminer.six, nvidia-cusolver-cu12, pdfplumber, streamlit, bitsandbytes\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.23.4\n","    Uninstalling nvidia-nccl-cu12-2.23.4:\n","      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed PyMuPDF-1.26.3 bitsandbytes-0.47.0 faiss-cpu-1.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 pdf2image-1.17.0 pdfminer.six-20250506 pdfplumber-0.11.7 pydeck-0.9.1 pypdfium2-4.30.0 pytesseract-0.3.13 rank_bm25-0.2.2 streamlit-1.48.1 watchdog-6.0.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"tLVgOIteVrX_"},"source":["## 1. Data Extraction (`_01_data_extract.py`)\n","\n","This script is the first step in our pipeline. It's responsible for extracting raw text from PDF documents. It uses the `pdfplumber` library, which is excellent at preserving the layout and reading order of the text within a PDF. It also includes a fallback mechanism to use Optical Character Recognition (OCR) via `pytesseract` if a PDF contains images of text instead of selectable text."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zhGxaskLVrX_","executionInfo":{"status":"ok","timestamp":1755542504328,"user_tz":-330,"elapsed":3177,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"d5366445-e4e7-4c21-8143-38eeb709387e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import fitz  # PyMuPDF\n","import pytesseract\n","from pdf2image import convert_from_path\n","import os\n","import logging\n","import pdfplumber\n","\n","logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n","logger = logging.getLogger(__name__)\n","# --- Google Drive Mounting ---\n","# This will prompt you for authorization when you run it in a Colab cell.\n","drive.mount('/content/drive')\n","\n","def pdf_to_text(pdf_path, ocr=False):\n","    \"\"\"Extract text from a PDF, preserving the natural reading order.\"\"\"\n","    logger.info(f\"Extracting text from {pdf_path}...\")\n","    full_text = \"\"\n","    try:\n","        if not ocr:\n","            # Using pdfplumber for better layout preservation\n","            with pdfplumber.open(pdf_path) as pdf:\n","                for page in pdf.pages:\n","                    page_text = page.extract_text(layout=True, x_tolerance=2)\n","                    if page_text:\n","                        full_text += page_text + \"\\n\"\n","            logger.info(f\"Direct text extracted from {pdf_path} in reading order.\")\n","        else:\n","            # OCR fallback\n","            images = convert_from_path(pdf_path)\n","            for i, img in enumerate(images):\n","                full_text += pytesseract.image_to_string(img) + \"\\n\"\n","                logger.info(f\"OCR processed page {i+1}/{len(images)}\")\n","\n","        return full_text.strip()\n","\n","    except Exception as e:\n","        logger.error(f\"Error extracting text from {pdf_path}: {e}\")\n","        return \"\"\n"]},{"cell_type":"markdown","metadata":{"id":"u4-Fvt3DVrYA"},"source":["## 2. Text Cleaning (`_02_data_clean.py`)\n","\n","Once the text is extracted, it often contains unwanted elements like page numbers, headers, footers, and excessive whitespace. This script defines a `clean_text` function that uses regular expressions (`re`) to remove this noise, making the text much cleaner for the subsequent processing steps."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"CdRr5906VrYA","executionInfo":{"status":"ok","timestamp":1755543364297,"user_tz":-330,"elapsed":41,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}}},"outputs":[],"source":["import re\n","import os\n","import logging\n","\n","logging.basicConfig(level=logging.INFO, force=True, )\n","logger = logging.getLogger(__name__)\n","\n","def clean_text(raw_text):\n","    \"\"\"Clean raw text by removing page numbers, headers, footers, and extra whitespace.\"\"\"\n","    logger.info(\"Text cleaning started...\")\n","    try:\n","        # Remove page numbers\n","        text = re.sub(r'Page\\s+\\d+\\s+of\\s+\\d+', '', raw_text, flags=re.IGNORECASE)\n","        text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE)\n","\n","        # Remove common report headers/footers (add more specific patterns as needed)\n","        text = re.sub(r'Infosys Limited and subsidiaries', '', text, flags=re.IGNORECASE)\n","        text = re.sub(r'Annual Report \\d{4}', '', text, flags=re.IGNORECASE)\n","\n","        # Remove multiple blank lines\n","        text = re.sub(r'\\n\\s*\\n+', '\\n', text)\n","\n","        # Normalize whitespace\n","        text = re.sub(r'\\s+', ' ', text).strip()\n","\n","        logger.info(\"Text cleaned successfully...\")\n","        return text\n","    except Exception as e:\n","        logger.error(f\"Error cleaning text: {e}\")\n","        return raw_text\n"]},{"cell_type":"markdown","metadata":{"id":"tdlJOf-YVrYB"},"source":["## 3. Document Segmentation (`_03_data_segment.py`)\n","\n","Financial reports are long and contain many different sections. For our purpose, we're interested in specific statements. This script segments the cleaned text to isolate the **Income Statement** and the **Balance Sheet**. It uses powerful regular expressions to identify the start and end points of these sections. The script also orchestrates the full extract-clean-segment pipeline in its `main` function and includes a validation step."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-LpF5bDdVrYB","executionInfo":{"status":"ok","timestamp":1755544547755,"user_tz":-330,"elapsed":12384,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"bd3d6ea6-d784-4fcb-9f43-8ab3c34572c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  poppler-utils\n","0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n","Need to get 186 kB of archives.\n","After this operation, 697 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.9 [186 kB]\n","Fetched 186 kB in 1s (310 kB/s)\n","Selecting previously unselected package poppler-utils.\n","(Reading database ... 126380 files and directories currently installed.)\n","Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.9_amd64.deb ...\n","Unpacking poppler-utils (22.02.0-2ubuntu0.9) ...\n","Setting up poppler-utils (22.02.0-2ubuntu0.9) ...\n","Processing triggers for man-db (2.10.2-1) ...\n"]},{"output_type":"stream","name":"stderr","text":["2025-08-18 19:15:44,612 - INFO - Executing the segmentation module\n","2025-08-18 19:15:44,616 - INFO - Processing /content/drive/MyDrive/data/raw/infosys_2023.pdf for year 2023\n","2025-08-18 19:15:46,014 - INFO - Saved raw text to /content/drive/MyDrive/data/output/infosys_2023_raw.txt\n","2025-08-18 19:15:46,387 - INFO - Saved cleaned text to /content/drive/MyDrive/data/clean/infosys_2023_cleaned.txt\n","2025-08-18 19:15:46,388 - INFO - Segmenting report into logical sections...\n","2025-08-18 19:15:46,403 - INFO - Found and extracted 'income_statement'.\n","2025-08-18 19:15:46,416 - INFO - Found and extracted 'balance_sheet'.\n","2025-08-18 19:15:46,424 - INFO - Saved income_statement to /content/drive/MyDrive/data/segmented/infosys_2023_income_statement.txt\n","2025-08-18 19:15:46,430 - INFO - Saved balance_sheet to /content/drive/MyDrive/data/segmented/infosys_2023_balance_sheet.txt\n","2025-08-18 19:15:46,431 - INFO - Processing /content/drive/MyDrive/data/raw/infosys_2024.pdf for year 2024\n","2025-08-18 19:15:48,468 - INFO - Saved raw text to /content/drive/MyDrive/data/output/infosys_2024_raw.txt\n","2025-08-18 19:15:48,617 - INFO - Saved cleaned text to /content/drive/MyDrive/data/clean/infosys_2024_cleaned.txt\n","2025-08-18 19:15:48,618 - INFO - Segmenting report into logical sections...\n","2025-08-18 19:15:48,626 - INFO - Found and extracted 'income_statement'.\n","2025-08-18 19:15:48,633 - INFO - Found and extracted 'balance_sheet'.\n","2025-08-18 19:15:48,639 - INFO - Saved income_statement to /content/drive/MyDrive/data/segmented/infosys_2024_income_statement.txt\n","2025-08-18 19:15:48,644 - INFO - Saved balance_sheet to /content/drive/MyDrive/data/segmented/infosys_2024_balance_sheet.txt\n","2025-08-18 19:15:48,646 - INFO - validating the segmentation module\n","2025-08-18 19:15:48,646 - INFO - Validating balance_sheet for 2023\n","2025-08-18 19:15:48,650 - INFO - Found header Consolidated Balance Sheet in /content/drive/MyDrive/data/segmented/infosys_2023_balance_sheet.txt\n","2025-08-18 19:15:48,650 - INFO - Found Total assets: 15,312 in /content/drive/MyDrive/data/segmented/infosys_2023_balance_sheet.txt\n","2025-08-18 19:15:48,651 - INFO - Found Total equity: 9,224 in /content/drive/MyDrive/data/segmented/infosys_2023_balance_sheet.txt\n","2025-08-18 19:15:48,653 - INFO - Validation successful for /content/drive/MyDrive/data/segmented/infosys_2023_balance_sheet.txt\n","2025-08-18 19:15:48,653 - INFO - Validating income_statement for 2023\n","2025-08-18 19:15:48,657 - INFO - Found header Consolidated Statements of Comprehensive Income in /content/drive/MyDrive/data/segmented/infosys_2023_income_statement.txt\n","2025-08-18 19:15:48,658 - INFO - Found Revenues: 18,212 in /content/drive/MyDrive/data/segmented/infosys_2023_income_statement.txt\n","2025-08-18 19:15:48,659 - INFO - Found Net profit: 2,983 in /content/drive/MyDrive/data/segmented/infosys_2023_income_statement.txt\n","2025-08-18 19:15:48,660 - INFO - Validation successful for /content/drive/MyDrive/data/segmented/infosys_2023_income_statement.txt\n","2025-08-18 19:15:48,661 - INFO - Validating balance_sheet for 2024\n","2025-08-18 19:15:48,666 - INFO - Found header Consolidated Balance Sheet in /content/drive/MyDrive/data/segmented/infosys_2024_balance_sheet.txt\n","2025-08-18 19:15:48,666 - INFO - Found Total assets: 16,523 in /content/drive/MyDrive/data/segmented/infosys_2024_balance_sheet.txt\n","2025-08-18 19:15:48,668 - INFO - Found Total equity: 10,605 in /content/drive/MyDrive/data/segmented/infosys_2024_balance_sheet.txt\n","2025-08-18 19:15:48,668 - INFO - Validation successful for /content/drive/MyDrive/data/segmented/infosys_2024_balance_sheet.txt\n","2025-08-18 19:15:48,669 - INFO - Validating income_statement for 2024\n","2025-08-18 19:15:48,673 - INFO - Found header Consolidated Statements of Comprehensive Income in /content/drive/MyDrive/data/segmented/infosys_2024_income_statement.txt\n","2025-08-18 19:15:48,673 - INFO - Found Revenues: 18,562 in /content/drive/MyDrive/data/segmented/infosys_2024_income_statement.txt\n","2025-08-18 19:15:48,675 - INFO - Found Net profit: 3,169 in /content/drive/MyDrive/data/segmented/infosys_2024_income_statement.txt\n","2025-08-18 19:15:48,675 - INFO - Validation successful for /content/drive/MyDrive/data/segmented/infosys_2024_income_statement.txt\n"]}],"source":["import re\n","import os\n","import fitz  # PyMuPDF\n","from pdf2image import convert_from_path\n","from collections import Counter\n","\n","import logging\n","\n","# NEW: Imports and setup for Google Drive in Colab\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Optional: Install poppler if needed for pdf2image (run once)\n","!apt-get install poppler-utils -y\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","def normalize_number(value):\n","    \"\"\"Normalize numbers by removing commas and handling thousands/millions.\"\"\"\n","    try:\n","        value = str(value).replace(\",\", \"\").strip()\n","        # Convert millions (e.g., \"1,661\" to \"1661000000\" if in thousands)\n","        if \".\" not in value and len(value) < 6:\n","            return str(int(value) * 1_000_000)\n","        return value\n","    except ValueError:\n","        return value\n","\n","def validate_key_figures(file_path, expected_values, section_header, forbidden_keywords):\n","    \"\"\"Validate key figures, header, and section purity in a segmented file.\"\"\"\n","    try:\n","        if not os.path.exists(file_path):\n","            logger.error(f\"File not found: {file_path}\")\n","            return False\n","\n","        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","            text = f.read()\n","\n","        # Check section header\n","        if section_header.upper() not in text.upper():\n","            logger.warning(f\"Missing header {section_header} in {file_path}\")\n","            return False\n","        else:\n","            logger.info(f\"Found header {section_header} in {file_path}\")\n","\n","        # Check key figures\n","        all_found = True\n","        for key, value in expected_values.items():\n","            normalized_value = normalize_number(value)\n","            # Try direct match, regex with key-value, and standalone value\n","            if (normalized_value in text or\n","                re.search(rf\"{key}\\s*[:=]?\\s*{value}\", text, re.IGNORECASE) or\n","                re.search(rf\"\\b{normalized_value}\\b\", text, re.IGNORECASE)):\n","                logger.info(f\"Found {key}: {value} in {file_path}\")\n","            else:\n","                logger.warning(f\"Missing or incorrect {key}: {value} in {file_path}\")\n","                all_found = False\n","\n","        # Check for contamination\n","        for keyword in forbidden_keywords:\n","            if keyword.upper() in text.upper():\n","                logger.warning(f\"Found forbidden {keyword} in {file_path}\")\n","                all_found = False\n","\n","        return all_found\n","    except Exception as e:\n","        logger.error(f\"Error validating {file_path}: {e}\")\n","        return False\n","\n","def segment_report(text: str) -> dict:\n","    \"\"\"\n","    Segments the cleaned text into logical sections like balance sheet and income statement.\n","    \"\"\"\n","    logger.info(\"Segmenting report into logical sections...\")\n","    sections = {}\n","\n","    # Define start and end patterns for each section\n","    patterns = {\n","        'income_statement': re.compile(r'Consolidated Statements of Comprehensive Income for the years ended March 31, \\(Dollars in millions.*?Diluted \\(in \\$ per share\\).*?\\d+\\.\\d+', re.IGNORECASE | re.DOTALL),\n","        'balance_sheet': re.compile(r'Consolidated Balance Sheet as of March 31, \\(Dollars in millions.*?Total liabilities and equity.*?\\d{1,3}(?:,\\d{3})*', re.IGNORECASE | re.DOTALL)\n","    }\n","\n","    for section_name, pattern in patterns.items():\n","        match = pattern.search(text)\n","        if match:\n","            sections[section_name] = match.group(0).strip()\n","            logger.info(f\"Found and extracted '{section_name}'.\")\n","        else:\n","            sections[section_name] = \"Not found\"\n","            logger.warning(f\"Could not find '{section_name}'.\")\n","\n","    return sections\n","\n","def clean_processed_sheet(text):\n","    lines = text.split(\"\\n\")\n","    cleaned_lines = []\n","    seen_headers = set()\n","\n","    for line in lines:\n","        stripped = line.strip()\n","        # Remove repeated column headers\n","        if stripped in seen_headers:\n","            continue\n","        if re.match(r'^\\s*(March\\s+\\d{1,2},\\s+\\d{4}|USD.*)$', stripped):\n","            seen_headers.add(stripped)\n","\n","        # Remove \"Note\" column values (e.g., \"2.11\", \"2.18\", etc.)\n","        # Match patterns like \"2.11\", \"2.11|\", or \"2.11 \" followed by non-digits or end of line\n","        cleaned_line = re.sub(r'^\\s*\\d+\\.\\d+\\s*(?:\\||\\s|$)', '', stripped)  # Note at line start\n","        cleaned_line = re.sub(r'\\s+\\d+\\.\\d+\\s*(?:\\||\\s|$)', ' ', cleaned_line)  # Note within line\n","        cleaned_lines.append(cleaned_line.strip())\n","\n","    return \"\\n\".join(cleaned_lines)\n","\n","def main():\n","    # MODIFIED: Use Google Drive paths\n","    base_dir = '/content/drive/MyDrive/'  # Adjust if your folder structure is different\n","    files = [os.path.join(base_dir, \"data/raw/infosys_2023.pdf\"),\n","             os.path.join(base_dir, \"data/raw/infosys_2024.pdf\")]\n","\n","    os.makedirs(os.path.join(base_dir, \"data/output\"), exist_ok=True)\n","    os.makedirs(os.path.join(base_dir, \"data/clean\"), exist_ok=True)\n","    os.makedirs(os.path.join(base_dir, \"data/segmented\"), exist_ok=True)\n","\n","    for file_path in files:\n","        year = os.path.basename(file_path).split(\"_\")[1].split(\".\")[0]\n","        logger.info(f\"Processing {file_path} for year {year}\")\n","\n","        # Direct text extraction\n","        text = \"\"\n","        with fitz.open(file_path) as doc:\n","            for page in doc:\n","                text += page.get_text(\"text\") + \"\\n\"\n","\n","        # OCR fallback if no text found\n","        if not text.strip():\n","            logger.warning(f\"Direct extraction failed for {file_path}, trying OCR\")\n","            text = pdf_to_text(file_path, ocr=True)\n","\n","        if not text.strip():\n","            logger.error(f\"Failed to extract text from {file_path}\")\n","            continue\n","\n","        # Save raw text\n","        raw_text_path = os.path.join(base_dir, f\"data/output/infosys_{year}_raw.txt\")\n","        with open(raw_text_path, \"w\", encoding=\"utf-8\") as f:\n","            f.write(text)\n","        logger.info(f\"Saved raw text to {raw_text_path}\")\n","\n","        # Clean text and save\n","        cleaned_text = clean_text(text)\n","        # Apply additional cleaning to remove Note column values\n","        cleaned_text_processed = clean_processed_sheet(cleaned_text)\n","        cleaned_text_path = os.path.join(base_dir, f\"data/clean/infosys_{year}_cleaned.txt\")\n","        with open(cleaned_text_path, \"w\", encoding=\"utf-8\") as f:\n","            f.write(cleaned_text_processed)\n","        logger.info(f\"Saved cleaned text to {cleaned_text_path}\")\n","\n","        # Step 3: Segment\n","        sections = segment_report(cleaned_text_processed)\n","\n","        for name, content in sections.items():\n","            segmented_path = os.path.join(base_dir, f\"data/segmented/infosys_{year}_{name}.txt\")\n","            with open(segmented_path, \"w\", encoding=\"utf-8\") as f:\n","                f.write(content)\n","            logger.info(f\"Saved {name} to {segmented_path}\")\n","\n","def validate_steps():\n","    # MODIFIED: Use Google Drive paths\n","    base_dir = '/content/drive/MyDrive/'\n","\n","    # Define expected values and headers\n","    validation_config = [\n","        {\n","            \"file\": os.path.join(base_dir, \"data/segmented/infosys_2023_balance_sheet.txt\"),\n","            \"year\": \"2023\",\n","            \"section\": \"balance_sheet\",\n","            \"header\": \"Consolidated Balance Sheet\",\n","            \"expected_values\": {\n","                \"Total assets\": \"15,312\",  # Approximate, in USD millions\n","                \"Total equity\": \"9,224\"\n","            }\n","        },\n","        {\n","            \"file\": os.path.join(base_dir, \"data/segmented/infosys_2023_income_statement.txt\"),\n","            \"year\": \"2023\",\n","            \"section\": \"income_statement\",\n","            \"header\": \"Consolidated Statements of Comprehensive Income\",\n","            \"expected_values\": {\n","                \"Revenues\": \"18,212\",\n","                \"Net profit\": \"2,983\"\n","            }\n","        },\n","        {\n","            \"file\": os.path.join(base_dir, \"data/segmented/infosys_2024_balance_sheet.txt\"),\n","            \"year\": \"2024\",\n","            \"section\": \"balance_sheet\",\n","            \"header\": \"Consolidated Balance Sheet\",\n","            \"expected_values\": {\n","                \"Total assets\": \"16,523\",  # From prior input, in USD millions\n","                \"Total equity\": \"10,605\"\n","            }\n","        },\n","        {\n","            \"file\": os.path.join(base_dir, \"data/segmented/infosys_2024_income_statement.txt\"),\n","            \"year\": \"2024\",\n","            \"section\": \"income_statement\",\n","            \"header\": \"Consolidated Statements of Comprehensive Income\",\n","            \"expected_values\": {\n","                \"Revenues\": \"18,562\",\n","                \"Net profit\": \"3,169\"\n","            }\n","        }\n","    ]\n","\n","    # Forbidden keywords to check for contamination\n","    forbidden_keywords = [\n","        \"CASH FLOWS\",\n","        \"CHANGES IN EQUITY\",\n","        \"NOTES TO THE CONSOLIDATED\"\n","    ]\n","\n","    # Validate each file\n","    for config in validation_config:\n","        logger.info(f\"Validating {config['section']} for {config['year']}\")\n","        success = validate_key_figures(\n","            config[\"file\"],\n","            config[\"expected_values\"],\n","            config[\"header\"],\n","            forbidden_keywords\n","        )\n","        if success:\n","            logger.info(f\"Validation successful for {config['file']}\")\n","        else:\n","            logger.warning(f\"Validation failed for {config['file']}\")\n","\n","if __name__ == \"__main__\":\n","    logger.info(\"Executing the segmentation module\")\n","    main()\n","    logger.info(\"validating the segmentation module\")\n","    validate_steps()"]},{"cell_type":"markdown","metadata":{"id":"WYTEzmUJVrYB"},"source":["## 4. Data Validation (`_04_data_validate.py`)\n","\n","Garbage in, garbage out. This validation script is a crucial quality control step. It checks the segmented text files to ensure they contain the correct headers (e.g., \"Consolidated Balance Sheet\") and key financial figures (e.g., \"Total assets\"). It also checks for the presence of *forbidden keywords* to ensure that text from other sections (like \"CASH FLOWS\") hasn't accidentally leaked into our segments. This helps guarantee the quality of the data before we proceed to more complex tasks."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OL2Ijh-eVrYC","executionInfo":{"status":"ok","timestamp":1755543397053,"user_tz":-330,"elapsed":3757,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"e4e3d5c4-c7a8-492a-c9b4-3740a252c196"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Missing or incorrect 'Total assets: 15,312' in /content/drive/My Drive/data/segmented/infosys_2023_balance_sheet.txt\n","WARNING:__main__:Missing or incorrect 'Total equity: 9,224' in /content/drive/My Drive/data/segmented/infosys_2023_balance_sheet.txt\n","WARNING:__main__:Found forbidden keyword 'CASH FLOWS' in /content/drive/My Drive/data/segmented/infosys_2023_balance_sheet.txt\n","WARNING:__main__:Found forbidden keyword 'CHANGES IN EQUITY' in /content/drive/My Drive/data/segmented/infosys_2023_balance_sheet.txt\n","WARNING:__main__:❌ Validation failed for infosys_2023_balance_sheet.txt\n","WARNING:__main__:Missing or incorrect 'Revenues: 18,212' in /content/drive/My Drive/data/segmented/infosys_2023_income_statement.txt\n","WARNING:__main__:Missing or incorrect 'Net profit: 2,983' in /content/drive/My Drive/data/segmented/infosys_2023_income_statement.txt\n","WARNING:__main__:Found forbidden keyword 'CASH FLOWS' in /content/drive/My Drive/data/segmented/infosys_2023_income_statement.txt\n","WARNING:__main__:Found forbidden keyword 'CHANGES IN EQUITY' in /content/drive/My Drive/data/segmented/infosys_2023_income_statement.txt\n","WARNING:__main__:❌ Validation failed for infosys_2023_income_statement.txt\n","WARNING:__main__:Missing or incorrect 'Total assets: 16,523' in /content/drive/My Drive/data/segmented/infosys_2024_balance_sheet.txt\n","WARNING:__main__:Missing or incorrect 'Total equity: 10,605' in /content/drive/My Drive/data/segmented/infosys_2024_balance_sheet.txt\n","WARNING:__main__:Found forbidden keyword 'CASH FLOWS' in /content/drive/My Drive/data/segmented/infosys_2024_balance_sheet.txt\n","WARNING:__main__:Found forbidden keyword 'CHANGES IN EQUITY' in /content/drive/My Drive/data/segmented/infosys_2024_balance_sheet.txt\n","WARNING:__main__:❌ Validation failed for infosys_2024_balance_sheet.txt\n","WARNING:__main__:Missing or incorrect 'Revenues: 18,562' in /content/drive/My Drive/data/segmented/infosys_2024_income_statement.txt\n","WARNING:__main__:Missing or incorrect 'Net profit: 3,169' in /content/drive/My Drive/data/segmented/infosys_2024_income_statement.txt\n","WARNING:__main__:Found forbidden keyword 'CASH FLOWS' in /content/drive/My Drive/data/segmented/infosys_2024_income_statement.txt\n","WARNING:__main__:Found forbidden keyword 'CHANGES IN EQUITY' in /content/drive/My Drive/data/segmented/infosys_2024_income_statement.txt\n","WARNING:__main__:❌ Validation failed for infosys_2024_income_statement.txt\n"]},{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import re\n","import logging\n","import os\n","\n","logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n","logger = logging.getLogger(__name__)\n","\n","import re\n","import logging\n","import os\n","\n","# New imports for Google Colab\n","from google.colab import drive\n","\n","# --- Basic Configuration ---\n","logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n","logger = logging.getLogger(__name__)\n","\n","# --- Google Drive Mounting ---\n","# This will prompt you for authorization when you run it in a Colab cell.\n","drive.mount('/content/drive')\n","\n","# --- Core Validation Functions (No changes needed here) ---\n","\n","def normalize_number(value):\n","    \"\"\"Normalize numbers by removing commas.\"\"\"\n","    try:\n","        return str(value).replace(\",\", \"\").strip()\n","    except (ValueError, AttributeError):\n","        return str(value)\n","\n","def validate_key_figures(file_path, expected_values, section_header, forbidden_keywords):\n","    \"\"\"Validate key figures, header, and section purity in a segmented file.\"\"\"\n","    try:\n","        if not os.path.exists(file_path):\n","            logger.error(f\"File not found: {file_path}\")\n","            return False\n","\n","        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","            text = f.read()\n","\n","        # Check section header\n","        if section_header.upper() not in text.upper():\n","            logger.warning(f\"Missing header '{section_header}' in {file_path}\")\n","            return False\n","\n","        # Check key figures\n","        all_found = True\n","        for key, value in expected_values.items():\n","            normalized_value = normalize_number(value)\n","            # Use regex to find the key followed by the value anywhere in the text\n","            if re.search(rf\"{re.escape(key)}.*?{re.escape(normalized_value)}\", text, re.IGNORECASE | re.DOTALL):\n","                logger.info(f\"Found '{key}: {value}' in {file_path}\")\n","            else:\n","                logger.warning(f\"Missing or incorrect '{key}: {value}' in {file_path}\")\n","                all_found = False\n","\n","        # Check for contamination from other sections\n","        for keyword in forbidden_keywords:\n","            if keyword.upper() in text.upper():\n","                logger.warning(f\"Found forbidden keyword '{keyword}' in {file_path}\")\n","                all_found = False\n","\n","        return all_found\n","    except Exception as e:\n","        logger.error(f\"Error validating {file_path}: {e}\")\n","        return False\n","\n","def main_validator():\n","    # --- MODIFIED: Define base path for Google Drive ---\n","    drive_base_path = \"/content/drive/My Drive/\"\n","\n","    # --- MODIFIED: Use os.path.join to create full paths for Google Drive ---\n","    # The relative paths are now joined with your Drive's base path.\n","    validation_config = [\n","        {\n","            \"file\": os.path.join(drive_base_path, \"data/segmented/infosys_2023_balance_sheet.txt\"),\n","            \"header\": \"Consolidated Balance Sheet\",\n","            \"expected_values\": {\"Total assets\": \"15,312\", \"Total equity\": \"9,224\"}\n","        },\n","        {\n","            \"file\": os.path.join(drive_base_path, \"data/segmented/infosys_2023_income_statement.txt\"),\n","            \"header\": \"Consolidated Statements of Comprehensive Income\",\n","            \"expected_values\": {\"Revenues\": \"18,212\", \"Net profit\": \"2,983\"}\n","        },\n","        # You can add configurations for 2024 as well\n","        {\n","            \"file\": os.path.join(drive_base_path, \"data/segmented/infosys_2024_balance_sheet.txt\"),\n","            \"header\": \"Consolidated Balance Sheet\",\n","            \"expected_values\": {\"Total assets\": \"16,523\", \"Total equity\": \"10,605\"}\n","        },\n","        {\n","            \"file\": os.path.join(drive_base_path, \"data/segmented/infosys_2024_income_statement.txt\"),\n","            \"header\": \"Consolidated Statements of Comprehensive Income\",\n","            \"expected_values\": {\"Revenues\": \"18,562\", \"Net profit\": \"3,169\"}\n","        }\n","    ]\n","\n","    forbidden_keywords = [\"CASH FLOWS\", \"CHANGES IN EQUITY\"]\n","\n","    for config in validation_config:\n","        # The file path is now the full, correct path in Google Drive\n","        logger.info(f\"--- Validating {config['file']} ---\")\n","        success = validate_key_figures(\n","            config[\"file\"],\n","            config[\"expected_values\"],\n","            config[\"header\"],\n","            forbidden_keywords\n","        )\n","        if success:\n","            logger.info(f\"✅ Validation successful for {os.path.basename(config['file'])}\")\n","        else:\n","            logger.warning(f\"❌ Validation failed for {os.path.basename(config['file'])}\")\n","\n","if __name__ == \"__main__\":\n","    main_validator()"]},{"cell_type":"markdown","metadata":{"id":"VQMS8VgWVrYC"},"source":["## 5. Q&A Generation (`_05_data_qa_generate.py`)\n","\n","This script parses the validated, segmented financial statements to automatically generate question-and-answer pairs. For example, from a line item like `Revenues ... 18,562`, it creates a question: \"What was the Revenues in 2024?\" and an answer: \"For the year 2024, the Revenues was $18,562 million.\" These Q&A pairs can be invaluable for fine-tuning a language model or for creating a test set to evaluate the final retrieval system's performance."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"8aLI5Ys0VrYC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755544692514,"user_tz":-330,"elapsed":1721,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"5010c7c6-0c1c-4098-c2ad-b6f94a77ef66"},"outputs":[{"output_type":"stream","name":"stderr","text":["2025-08-18 19:18:13,476 - INFO - Successfully generated 79 Q&A pairs to /content/drive/MyDrive/data/qa/financial_qa_pairs.json\n"]},{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","import re\n","import json\n","import logging\n","\n","# NEW: Imports and setup for Google Drive in Colab\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n","logger = logging.getLogger(__name__)\n","\n","def generate_qa_from_text(text_content, file_path):\n","    \"\"\"Generates Q&A pairs from a block of financial text.\"\"\"\n","    qa_pairs = []\n","    year_match = re.search(r'_(\\d{4})_', file_path)\n","    if not year_match:\n","        return []\n","    main_year = int(year_match.group(1))\n","\n","    text_data = ' '.join(text_content.split())\n","    header_pattern = re.compile(r'.*?\\(Dollars in millions.*?data\\)\\s*Note\\s*', re.IGNORECASE)\n","    text_data = header_pattern.sub('', text_data)\n","\n","    val_pattern = r'[\\d,.-]+|\\([\\d,.-]+\\)'\n","    delimiter_pattern_2_col = re.compile(f'\\\\s+({val_pattern})\\\\s+({val_pattern})\\\\s*')\n","    parts = delimiter_pattern_2_col.split(text_data)\n","\n","    i = 0\n","    while i < len(parts):\n","        item_name = parts[i].strip().lower().rstrip('.--: ')\n","\n","        num_values = 0\n","        if (i + 1 < len(parts)) and re.fullmatch(val_pattern, parts[i+1].strip()):\n","            num_values = 1\n","            if (i + 2 < len(parts)) and re.fullmatch(val_pattern, parts[i+2].strip()):\n","                num_values = 2\n","\n","        if item_name and num_values > 0:\n","            values = parts[i+1 : i+1+num_values]\n","            if values[0]:\n","                question = f\"What was the {item_name} in {main_year}?\"\n","                answer = f\"For the year {main_year}, the {item_name} was ${values[0]} million.\"\n","                qa_pairs.append({\"question\": question, \"answer\": answer})\n","            i += (1 + num_values)\n","        else:\n","            i += 1\n","\n","    return qa_pairs\n","\n","def main_qa_generator():\n","    # MODIFIED: Use Google Drive paths\n","    base_dir = '/content/drive/MyDrive/'  # Adjust if your folder structure is different\n","    input_files = [\n","        os.path.join(base_dir, \"data/segmented/infosys_2023_balance_sheet.txt\"),\n","        os.path.join(base_dir, \"data/segmented/infosys_2024_balance_sheet.txt\"),\n","        os.path.join(base_dir, \"data/segmented/infosys_2023_income_statement.txt\"),\n","        os.path.join(base_dir, \"data/segmented/infosys_2024_income_statement.txt\")\n","    ]\n","\n","    all_qa_pairs = []\n","    for file_path in input_files:\n","        try:\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                content = f.read().strip()\n","            if content:\n","                qa_pairs = generate_qa_from_text(content, file_path)\n","                all_qa_pairs.extend(qa_pairs)\n","        except Exception as e:\n","            logger.error(f\"Error processing {file_path}: {e}\")\n","\n","    output_path = os.path.join(base_dir, \"data/qa/financial_qa_pairs.json\")\n","    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","    with open(output_path, 'w', encoding='utf-8') as f:\n","        json.dump(all_qa_pairs, f, indent=4)\n","\n","    logger.info(f\"Successfully generated {len(all_qa_pairs)} Q&A pairs to {output_path}\")\n","\n","if __name__ == \"__main__\":\n","    main_qa_generator()"]},{"cell_type":"markdown","metadata":{"id":"OhGl_AMjVrYD"},"source":["## 6. Data Chunking (`_06_data_create_chunks.py`)\n","\n","To prepare our data for modern retrieval systems (like semantic search), we need to break it down into small, digestible pieces called \"chunks.\" This script takes the financial statements and converts each line item into a self-contained sentence. For example, a row for 'Total assets' becomes a chunk like: `\"For the year 2024, the total assets was $16,523 million.\"`. Each chunk is given a unique ID and enriched with metadata (like the source file, section, and year), which is crucial for filtering and context-aware retrieval. All chunks are saved into a single JSON file for easy access in the next step."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"FA55XgGvVrYD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755544756481,"user_tz":-330,"elapsed":1711,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"d6584413-e8e0-44ca-eac1-01711260c33e"},"outputs":[{"output_type":"stream","name":"stderr","text":["2025-08-18 19:19:17,258 - INFO - Creating sentence chunks for infosys_2023_balance_sheet.txt\n","2025-08-18 19:19:17,281 - INFO - Generated 44 sentence chunks from infosys_2023_balance_sheet.txt.\n","2025-08-18 19:19:17,284 - INFO - Creating sentence chunks for infosys_2024_balance_sheet.txt\n","2025-08-18 19:19:17,306 - INFO - Generated 44 sentence chunks from infosys_2024_balance_sheet.txt.\n","2025-08-18 19:19:17,309 - INFO - Creating sentence chunks for infosys_2023_income_statement.txt\n","2025-08-18 19:19:17,321 - INFO - Generated 34 sentence chunks from infosys_2023_income_statement.txt.\n","2025-08-18 19:19:17,324 - INFO - Creating sentence chunks for infosys_2024_income_statement.txt\n","2025-08-18 19:19:17,336 - INFO - Generated 36 sentence chunks from infosys_2024_income_statement.txt.\n","2025-08-18 19:19:17,436 - INFO - ✅ Successfully saved all 158 chunks to /content/drive/My Drive/data/chunks/all_sentence_chunks.json\n"]},{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","import logging\n","import re\n","from uuid import uuid4\n","import json\n","\n","# New imports for Google Colab\n","from google.colab import drive\n","\n","# --- Basic Configuration ---\n","logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n","logger = logging.getLogger(__name__)\n","\n","# --- Google Drive Mounting ---\n","# This will prompt you for authorization when you run it in a Colab cell.\n","drive.mount('/content/drive')\n","\n","# --- Core Chunking Functions (No changes needed here) ---\n","\n","def create_sentence_chunks(text_content, file_path):\n","    \"\"\"Processes a block of financial text into meaningful, sentence-like chunks.\"\"\"\n","    logger.info(f\"Creating sentence chunks for {os.path.basename(file_path)}\")\n","\n","    file_name = os.path.basename(file_path)\n","    section = \"balance_sheet\" if \"balance_sheet\" in file_name.lower() else \"income_statement\"\n","    year_match = re.search(r'_(\\d{4})_', file_name)\n","    if not year_match:\n","        return []\n","    main_year = int(year_match.group(1))\n","    # Create tuples of years to process for each line item\n","    years = (main_year, main_year - 1)\n","\n","    generated_chunks = []\n","    # Normalize and clean the text block for easier parsing\n","    text_data = ' '.join(text_content.split())\n","    header_pattern = re.compile(r'.*?\\(Dollars in millions.*?data\\)\\s*Note\\s*', re.IGNORECASE)\n","    text_data = header_pattern.sub('', text_data)\n","\n","    # Regex to find financial values\n","    val_pattern = r'[\\d,.-]+|\\([\\d,.-]+\\)'\n","    # Regex to split text by line items followed by two values\n","    delimiter_pattern = re.compile(f'\\\\s+({val_pattern})\\\\s+({val_pattern})\\\\s*')\n","    parts = delimiter_pattern.split(text_data)\n","\n","    i = 0\n","    while i < len(parts):\n","        item_name = parts[i].strip().lower().rstrip('.-–: ')\n","\n","        # Check if the next parts are valid numbers\n","        num_values = 0\n","        if (i + 1 < len(parts)) and re.fullmatch(val_pattern, parts[i+1].strip()):\n","            num_values = 1\n","            if (i + 2 < len(parts)) and re.fullmatch(val_pattern, parts[i+2].strip()):\n","                num_values = 2\n","\n","        # If a valid item and values are found, create sentence chunks\n","        if item_name and num_values > 0:\n","            values = parts[i+1 : i+1+num_values]\n","\n","            for j, year in enumerate(years):\n","                if j < len(values):\n","                    value_str = values[j]\n","                    clean_val = value_str.strip().replace(\",\", \"\")\n","                    sentence = f\"For the year {year}, the {item_name} was ${clean_val} million.\"\n","\n","                    # Append the chunk with its metadata\n","                    generated_chunks.append({\n","                        \"id\": str(uuid4()),\n","                        \"text\": sentence,\n","                        \"metadata\": {\n","                            \"file_path\": file_path,\n","                            \"section\": section,\n","                            \"year\": year,\n","                            \"original_item\": item_name\n","                        }\n","                    })\n","            i += (1 + num_values)\n","        else:\n","            i += 1\n","\n","    logger.info(f\"Generated {len(generated_chunks)} sentence chunks from {file_name}.\")\n","    return generated_chunks\n","\n","def main_chunker():\n","    # --- MODIFIED: Define base path for Google Drive ---\n","    drive_base_path = \"/content/drive/My Drive/\"\n","\n","    # --- MODIFIED: Create full input file paths for Google Drive ---\n","    segmented_dir = os.path.join(drive_base_path, \"data/segmented\")\n","    input_files = [\n","        os.path.join(segmented_dir, \"infosys_2023_balance_sheet.txt\"),\n","        os.path.join(segmented_dir, \"infosys_2024_balance_sheet.txt\"),\n","        os.path.join(segmented_dir, \"infosys_2023_income_statement.txt\"),\n","        os.path.join(segmented_dir, \"infosys_2024_income_statement.txt\")\n","    ]\n","\n","    # --- MODIFIED: Define the output folder in Google Drive ---\n","    output_folder = os.path.join(drive_base_path, \"data/chunks\")\n","    os.makedirs(output_folder, exist_ok=True)\n","\n","    all_chunks = []\n","    for file_path in input_files:\n","        try:\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                text = f.read().strip()\n","            if text:\n","                chunks = create_sentence_chunks(text, file_path)\n","                all_chunks.extend(chunks)\n","        except FileNotFoundError:\n","            logger.error(f\"File not found: {file_path}. Skipping.\")\n","        except Exception as e:\n","            logger.error(f\"Error processing {file_path}: {str(e)}\")\n","\n","    # --- MODIFIED: Define the full output path for the JSON file ---\n","    output_json_path = os.path.join(output_folder, \"all_sentence_chunks.json\")\n","    with open(output_json_path, 'w', encoding='utf-8') as f:\n","        json.dump(all_chunks, f, indent=4)\n","\n","    logger.info(f\"✅ Successfully saved all {len(all_chunks)} chunks to {output_json_path}\")\n","\n","if __name__ == \"__main__\":\n","    main_chunker()"]},{"cell_type":"markdown","metadata":{"id":"M44NAExLVrYD"},"source":["## 7. Embedding and Indexing (`_07_data_create_embedding.py`)\n","\n","This is where we build the core of our retrieval system. The script performs two main tasks:\n","\n","1.  **Embedding**: It loads the sentence chunks and uses a `SentenceTransformer` model (`intfloat/e5-small-v2`) to convert each chunk's text into a high-dimensional numerical vector (an embedding). These embeddings capture the semantic meaning of the text.\n","\n","2.  **Indexing**: It builds two different types of search indexes:\n","    * **FAISS Index**: A library for efficient similarity search. We use it to create an index of our text embeddings, allowing us to quickly find the most semantically similar chunks to a user's query.\n","    * **BM25 Index**: A classical keyword-based search algorithm. This index is great for matching specific terms and numbers, complementing the semantic search of FAISS.\n","\n","Both indexes are saved to disk for later use."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"pmt1IVBbVrYD","colab":{"base_uri":"https://localhost:8080/","height":284,"referenced_widgets":["ee1851d8ba7544a0abebf5e5cc8cfb06","f48eab17276041e7853257d5cc451794","c3a10eda7f8a405e9e6ba799cb1e985a","32f736ad4bba4760b539f53d20486a22","e8b2167cfcac4146942fe3830a387df5","50f4a4b55d3e45da9211b37b1701131d","1375bf9260b147fa8750fe31473e15be","e640f7f4e7e744a1aad47e754b48ad21","c3251c7434a94dd6b12ad91726833289","c42bda3799074568bdf38ad0968acd1e","8d139f3f9a93479cb36bd441c8933a9d"]},"executionInfo":{"status":"ok","timestamp":1755544894004,"user_tz":-330,"elapsed":2958,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"72251394-c450-45bc-eb7f-3668e7d2a586"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","2025-08-18 19:21:33,785 - INFO - --- 🚀 Starting Embedding and Indexing Pipeline 🚀 ---\n","2025-08-18 19:21:33,786 - INFO - Reading chunks from /content/drive/My Drive/data/chunks/all_sentence_chunks.json\n","2025-08-18 19:21:33,790 - INFO - Embedding chunks with intfloat/e5-small-v2...\n","2025-08-18 19:21:33,795 - INFO - Use pytorch device_name: cuda:0\n","2025-08-18 19:21:33,795 - INFO - Load pretrained SentenceTransformer: intfloat/e5-small-v2\n"]},{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"display_data","data":{"text/plain":["Batches:   0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee1851d8ba7544a0abebf5e5cc8cfb06"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-08-18 19:21:34,928 - INFO - Building FAISS index...\n","2025-08-18 19:21:34,944 - INFO - Saved FAISS index with 158 vectors to /content/drive/My Drive/data/retrieval\n","2025-08-18 19:21:34,945 - INFO - Building BM25 index...\n","2025-08-18 19:21:34,986 - INFO - Saved BM25 index with 158 documents to /content/drive/My Drive/data/retrieval\n","2025-08-18 19:21:34,988 - INFO - --- ✅ Pipeline Finished Successfully ---\n"]}],"source":["import os\n","import logging\n","import pickle\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from rank_bm25 import BM25Okapi\n","import faiss\n","import nltk\n","from nltk.tokenize import word_tokenize\n","import json\n","nltk.download('punkt_tab')\n","# New imports for Google Colab\n","from google.colab import drive\n","\n","# --- Initial Setup ---\n","# Download necessary NLTK data\n","nltk.download('punkt', quiet=True)\n","# Configure logging\n","logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n","logger = logging.getLogger(__name__)\n","\n","# --- Google Drive Mounting ---\n","# This will prompt you for authorization when you run it in a Colab cell.\n","drive.mount('/content/drive')\n","\n","# --- Core Indexing Functions ---\n","\n","def load_chunks_from_json(file_path):\n","    \"\"\"Loads chunks from the consolidated JSON file.\"\"\"\n","    logger.info(f\"Reading chunks from {file_path}\")\n","    try:\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            return json.load(f)\n","    except FileNotFoundError:\n","        logger.error(f\"Error: Chunks file not found at {file_path}.\")\n","        return []\n","    except Exception as e:\n","        logger.error(f\"Error reading chunks from {file_path}: {e}\")\n","        return []\n","\n","def embed_chunks(chunks, model_name=\"intfloat/e5-small-v2\"):\n","    \"\"\"Embeds chunks using a sentence transformer model.\"\"\"\n","    logger.info(f\"Embedding chunks with {model_name}...\")\n","    try:\n","        model = SentenceTransformer(model_name)\n","        # Prepend \"passage: \" as recommended by the e5 model documentation for documents\n","        texts = [f\"passage: {chunk['text']}\" for chunk in chunks]\n","        return model.encode(texts, show_progress_bar=True, normalize_embeddings=True)\n","    except Exception as e:\n","        logger.error(f\"Error embedding chunks: {e}\")\n","        return np.array([])\n","\n","def build_faiss_index(embeddings, chunk_ids, output_dir):\n","    \"\"\"Builds and saves a FAISS index for semantic search.\"\"\"\n","    logger.info(\"Building FAISS index...\")\n","    os.makedirs(output_dir, exist_ok=True)\n","    dimension = embeddings.shape[1]\n","    # Using IndexFlatIP for cosine similarity with normalized embeddings\n","    index = faiss.IndexFlatIP(dimension)\n","    index.add(embeddings.astype('float32'))\n","\n","    faiss.write_index(index, os.path.join(output_dir, \"faiss_index.bin\"))\n","    with open(os.path.join(output_dir, \"faiss_index_ids.pkl\"), 'wb') as f:\n","        pickle.dump(chunk_ids, f)\n","    logger.info(f\"Saved FAISS index with {index.ntotal} vectors to {output_dir}\")\n","\n","def build_bm25_index(chunks, output_dir):\n","    \"\"\"Builds and saves a BM25 index for keyword search.\"\"\"\n","    logger.info(\"Building BM25 index...\")\n","    os.makedirs(output_dir, exist_ok=True)\n","    tokenized_chunks = [word_tokenize(chunk[\"text\"].lower()) for chunk in chunks]\n","    bm25 = BM25Okapi(tokenized_chunks)\n","\n","    with open(os.path.join(output_dir, \"bm25_index.pkl\"), 'wb') as f:\n","        pickle.dump(bm25, f)\n","    logger.info(f\"Saved BM25 index with {len(tokenized_chunks)} documents to {output_dir}\")\n","\n","def main_indexer():\n","    \"\"\"Main function to run the full embedding and indexing pipeline.\"\"\"\n","    logger.info(\"--- 🚀 Starting Embedding and Indexing Pipeline 🚀 ---\")\n","\n","    # --- MODIFIED: Define paths for Google Drive ---\n","    drive_base_path = \"/content/drive/My Drive/\"\n","    chunks_file_path = os.path.join(drive_base_path, \"data/chunks/all_sentence_chunks.json\")\n","    retrieval_output_dir = os.path.join(drive_base_path, \"data/retrieval\")\n","\n","    # Step 1: Load the processed chunks\n","    chunks = load_chunks_from_json(chunks_file_path)\n","    if not chunks:\n","        logger.error(\"No chunks found. Please run the chunking script first. Exiting.\")\n","        return\n","\n","    # Step 2: Generate embeddings for the chunks\n","    embeddings = embed_chunks(chunks)\n","    if embeddings.size == 0:\n","        logger.error(\"No embeddings were generated. Exiting.\")\n","        return\n","\n","    # Step 3: Build and save the search indexes\n","    chunk_ids = [chunk[\"id\"] for chunk in chunks]\n","    build_faiss_index(embeddings, chunk_ids, retrieval_output_dir)\n","    build_bm25_index(chunks, retrieval_output_dir)\n","\n","    logger.info(\"--- ✅ Pipeline Finished Successfully ---\")\n","\n","if __name__ == \"__main__\":\n","    main_indexer()"]},{"cell_type":"markdown","metadata":{"id":"dYDVYNsTVrYE"},"source":["## 8. Data Loading Utilities (`_08_data_load_data.py`)\n","\n","Finally, this script provides a set of simple, reusable functions to load the artifacts we created in the previous steps. It contains functions to load:\n","- The JSON file of sentence chunks.\n","- The FAISS index and its corresponding chunk IDs.\n","- The BM25 index.\n","\n","These functions will be used by the final application (e.g., a chatbot or an API) to quickly load the necessary data into memory to perform searches and answer user questions."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"JvQRfoKKVrYE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755544901580,"user_tz":-330,"elapsed":6,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"53f65d2e-1632-40f1-f1cc-8f4f47623323"},"outputs":[{"output_type":"stream","name":"stderr","text":["2025-08-18 19:21:42,561 - INFO - This module provides data loading functions. Example usage:\n"]}],"source":["import logging\n","from typing import List, Dict, Tuple\n","import json\n","import pickle\n","import faiss\n","from rank_bm25 import BM25Okapi\n","\n","logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n","logger = logging.getLogger(__name__)\n","\n","def load_chunks(file_path: str = \"data/chunks/all_sentence_chunks.json\") -> List[Dict]:\n","    logger.info(f\"Loading chunks from: {file_path}\")\n","    try:\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            return json.load(f)\n","    except FileNotFoundError:\n","        logger.error(f\"Error: File not found at {file_path}.\")\n","        return []\n","    except Exception as e:\n","        logger.error(f\"An unexpected error occurred: {str(e)}\")\n","        return []\n","\n","def load_faiss_index(index_path: str = \"data/retrieval/faiss_index.bin\") -> Tuple[faiss.Index, List[int]]:\n","    logger.info(f\"Loading FAISS index from {index_path}\")\n","    try:\n","        index = faiss.read_index(index_path)\n","        id_path = index_path.replace(\".bin\", \"_ids.pkl\")\n","        with open(id_path, 'rb') as f:\n","            chunk_ids = pickle.load(f)\n","        logger.info(f\"Loaded FAISS index with {index.ntotal} vectors.\")\n","        return index, chunk_ids\n","    except Exception as e:\n","        logger.error(f\"Failed to load FAISS index: {e}\")\n","        return None, None\n","\n","def load_bm25_index(index_path: str = \"data/retrieval/bm25_index.pkl\") -> BM25Okapi:\n","    logger.info(f\"Loading BM25 index from {index_path}\")\n","    try:\n","        with open(index_path, 'rb') as f:\n","            return pickle.load(f)\n","    except Exception as e:\n","        logger.error(f\"Failed to load BM25 index: {e}\")\n","        return None\n","\n","if __name__ == \"__main__\":\n","    logger.info(\"This module provides data loading functions. Example usage:\")\n","\n","    # Example of how to use the functions:\n","    # chunks = load_chunks()\n","    # if chunks:\n","    #     logger.info(f\"Loaded {len(chunks)} chunks.\")\n","\n","    # faiss_index, faiss_ids = load_faiss_index()\n","    # if faiss_index:\n","    #     logger.info(\"FAISS index loaded.\")\n","\n","    # bm25_index = load_bm25_index()\n","    # if bm25_index:\n","    #     logger.info(\"BM25 index loaded.\")"]},{"cell_type":"markdown","source":["# Financial Document Analysis and Q&A Pipeline"],"metadata":{"id":"yO511f2MojHN"}},{"cell_type":"markdown","source":["### **Hybrid Retrieval Pipeline**\n","\n","For each user query, the following steps are performed:\n","1.  **Preprocess**: The query is cleaned, converted to lowercase, and stopwords are removed.\n","2.  **Generate Query Embedding**: A numerical vector representation of the query is created.\n","3.  **Retrieve Top-N Chunks**: The most relevant chunks are retrieved from the knowledge base using two methods:\n","    * **Dense Retrieval**: Based on vector similarity (e.g., cosine similarity).\n","    * **Sparse Retrieval**: Based on keyword matching using an algorithm like BM25.\n","4.  **Combine Results**: The results from both dense and sparse retrieval are combined, either by taking the union of the two sets or by using a weighted score fusion to rank the combined results.\n","\n","---\n","\n","### **Advanced RAG Technique (Select One)**\n","\n","Based on your group number, you will implement one of the following advanced Retrieval-Augmented Generation (RAG) techniques:\n","\n","| Remainder (Group Number mod 5) | Advanced Technique                | Description                                                                     | **Hybrid Search** | Combine BM25 keyword search with dense vector retrieval for a balance of recall and precision. |\n","|                                \n","---\n","\n","### **Response Generation**\n","\n","To generate the final answer, follow these steps:\n","1.  **Use a small, open-source generative model** (e.g., DistilGPT2, GPT-2 Small, or Llama-2 7B if available).\n","2.  **Concatenate the retrieved passages and the user query** to form the input prompt for the model.\n","3.  **Limit the total input tokens** to ensure the prompt fits within the model's context window.\n","\n","---\n","\n","### **Guardrail Implementation**\n","\n","Implement one of the following guardrails to improve the reliability and safety of your system:\n","\n","* **Input-side Guardrail**: Validate user queries to filter out irrelevant, inappropriate, or harmful inputs before they are processed.\n","* **Output-side Guardrail**: Check the generated response to filter or flag any hallucinated (non-factual) or undesirable outputs before they are shown to the user."],"metadata":{"id":"R5BixNJholpf"}},{"cell_type":"markdown","source":["## 8. RAG  Retrieval"],"metadata":{"id":"jk1iC5dppkwJ"}},{"cell_type":"code","source":["import logging\n","import numpy as np\n","import streamlit as st\n","import os\n","import sys\n","import re  # Ensure re is imported at the top\n","import time\n","from typing import List, Dict, Tuple\n","\n","from sentence_transformers import SentenceTransformer\n","from transformers import pipeline, AutoTokenizer\n","import json\n","import pickle\n","import faiss\n","from rank_bm25 import BM25Okapi\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import nltk\n","\n","# NEW: Imports and setup for Google Drive in Colab\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set the base directory in Google Drive (adjust if your project folder is different, e.g., '/content/drive/MyDrive/your_project_folder/')\n","base_dir = '/content/drive/MyDrive/'\n","os.chdir(base_dir)  # Change working directory to Google Drive base to handle relative paths\n","\n","sys.path.append(os.path.join(base_dir, os.path.dirname(os.path.dirname(os.path.abspath('__file__')))))  # Adjusted for potential __file__ issues; may need tweaking based on structure\n","from preprocess._08_data_load_data import load_chunks, load_faiss_index, load_bm25_index\n","\n","\n","\n","# =============================\n","# Initial Setup\n","# =============================\n","nltk.download('punkt', quiet=True)\n","nltk.download('stopwords', quiet=True)\n","nltk.download('averaged_perceptron_tagger', quiet=True)\n","\n","logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n","logger = logging.getLogger(__name__)\n","\n","class RetrievalConfig:\n","    INITIAL_CANDIDATE_COUNT = 80\n","    BM25_TOP_MULTIPLIER = 2\n","    DENSE_WEIGHT = 0.5\n","    SPARSE_WEIGHT = 0.5\n","    FINAL_TOP_K = 8\n","    CTX_MAX_TOKENS = 900\n","    EMB_MODEL_NAME = \"intfloat/e5-small-v2\"\n","    GEN_MODEL_NAME = \"distilgpt2\"\n","    FAISS_INDEX_IS_INNER_PRODUCT = True\n","\n","# =============================\n","# Utilities\n","# =============================\n","\n","def preprocess_query(query: str) -> Tuple[str, List[str]]:\n","    stop_words = set(stopwords.words('english'))\n","    tokens = word_tokenize(query.lower())\n","    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n","    return \" \".join(filtered_tokens), filtered_tokens\n","\n","def _normalize_minmax(d: Dict[int, float]) -> Dict[int, float]:\n","    if not d:\n","        return d\n","    vals = list(d.values())\n","    vmin, vmax = min(vals), max(vals)\n","    if vmax - vmin < 1e-9:\n","        return {k: 1.0 for k in d}\n","    return {k: (v - vmin) / (vmax - vmin) for k, v in d.items()}\n","\n","def _faiss_scores_to_similarity(distances: np.ndarray) -> np.ndarray:\n","    if RetrievalConfig.FAISS_INDEX_IS_INNER_PRODUCT:\n","        return distances\n","    return -distances\n","\n","# =============================\n","# Hybrid Retrieval (Deduplicated)\n","# =============================\n","\n","def hybrid_retrieval(query: str,\n","                     chunks: List[Dict],\n","                     faiss_index: faiss.Index,\n","                     bm25: BM25Okapi,\n","                     chunk_ids: List[int],\n","                     emb_model: SentenceTransformer) -> List[Dict]:\n","    logger.info(f\"Hybrid retrieval for query: {query}\")\n","\n","    processed_query, query_tokens = preprocess_query(query)\n","    q_emb = emb_model.encode([f\"query: {processed_query}\"], show_progress_bar=False, normalize_embeddings=True)[0]\n","\n","    distances, indices = faiss_index.search(q_emb.reshape(1, -1), RetrievalConfig.INITIAL_CANDIDATE_COUNT)\n","    sim = _faiss_scores_to_similarity(distances)[0]\n","    dense_scores = {chunk_ids[i]: float(sim[j]) for j, i in enumerate(indices[0]) if i != -1}\n","\n","    bm25_scores = bm25.get_scores(query_tokens)\n","    top_bm25_idx = np.argsort(bm25_scores)[::-1][:RetrievalConfig.INITIAL_CANDIDATE_COUNT * RetrievalConfig.BM25_TOP_MULTIPLIER]\n","    sparse_scores = {chunk_ids[i]: float(bm25_scores[i]) for i in top_bm25_idx}\n","\n","    dn = _normalize_minmax(dense_scores)\n","    sn = _normalize_minmax(sparse_scores)\n","\n","    combined = {cid: RetrievalConfig.DENSE_WEIGHT * dn.get(cid, 0.0) +\n","                        RetrievalConfig.SPARSE_WEIGHT * sn.get(cid, 0.0)\n","                for cid in set(dn) | set(sn)}\n","\n","    top_ids = sorted(combined, key=combined.get, reverse=True)\n","    seen_texts = set()\n","    candidate_chunks = []\n","    for cid in top_ids:\n","        chunk = next((c for c in chunks if c[\"id\"] == cid), None)\n","        if chunk and chunk[\"text\"].strip() not in seen_texts:\n","            seen_texts.add(chunk[\"text\"].strip())\n","            candidate_chunks.append(chunk)\n","        if len(candidate_chunks) >= RetrievalConfig.FINAL_TOP_K:\n","            break\n","\n","    logger.info(f\"Retrieved {len(candidate_chunks)} unique chunks for generation.\")\n","    for i, chunk in enumerate(candidate_chunks, 1):\n","        logger.info(f\"Chunk {i}: {chunk['text'][:200]}... (Source: {chunk['metadata'].get('file_path', 'unknown')})\")\n","    return candidate_chunks\n","\n","# =============================\n","# RAG Generation\n","# =============================\n","\n","def rag_generate(query: str, retrieved_chunks: List[Dict], cfg: RetrievalConfig) -> str:\n","    logger.info(\"Generating answer from merged chunks...\")\n","    if not retrieved_chunks:\n","        return \"No relevant information was found to generate an answer.\"\n","\n","    # Updated regex to match full financial numbers\n","    number_pattern = re.compile(\n","        r\"\\$?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d+)?(?:\\s*(?:million|billion|mn|bn|m|b))\\b\",\n","        re.IGNORECASE\n","    )\n","    year_pattern = re.compile(r\"\\b(19|20)\\d{2}\\b\")\n","    keyword_variants = [\n","        \"total assets\", \"total asset\", \"assets total\", \"total liabilities\",\n","        \"total equity\", \"cash and cash equivalents\", \"revenues\", \"net profit\",\n","        \"income tax expense\"\n","    ]\n","\n","    def find_number_near_keyword(chunks, keywords, window_chars=200):\n","        # Prioritize chunks containing the query's main keyword\n","        query_keywords = [kw for kw in keywords if kw in query.lower()]\n","        for chunk in chunks:\n","            txt = chunk.get(\"text\", \"\").lower()\n","            src = chunk.get(\"metadata\", {}).get(\"file_path\", \"unknown\")\n","            # Log chunk metadata for debugging\n","            logger.debug(f\"Processing chunk: {txt[:200]}... (Source: {src})\")\n","            # Check for query-specific keywords first\n","            for kw in query_keywords:\n","                idx = txt.find(kw)\n","                if idx != -1:\n","                    start = max(0, idx - 50)\n","                    end = min(len(txt), idx + window_chars)\n","                    window = txt[start:end]\n","                    matches = number_pattern.finditer(window)\n","                    for m in matches:\n","                        val = m.group(0).strip()\n","                        # Log all matches for debugging\n","                        logger.debug(f\"Found potential number: {val} in window: {window[:100]}...\")\n","                        # Check if the number is part of a year\n","                        if not year_pattern.search(txt[max(0, m.start() - 10):m.end() + 10]):\n","                            # Verify metadata consistency\n","                            if \"2023\" in query.lower() and \"2023\" in txt and \"2024\" in src:\n","                                logger.warning(f\"Metadata mismatch: 2023 data in {src}\")\n","                            logger.info(f\"Extracted number: {val} from chunk: {txt[:200]}...\")\n","                            return val, chunk\n","        # Fallback to any chunk with any keyword\n","        for chunk in chunks:\n","            txt = chunk.get(\"text\", \"\").lower()\n","            src = chunk.get(\"metadata\", {}).get(\"file_path\", \"unknown\")\n","            logger.debug(f\"Processing chunk: {txt[:200]}... (Source: {src})\")\n","            for kw in keywords:\n","                idx = txt.find(kw)\n","                if idx != -1:\n","                    start = max(0, idx - 50)\n","                    end = min(len(txt), idx + window_chars)\n","                    window = txt[start:end]\n","                    matches = number_pattern.finditer(window)\n","                    for m in matches:\n","                        val = m.group(0).strip()\n","                        logger.debug(f\"Found potential number: {val} in window: {window[:100]}...\")\n","                        if not year_pattern.search(txt[max(0, m.start() - 10):m.end() + 10]):\n","                            if \"2023\" in query.lower() and \"2023\" in txt and \"2024\" in src:\n","                                logger.warning(f\"Metadata mismatch: 2023 data in {src}\")\n","                            logger.info(f\"Extracted number: {val} from chunk: {txt[:200]}...\")\n","                            return val, chunk\n","        return None, None\n","\n","    tokenizer = AutoTokenizer.from_pretrained(cfg.GEN_MODEL_NAME)\n","    context_parts = []\n","    token_budget = cfg.CTX_MAX_TOKENS\n","    for ch in retrieved_chunks:\n","        t = ch.get(\"text\", \"\").strip()\n","        tokens = tokenizer(t, return_tensors='pt')['input_ids'].shape[1]\n","        if tokens <= token_budget:\n","            context_parts.append(t)\n","            token_budget -= tokens\n","        if token_budget <= 0:\n","            break\n","    context = \"\\n\\n\".join(context_parts)\n","\n","    numeric_query = bool(re.search(r\"\\b(19|20)\\d{2}\\b\", query)) or any(w in query.lower() for w in [\"assets\", \"revenue\", \"profit\", \"income\", \"liabilities\", \"cash\"])\n","    if numeric_query:\n","        match_text, match_chunk = find_number_near_keyword(retrieved_chunks, keyword_variants)\n","        if match_text:\n","            src = match_chunk.get(\"metadata\", {}).get(\"file_path\", \"unknown\")\n","            logger.info(f\"Numeric extraction success: '{match_text}' from {src}\")\n","            return match_text\n","\n","    prompt = (\n","        \"You are a precise financial assistant. Answer ONLY using the exact words or numbers from the context.\\n\"\n","        \"If the exact answer is not present, reply 'Not found'. Do not invent numbers.\\n\\n\"\n","        f\"Context:\\n{context}\\n\\n\"\n","        f\"Question: {query}\\nAnswer:\"\n","    )\n","\n","    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=1024)\n","    truncated_prompt = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n","\n","    try:\n","        generator = pipeline('text-generation', model=cfg.GEN_MODEL_NAME, device=-1)\n","        response = generator(\n","            truncated_prompt,\n","            max_new_tokens=120,\n","            do_sample=False,\n","            num_return_sequences=1,\n","            pad_token_id=tokenizer.eos_token_id,\n","            eos_token_id=tokenizer.eos_token_id\n","        )[0]['generated_text']\n","        answer = response.replace(truncated_prompt, \"\").strip().split('\\n')[0].strip()\n","        logger.info(f\"Generated answer: {answer}\")\n","        return answer\n","    except ValueError as ve:\n","        logger.error(f\"Model loading error: {ve}\")\n","        return \"Failed to load the generative model.\"\n","    except RuntimeError as excp:\n","        logger.error(f\"Generation error: {excp}\")\n","        return \"An error occurred during answer generation.\"\n","\n","# =============================\n","# Streamlit App\n","# =============================\n","\n","def main():\n","    st.set_page_config(layout=\"wide\")\n","    st.title(\"Infosys Financial RAG System 📈 (Hybrid Retrieval Only)\")\n","    st.write(\"Ask questions about Infosys's financial statements. Hybrid retrieval = BM25 + Dense.\")\n","\n","    @st.cache_resource(show_spinner=False)\n","    def load_resources():\n","        try:\n","            emb_model = SentenceTransformer(RetrievalConfig.EMB_MODEL_NAME)\n","            chunks = load_chunks()\n","            faiss_index, chunk_ids = load_faiss_index()\n","            bm25 = load_bm25_index()\n","            if faiss_index is None or bm25 is None or not chunks:\n","                return None, None, None, None, None\n","            return emb_model, chunks, faiss_index, chunk_ids, bm25\n","        except Exception as e:\n","            st.error(f\"Resource loading error: {e}\")\n","            return None, None, None, None, None\n","\n","    resources = load_resources()\n","    if not resources or len(resources) < 5:\n","        st.error(\"Failed to load one or more critical resources.\")\n","        return\n","\n","    emb_model, chunks, faiss_index, chunk_ids, bm25 = resources\n","    if not all([emb_model, chunks, faiss_index, chunk_ids, bm25]):\n","        st.error(\"Failed to load all resources.\")\n","        return\n","\n","    query = st.text_input(\"Enter your financial query:\", \"What were the total assets in 2023?\")\n","\n","    if st.button(\"Submit Query\"):\n","        if not query.strip():\n","            st.warning(\"Please enter a query.\")\n","            return\n","\n","        start = time.time()\n","        with st.spinner(\"Retrieving relevant documents...\"):\n","            results = hybrid_retrieval(query, chunks, faiss_index, bm25, chunk_ids, emb_model)\n","\n","        if not results:\n","            st.error(\"No relevant information found.\")\n","            return\n","\n","        with st.spinner(\"Generating answer...\"):\n","            answer = rag_generate(query, results, RetrievalConfig)\n","\n","        elapsed = time.time() - start\n","\n","        st.subheader(\"Answer\")\n","        st.markdown(f\"**{answer}**\")\n","\n","        with st.expander(\"Show Retrieval Details\"):\n","            st.write(f\"**Response Time**: {elapsed:.2f} sec\")\n","            st.write(f\"**Merged Context Blocks**: {len(results)}\")\n","            for i, ch in enumerate(results, 1):\n","                src = ch['metadata'].get('file_path', 'unknown')\n","                st.info(f\"**[{i}] Source**: {src}\\n\\n**Text**: {ch['text'][:1200]}{'...' if len(ch['text'])>1200 else ''}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"bJudJNQsppz0"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[],"collapsed_sections":["WYTEzmUJVrYB"],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ee1851d8ba7544a0abebf5e5cc8cfb06":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f48eab17276041e7853257d5cc451794","IPY_MODEL_c3a10eda7f8a405e9e6ba799cb1e985a","IPY_MODEL_32f736ad4bba4760b539f53d20486a22"],"layout":"IPY_MODEL_e8b2167cfcac4146942fe3830a387df5"}},"f48eab17276041e7853257d5cc451794":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50f4a4b55d3e45da9211b37b1701131d","placeholder":"​","style":"IPY_MODEL_1375bf9260b147fa8750fe31473e15be","value":"Batches: 100%"}},"c3a10eda7f8a405e9e6ba799cb1e985a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e640f7f4e7e744a1aad47e754b48ad21","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3251c7434a94dd6b12ad91726833289","value":5}},"32f736ad4bba4760b539f53d20486a22":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c42bda3799074568bdf38ad0968acd1e","placeholder":"​","style":"IPY_MODEL_8d139f3f9a93479cb36bd441c8933a9d","value":" 5/5 [00:00&lt;00:00, 16.19it/s]"}},"e8b2167cfcac4146942fe3830a387df5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50f4a4b55d3e45da9211b37b1701131d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1375bf9260b147fa8750fe31473e15be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e640f7f4e7e744a1aad47e754b48ad21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3251c7434a94dd6b12ad91726833289":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c42bda3799074568bdf38ad0968acd1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d139f3f9a93479cb36bd441c8933a9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}