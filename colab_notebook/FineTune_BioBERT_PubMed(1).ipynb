{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/drive/your-notebook-name\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"title"},"source":["# Fine-Tune BioBERT on PubMed Abstracts\n","\n","This notebook fine-tunes BioBERT (`allenai/biobert-v1.1-pubmed`) on PubMed abstracts using Masked Language Modeling (MLM) for biomedical NLP tasks, such as improving spellchecking in medical texts (e.g., correcting 'arbitysratsddion' to 'arteries'). The model is trained in Google Colab with GPU support and saved to Google Drive for local use in a spellchecker project.\n","\n","**Setup**: Google Colab (GPU), Hugging Face `pubmed` dataset, PyTorch.\n","**Output**: Fine-tuned BioBERT model and tokenizer saved to `/content/drive/MyDrive/biobert_finetuned`."]},{"cell_type":"code","metadata":{"id":"install-dependencies","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750486628814,"user_tz":-330,"elapsed":5332,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"e20d83f5-0fe6-48ab-bcd0-b24480c8cf07"},"source":["!pip install transformers==4.28.0 datasets==2.10.0 torch==1.13.1 sentencepiece==0.1.97 numpy==1.24.2\n","\n","# Verify installation\n","import torch\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers==4.28.0 in /usr/local/lib/python3.11/dist-packages (4.28.0)\n","Requirement already satisfied: datasets==2.10.0 in /usr/local/lib/python3.11/dist-packages (2.10.0)\n","Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.11/dist-packages (1.13.1)\n","Requirement already satisfied: sentencepiece==0.1.97 in /usr/local/lib/python3.11/dist-packages (0.1.97)\n","Requirement already satisfied: numpy==1.24.2 in /usr/local/lib/python3.11/dist-packages (1.24.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (0.33.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (2.32.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (4.67.1)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.10.0) (18.1.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.10.0) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.10.0) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.10.0) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.10.0) (0.70.14)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets==2.10.0) (2025.3.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.10.0) (3.11.15)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.11/dist-packages (from datasets==2.10.0) (0.18.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==1.13.1) (4.14.0)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==1.13.1) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==1.13.1) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==1.13.1) (11.10.3.66)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==1.13.1) (11.7.99)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (75.2.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.45.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.10.0) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.10.0) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.10.0) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.10.0) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.10.0) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.10.0) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.10.0) (1.20.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (1.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.0) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.0) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.0) (2025.6.15)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.10.0) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.10.0) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.10.0) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.10.0) (1.17.0)\n","PyTorch version: 1.13.1+cu117\n","CUDA available: True\n","GPU: Tesla T4\n"]}]},{"cell_type":"code","metadata":{"id":"mount-drive","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750486679827,"user_tz":-330,"elapsed":29742,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"13f6ccd9-c5e4-42b5-cd6f-288f749f2b84"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set output directory\n","output_dir = '/content/drive/MyDrive/biobert_finetuned'\n","import os\n","os.makedirs(output_dir, exist_ok=True)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"load-data","colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"status":"error","timestamp":1750486799239,"user_tz":-330,"elapsed":257,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"d172c2d3-0514-4f2b-e8be-047d1714938e"},"source":["from datasets import load_dataset\n","\n","# Load PubMed dataset (10% subset for Colab compatibility)\n","dataset = load_dataset('pubmed_abstracts', split='train[:10%]')\n","\n","# Inspect dataset\n","print(dataset)\n","print(dataset[0]['Abstract']['AbstractText'] if 'Abstract' in dataset[0] else 'No abstract')"],"execution_count":5,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"Couldn't find a dataset script at /content/pubmed_abstracts/pubmed_abstracts.py or any data file in the same directory. Couldn't find 'pubmed_abstracts' on the Hugging Face Hub either: FileNotFoundError: Dataset 'pubmed_abstracts' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-5-1498601805.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load PubMed dataset (10% subset for Colab compatibility)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pubmed_abstracts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train[:10%]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Inspect dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1759\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1760\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1494\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_auth_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1497\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m                     raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1215\u001b[0m                         \u001b[0;34mf\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /content/pubmed_abstracts/pubmed_abstracts.py or any data file in the same directory. Couldn't find 'pubmed_abstracts' on the Hugging Face Hub either: FileNotFoundError: Dataset 'pubmed_abstracts' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`."]}]},{"cell_type":"code","metadata":{"id":"preprocess-data","executionInfo":{"status":"aborted","timestamp":1750486640402,"user_tz":-330,"elapsed":2,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}}},"source":["from transformers import BertTokenizer, DataCollatorForLanguageModeling\n","\n","# Load BioBERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('allenai/biobert_v1.1_pubmed')\n","\n","# Preprocess function\n","def preprocess_function(examples):\n","    texts = [item['AbstractText'] for item in examples['Abstract'] if item['AbstractText']]\n","    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n","    return encodings\n","\n","# Apply preprocessing\n","encoded_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n","\n","# Data collator for MLM\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fine-tune-model","executionInfo":{"status":"aborted","timestamp":1750486640407,"user_tz":-330,"elapsed":6,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}}},"source":["from transformers import BertForMaskedLM, Trainer, TrainingArguments\n","\n","# Load BioBERT model\n","model = BertForMaskedLM.from_pretrained('allenai/biobert_v1.1_pubmed')\n","\n","# Training arguments\n","training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    overwrite_output_dir=True,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,  # Reduced for Colab free tier\n","    per_device_eval_batch_size=8,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir=f'{output_dir}/logs',\n","    logging_steps=100,\n","    save_steps=1000,\n","    save_total_limit=2,\n","    fp16=True if torch.cuda.is_available() else False,  # Mixed precision for GPU\n",")\n","\n","# Initialize Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=encoded_dataset,\n","    data_collator=data_collator,\n",")\n","\n","# Train\n","trainer.train()\n","\n","# Save model and tokenizer\n","model.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","print(f'Model saved to {output_dir}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"test-model","executionInfo":{"status":"aborted","timestamp":1750486640411,"user_tz":-330,"elapsed":6,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}}},"source":["from transformers import pipeline\n","\n","# Test the fine-tuned model\n","fill_mask = pipeline('fill-mask', model=output_dir, tokenizer=output_dir)\n","test_sentence = 'Hypertension is a [MASK] condition.'\n","results = fill_mask(test_sentence)\n","for result in results:\n","    print(f\"Token: {result['token_str']}, Score: {result['score']:.4f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"download-model","executionInfo":{"status":"aborted","timestamp":1750486640420,"user_tz":-330,"elapsed":2,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}}},"source":["# Zip the model for download\n","!zip -r /content/biobert_finetuned.zip /content/drive/MyDrive/biobert_finetuned\n","\n","from google.colab import files\n","files.download('/content/biobert_finetuned.zip')\n","print('Download the zip file to your local machine')"],"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}