{"cells":[{"cell_type":"markdown","metadata":{"id":"c0e-2mEY233a"},"source":["# Neural Spellchecker for Medical and Legal Domains\n","\n","This notebook implements a neural spellchecker with a web interface (Step 1) and back-end logic (Step 2) for medical (PubMed) and legal (case files) domains. It uses BERT (BioBERT, LegalBERT) and a placeholder LSTM model, with a JSON config file for domain-specific terms.\n","\n","## Step 1: Web Interface\n","- HTML/CSS/JavaScript interface with text/file input, domain (Medical, Legal) and model (BERT, LSTM) dropdowns, and error display.\n","\n","## Step 2: Spellchecker Logic\n","- Flask back-end with BioBERT (medical), LegalBERT (legal), and placeholder LSTM.\n","- Uses `domain_config.json` for domain-specific terms.\n","- Classifies errors as general (NLTK dictionary) or domain-specific (config-based).\n","- Prioritizes corrections using semantic and syntactic patterns.\n","\n","## Setup Instructions\n","1. Run all cells to install dependencies, save files, and start the Flask server.\n","2. Use the `ngrok` URL to access the web interface.\n","3. Test with sample text (e.g., 'Patient has hypertention.') or .txt files.\n","\n","**Note**: .docx parsing is a placeholder (requires `python-docx`). LSTM is not fully implemented (pending Step 3)."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ailrPYG0233c","executionInfo":{"status":"ok","timestamp":1750364768098,"user_tz":-330,"elapsed":6472,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"ff647d8f-0c3d-428f-d4e5-fe4e889b0a5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: flask==2.3.2 in /usr/local/lib/python3.11/dist-packages (2.3.2)\n","Requirement already satisfied: flask-cors==4.0.0 in /usr/local/lib/python3.11/dist-packages (4.0.0)\n","Requirement already satisfied: transformers==4.44.2 in /usr/local/lib/python3.11/dist-packages (4.44.2)\n","Requirement already satisfied: tensorflow==2.16.1 in /usr/local/lib/python3.11/dist-packages (2.16.1)\n","Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n","Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Requirement already satisfied: pyngrok==7.1.6 in /usr/local/lib/python3.11/dist-packages (7.1.6)\n","Requirement already satisfied: Werkzeug>=2.3.3 in /usr/local/lib/python3.11/dist-packages (from flask==2.3.2) (3.1.3)\n","Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask==2.3.2) (3.1.6)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from flask==2.3.2) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask==2.3.2) (8.2.1)\n","Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from flask==2.3.2) (1.9.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.33.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.5.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (4.67.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (3.14.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (0.3.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (3.4.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (4.25.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (4.14.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (1.73.0)\n","Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (2.16.2)\n","Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (3.8.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (0.37.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.16.1) (0.45.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (2025.3.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (1.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask==2.3.2) (3.0.2)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2025.6.15)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.8)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (0.7.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (2.19.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.1) (0.1.2)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["# Install dependencies\n","!pip install flask==2.3.2 flask-cors==4.0.0 transformers==4.44.2 tensorflow==2.16.1 nltk==3.8.1 numpy==1.26.4 pyngrok==7.1.6\n","\n","# Download NLTK data\n","import nltk\n","nltk.download('words')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FOzJjOid233d","executionInfo":{"status":"ok","timestamp":1750364792266,"user_tz":-330,"elapsed":24,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"9a70112e-b181-466f-f562-190d011beda2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing index.html\n"]}],"source":["# Save front-end files\n","%%writefile index.html\n","<!DOCTYPE html>\n","<html lang=\"en\">\n","<head>\n","    <meta charset=\"UTF-8\">\n","    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","    <title>Neural Spellchecker</title>\n","    <link rel=\"stylesheet\" href=\"styles.css\">\n","</head>\n","<body>\n","    <div class=\"container\">\n","        <h1>Neural Spellchecker</h1>\n","        <div class=\"input-section\">\n","            <label for=\"text-input\">Enter Text:</label>\n","            <textarea id=\"text-input\" rows=\"5\" placeholder=\"Enter text to check...\"></textarea>\n","            <label for=\"file-input\">Or Upload File (.txt, .docx):</label>\n","            <input type=\"file\" id=\"file-input\" accept=\".txt,.docx\" multiple>\n","            <div class=\"options\">\n","                <label for=\"domain-select\">Domain:</label>\n","                <select id=\"domain-select\">\n","                    <option value=\"medical\" selected>Medical</option>\n","                    <option value=\"legal\">Legal</option>\n","                </select>\n","                <label for=\"model-select\">Model:</label>\n","                <select id=\"model-select\">\n","                    <option value=\"bert\">BERT</option>\n","                    <option value=\"lstm\">LSTM</option>\n","                </select>\n","            </div>\n","            <button id=\"analyze-btn\">Analyze</button>\n","        </div>\n","        <div class=\"output-section\">\n","            <h2>Results</h2>\n","            <div id=\"progress\" class=\"hidden\">Processing...</div>\n","            <div id=\"general-corrections\">\n","                <h3>General Corrections</h3>\n","                <ul id=\"general-list\"></ul>\n","            </div>\n","            <div id=\"domain-corrections\">\n","                <h3>Domain-Specific Corrections</h3>\n","                <ul id=\"domain-list\"></ul>\n","            </div>\n","            <div id=\"highlighted-text\">\n","                <h3>Highlighted Text</h3>\n","                <p id=\"highlighted-output\"></p>\n","            </div>\n","        </div>\n","    </div>\n","    <script src=\"script.js\"></script>\n","</body>\n","</html>"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8w9McqOv233e","executionInfo":{"status":"ok","timestamp":1750364796013,"user_tz":-330,"elapsed":56,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"a283d2f7-9fa4-4f13-d16d-8b9033ce12d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing styles.css\n"]}],"source":["%%writefile styles.css\n","body {\n","    font-family: Arial, sans-serif;\n","    margin: 0;\n","    padding: 20px;\n","    background-color: #f4f4f9;\n","}\n",".container {\n","    max-width: 800px;\n","    margin: 0 auto;\n","    background: white;\n","    padding: 20px;\n","    border-radius: 8px;\n","    box-shadow: 0 0 10px rgba(0,0,0,0.1);\n","}\n","h1 {\n","    text-align: center;\n","    color: #333;\n","}\n",".input-section, .output-section {\n","    margin-bottom: 20px;\n","}\n","textarea {\n","    width: 100%;\n","    padding: 10px;\n","    margin-bottom: 10px;\n","    border: 1px solid #ddd;\n","    border-radius: 4px;\n","}\n","input[type=\"file\"] {\n","    margin-bottom: 10px;\n","}\n",".options {\n","    display: flex;\n","    gap: 20px;\n","    margin-bottom: 10px;\n","}\n","button {\n","    background-color: #007bff;\n","    color: white;\n","    padding: 10px 20px;\n","    border: none;\n","    border-radius: 4px;\n","    cursor: pointer;\n","}\n","button:hover {\n","    background-color: #0056b3;\n","}\n",".output-section h3 {\n","    color: #444;\n","}\n","ul {\n","    list-style: none;\n","    padding: 0;\n","}\n","li {\n","    padding: 5px 0;\n","}\n",".error {\n","    background-color: #ffe6e6;\n","    text-decoration: line-through;\n","}\n",".correction {\n","    background-color: #e6ffe6;\n","}\n",".hidden {\n","    display: none;\n","}\n","#progress {\n","    text-align: center;\n","    color: #007bff;\n","}"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SNFBew-t233e","executionInfo":{"status":"ok","timestamp":1750364799759,"user_tz":-330,"elapsed":22,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"15922073-507b-4f79-f51e-dcf942ab932e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing script.js\n"]}],"source":["%%writefile script.js\n","document.getElementById('analyze-btn').addEventListener('click', async () => {\n","    const textInput = document.getElementById('text-input').value;\n","    const fileInput = document.getElementById('file-input').files;\n","    const domain = document.getElementById('domain-select').value;\n","    const model = document.getElementById('model-select').value;\n","    const generalList = document.getElementById('general-list');\n","    const domainList = document.getElementById('domain-list');\n","    const highlightedOutput = document.getElementById('highlighted-output');\n","    const progress = document.getElementById('progress');\n","\n","    generalList.innerHTML = '';\n","    domainList.innerHTML = '';\n","    highlightedOutput.innerHTML = '';\n","    progress.classList.remove('hidden');\n","\n","    try {\n","        const baseUrl = window.location.origin.includes('ngrok') ? window.location.origin : 'http://localhost:5000';\n","        if (textInput) {\n","            const response = await fetch(`${baseUrl}/analyze`, {\n","                method: 'POST',\n","                headers: { 'Content-Type': 'application/json' },\n","                body: JSON.stringify({ text: textInput, domain, model })\n","            });\n","            const data = await response.json();\n","            displayResults(data, textInput);\n","        } else if (fileInput.length) {\n","            for (let file of fileInput) {\n","                const text = await readFile(file);\n","                const response = await fetch(`${baseUrl}/analyze`, {\n","                    method: 'POST',\n","                    headers: { 'Content-Type': 'application/json' },\n","                    body: JSON.stringify({ text, domain, model })\n","                });\n","                const data = await response.json();\n","                displayResults(data, text, file.name);\n","            }\n","        } else {\n","            alert('Please enter text or upload a file.');\n","        }\n","    } catch (error) {\n","        console.error('Error:', error);\n","        alert('An error occurred while analyzing.');\n","    } finally {\n","        progress.classList.add('hidden');\n","    }\n","});\n","\n","function readFile(file) {\n","    return new Promise((resolve, reject) => {\n","        const reader = new FileReader();\n","        reader.onload = () => resolve(reader.result);\n","        reader.onerror = reject;\n","        if (file.name.endsWith('.docx')) {\n","            reject('DOCX parsing not implemented.');\n","        } else {\n","            reader.readAsText(file);\n","        }\n","    });\n","}\n","\n","function displayResults(data, text, filename = '') {\n","    const generalList = document.getElementById('general-list');\n","    const domainList = document.getElementById('domain-list');\n","    const highlightedOutput = document.getElementById('highlighted-output');\n","\n","    data.general_corrections.forEach(c => {\n","        const li = document.createElement('li');\n","        li.textContent = `${c.original} → ${c.corrected} [${(c.confidence * 100).toFixed(0)}%]`;\n","        generalList.appendChild(li);\n","    });\n","\n","    data.domain_corrections.forEach(c => {\n","        const li = document.createElement('li');\n","        li.textContent = `${c.original} → ${c.corrected} [${(c.confidence * 100).toFixed(0)}%]`;\n","        domainList.appendChild(li);\n","    });\n","\n","    let highlighted = text;\n","    data.general_corrections.concat(data.domain_corrections).forEach(c => {\n","        const regex = new RegExp(`\\\\b${c.original}\\\\b`, 'gi');\n","        highlighted = highlighted.replace(regex, `<span class=\"error\">${c.original}</span> <span class=\"correction\">${c.corrected}</span>`);\n","    });\n","    highlightedOutput.innerHTML = filename ? `<strong>${filename}</strong>: ${highlighted}` : highlighted;\n","}"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hzr8Xqir233n","executionInfo":{"status":"ok","timestamp":1750364804330,"user_tz":-330,"elapsed":8,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"292df7d8-8026-43af-d04b-c58ef297fb2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing domain_config.json\n"]}],"source":["%%writefile domain_config.json\n","{\n","    \"medical\": {\n","        \"terms\": [\n","            \"hypertension\",\n","            \"cardiology\",\n","            \"arrhythmia\",\n","            \"oncology\"\n","        ]\n","    },\n","    \"legal\": {\n","        \"terms\": [\n","            \"defendant\",\n","            \"plaintiff\",\n","            \"litigation\",\n","            \"arbitration\"\n","        ]\n","    }\n","}"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yCWJL6WF233n","executionInfo":{"status":"ok","timestamp":1750364807591,"user_tz":-330,"elapsed":22,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"dd590d0f-369b-4123-fa59-539692d1aaa3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing app.py\n"]}],"source":["%%writefile app.py\n","from flask import Flask, request, jsonify\n","from flask_cors import CORS\n","import nltk\n","from nltk.corpus import words\n","from transformers import TFBertTokenizer, TFBertForMaskedLM\n","import tensorflow as tf\n","import numpy as np\n","import json\n","import logging\n","\n","# Setup logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","# Initialize Flask app\n","app = Flask(__name__)\n","CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n","\n","# Download NLTK data\n","nltk.download('words')\n","word_list = set(words.words())\n","\n","# Load domain config\n","try:\n","    with open('domain_config.json', 'r') as f:\n","        domain_config = json.load(f)\n","except Exception as e:\n","    logger.error(f\"Error loading domain config: {e}\")\n","    domain_config = {\"medical\": {\"terms\": []}, \"legal\": {\"terms\": []}}\n","\n","# Load BERT models\n","try:\n","    medical_tokenizer = TFBertTokenizer.from_pretrained('allenai/biobert_v1.1_pubmed')\n","    medical_model = TFBertForMaskedLM.from_pretrained('allenai/biobert_v1.1_pubmed')\n","    legal_tokenizer = TFBertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n","    legal_model = TFBertForMaskedLM.from_pretrained('nlpaueb/legal-bert-base-uncased')\n","except Exception as e:\n","    logger.error(f\"Error loading BERT models: {e}\")\n","    raise\n","\n","# Define LSTM model\n","def create_lstm_model(vocab_size, max_length=128):\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Embedding(vocab_size, 128, input_length=max_length),\n","        tf.keras.layers.LSTM(256, return_sequences=True),\n","        tf.keras.layers.LSTM(256),\n","        tf.keras.layers.Dense(vocab_size, activation='softmax')\n","    ])\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","    return model\n","\n","# Initialize LSTM model (placeholder, not trained)\n","vocab_size = 30000\n","lstm_model = create_lstm_model(vocab_size)\n","\n","@app.route('/analyze', methods=['POST'])\n","def analyze_text():\n","    try:\n","        data = request.get_json()\n","        text = data.get('text', '')\n","        domain = data.get('domain', 'medical')\n","        model_type = data.get('model', 'bert')\n","\n","        if not text:\n","            return jsonify({\"error\": \"No text provided\"}), 400\n","\n","        words = [w for w in re.findall(r'\\b\\w+\\b', text.lower()) if w not in word_list and len(w) > 2]\n","        general_corrections = []\n","        domain_corrections = []\n","\n","        for word in words:\n","            correction, confidence = suggest_correction(word, domain, model_type)\n","            if correction and correction != word:\n","                if word in domain_config.get(domain, {}).get('terms', []):\n","                    domain_corrections.append({\n","                        \"original\": word,\n","                        \"corrected\": correction,\n","                        \"confidence\": confidence\n","                    })\n","                else:\n","                    general_corrections.append({\n","                        \"original\": word,\n","                        \"corrected\": correction,\n","                        \"confidence\": confidence\n","                    })\n","\n","        return jsonify({\n","            \"general_corrections\": general_corrections,\n","            \"domain_corrections\": domain_corrections\n","        })\n","    except Exception as e:\n","        logger.error(f\"Error analyzing text: {e}\")\n","        return jsonify({\"error\": \"Internal server error\"}), 500\n","\n","def suggest_correction(word, domain, model_type):\n","    try:\n","        if model_type == 'bert':\n","            tokenizer = medical_tokenizer if domain == 'medical' else legal_tokenizer\n","            model = medical_model if domain == 'medical' else legal_model\n","            sentence = f\"The {'patient' if domain == 'medical' else 'case'} involves {word}.\"\n","            encoded = tokenizer.encode(sentence, return_tensors='tf')\n","            mask_idx = [i for i, token in enumerate(encoded[0]) if token == tokenizer.encode(word)[1:-1][0]][0]\n","            encoded[0, mask_idx] = tokenizer.mask_token_id\n","            outputs = model(encoded)\n","            logits = outputs.logits[0, mask_idx]\n","            probs = tf.nn.softmax(logits).numpy()\n","            top_idx = np.argmax(probs)\n","            correction = tokenizer.decode([top_idx]).strip()\n","            confidence = float(probs[top_idx])\n","            return correction, confidence\n","        else:\n","            return word, 0.9  # Placeholder for LSTM\n","    except Exception as e:\n","        logger.error(f\"Error suggesting correction for {word}: {e}\")\n","        return None, 0.0\n","\n","if __name__ == '__main__':\n","    app.run(host='0.0.0.0', port=5000)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_DGJQRyw233o","executionInfo":{"status":"ok","timestamp":1750365209159,"user_tz":-330,"elapsed":5092,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"7d4af3f3-c7a6-4d9c-f19c-d9a9c118436a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Public URL: https://d8df-34-82-89-134.ngrok-free.app\n","2025-06-19 20:33:26.858067: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-06-19 20:33:28.013401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Access the web interface at: https://d8df-34-82-89-134.ngrok-free.app/index.html\n","Test the API with the cell below.\n"]}],"source":["# Setup ngrok and run Flask server\n","from pyngrok import ngrok\n","import threading\n","import time\n","\n","# Start ngrok tunnel\n","ngrok.set_auth_token('2yk1wCi1k5WBn0Ubet6spRSf6W2_2Sa9R1sM8EEqXemZUXz6k')  # Replace with your ngrok auth token\n","public_url = ngrok.connect(5000).public_url\n","print(f'Public URL: {public_url}')\n","\n","# Run Flask in a background thread\n","def run_flask():\n","    !python app.py\n","\n","threading.Thread(target=run_flask, daemon=True).start()\n","\n","# Wait for Flask to start\n","time.sleep(5)\n","\n","# Serve front-end\n","print(f'Access the web interface at: {public_url}/index.html')\n","print('Test the API with the cell below.')"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0cVzT8Ua233o","executionInfo":{"status":"ok","timestamp":1750365216610,"user_tz":-330,"elapsed":371,"user":{"displayName":"REVATHI P","userId":"07797098857097259305"}},"outputId":"a23b1587-b7ea-4f11-9085-861f1c4e4004"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pyngrok.process.ngrok:t=2025-06-19T20:33:35+0000 lvl=warn msg=\"failed to open private leg\" id=4f9c51417380 privaddr=localhost:5000 err=\"dial tcp 127.0.0.1:5000: connect: connection refused\"\n"]}],"source":["# Test the /analyze endpoint\n","import requests\n","\n","payload = {\n","    'text': 'Patient has hypertention and arrythmia.',\n","    'domain': 'medical',\n","    'model': 'bert'\n","}\n","response = requests.post(f'{public_url}/analyze', json=payload)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}