{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0DxBj3qP4IC"
   },
   "source": [
    "1.The Embedding layer performs the task of mapping discrete word indices (integers) into dense vector representations (embeddings).\n",
    "\n",
    "input_dim (Vocabulary Size):\n",
    "\n",
    "The input to the embedding layer consists of word indices from the corpus. The size of the vocabulary (vocab_size) represents how many unique words are in the entire dataset (including padding).\n",
    "This parameter defines how many different words the model can handle.\n",
    "output_dim (Embedding Dimensions):\n",
    "\n",
    "The output_dim is the size of the dense vector that represents each word.\n",
    "In this example, embedding_size = 10, meaning each word will be represented as a 10-dimensional vector in continuous space.\n",
    "These dense vectors are learned during training and aim to capture semantic relationships between words (i.e., words with similar meanings will have similar embeddings).\n",
    "input_length (Context Length):\n",
    "\n",
    "The input_length represents the number of context words fed into the model at each step.\n",
    "In the CBOW model, the input is a set of 2 * window_size context words around a target word. So, if the window_size = 2, then input_length = 4. This means the model will take 4 context words to predict the target word.\n",
    "\n",
    "Why is the Embedding Layer needed?\n",
    "\n",
    "Convert Words into Vectors: Words in a corpus are typically represented as strings or categorical variables, but neural networks cannot work with such raw categorical data. Instead, they require numeric input. The embedding layer transforms words into continuous, dense vectors, making them suitable for processing by the neural network.\n",
    "\n",
    "Semantic Relationships: Through training, the embedding layer learns the relationships between words. For instance, the words \"dog\" and \"cat\" may end up with similar embeddings because they are likely to appear in similar contexts in the text. This is the core idea of word embeddings — capturing semantic meaning in a lower-dimensional space.\n",
    "\n",
    "Parameter Efficiency: Unlike one-hot encoding, which creates very sparse, high-dimensional vectors (each word is represented by a vector of size equal to the vocabulary size, with only one non-zero element), the embedding layer provides a more efficient representation by using dense, smaller vectors. This reduces both the memory requirements and the computational complexity of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vXfbsyqPzvk"
   },
   "source": [
    "2.The Lambda layer is used to perform a custom operation — in this case, calculating the mean of the embeddings of all the context words.\n",
    "\n",
    "Why is this important in CBOW?\n",
    "\n",
    "In the Continuous Bag of Words (CBOW) model, we do not use individual context word embeddings directly. Instead, we average the embeddings of all context words to get a single vector that represents the entire context.\n",
    "\n",
    "tf.reduce_mean(x, axis=1): This operation averages the embeddings of all context words across the sequence. The resulting output is a single vector representing the context. This vector will then be used to predict the target word.\n",
    "\n",
    "For example, if the context words have embeddings [0.3, 0.5, 0.7] for \"cat\", \"sat\", and \"on\", the Lambda layer averages them to produce a single vector. This aggregation helps the model generalize the context and not be too sensitive to any individual word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKMCdtLLQFIo"
   },
   "source": [
    "3.The Dense layer at the end of the model produces the final output, which is a probability distribution over the entire vocabulary.\n",
    "\n",
    "Softmax Activation:\n",
    "\n",
    "The softmax activation function transforms the output into a probability distribution, where each value corresponds to the probability of a word being the target, given the context.\n",
    "The output layer has as many units as the number of words in the vocabulary (vocab_size), and each unit represents a word in the vocabulary.\n",
    "\n",
    "Why is this necessary?\n",
    "\n",
    "After averaging the context word embeddings, the model needs to predict the target word. The Dense layer outputs a probability for each word in the vocabulary, and the word with the highest probability is selected as the predicted target word.\n",
    "\n",
    "Categorical Cross-Entropy Loss:\n",
    "During training, the categorical cross-entropy loss is used to minimize the difference between the predicted probabilities and the actual target word (which is represented as a one-hot vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0467lboR0i3p",
    "outputId": "b7f399f0-063e-45be-a396-4a0709d33015"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _c_internal_utils: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\madha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\matplotlib\\__init__.py:187\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32mc:\\users\\madha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\matplotlib\\cbook.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisibleDeprecationWarning\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _c_internal_utils\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_running_interactive_framework\u001b[39m():\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    Return the interactive framework whose event loop is currently running, if\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    any, or \"headless\" if no event loop can be started, or None.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m        \"macosx\", \"headless\", ``None``.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _c_internal_utils: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define the corpus (collection of sentences)\n",
    "corpus = [\"I love sunflowers\",\n",
    "                            \"Sunflowers fill my heart with joy\",\n",
    "                            \"I love to look into the garden and see the flowers\",\n",
    "                            \"Flowers especially sunflowers are the most beautiful\"]\n",
    "\n",
    "# Step 1: Tokenize the corpus and convert text to sequences of integers\n",
    "# Tokenizer: assigns a unique integer to each word in the corpus\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)  # Builds the vocabulary and word-index mapping\n",
    "sequences = tokenizer.texts_to_sequences(corpus)  # Converts each sentence to a sequence of word indices\n",
    "\n",
    "print(\"After converting the corpus into a sequence of integers:\")\n",
    "print(sequences)\n",
    "\n",
    "# Vocabulary size (including a padding index of 0)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding or unseen words\n",
    "embedding_size = 10  # The dimensionality of word embeddings\n",
    "window_size = 1     # Context window size (number of words to consider on both sides of the target word)\n",
    "\n",
    "# Step 2: Generate context-target pairs for the CBOW model\n",
    "# For each word in a sentence, use its neighboring words (window_size) as the context\n",
    "contexts = []  # Stores context words\n",
    "targets = []   # Stores target words\n",
    "\n",
    "for sequence in sequences:\n",
    "    for i in range(window_size, len(sequence) - window_size):\n",
    "        # Context words: words in the window (excluding the center word)\n",
    "        context = sequence[i - window_size:i] + sequence[i + 1:i + window_size + 1]\n",
    "        target = sequence[i]  # Center word (target)\n",
    "\n",
    "        # Append context and target to the lists\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "\n",
    "        # Debug output for context and target pairs\n",
    "        print(f\"Context: {context}, Target: {target}\")\n",
    "\n",
    "# Step 3: Convert context and target pairs to numpy arrays\n",
    "X = np.array(contexts)  # Context words (input)\n",
    "y = to_categorical(targets, num_classes=vocab_size)  # Convert targets to one-hot encoded vectors\n",
    "\n",
    "# Step 4: Define the Continuous Bag of Words (CBOW) model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer: maps input word indices to dense word vectors (embeddings)\n",
    "model.add(Embedding(input_dim=vocab_size,  # Size of the vocabulary\n",
    "                    output_dim=embedding_size,  # Embedding dimensions\n",
    "                    input_length=2 * window_size))  # Length of input (number of context words), the input is 2 * window_size context words around a target word. So, if the window_size = 2, then input_length = 4.\n",
    "\n",
    "# Lambda layer: averages the embeddings of all context words\n",
    "model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "\n",
    "# Dense layer: outputs a probability distribution over the vocabulary\n",
    "model.add(Dense(units=vocab_size, activation='softmax'))\n",
    "\n",
    "# Step 5: Compile the model\n",
    "# Loss: categorical cross-entropy (as it's a multi-class classification problem)\n",
    "# Optimizer: Adam (adaptive learning rate optimization)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 6: Train the CBOW model\n",
    "print(\"\\nTraining the CBOW model...\")\n",
    "model.fit(X, y, epochs=100, verbose=0)  # Train for 100 epochs in silent mode\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Step 7: Extract the learned word embeddings\n",
    "# Access the Embedding layer and get its weights (word embeddings)\n",
    "# Extract the embeddings from the trained Embedding layer\n",
    "embedding_layer = model.layers[0]\n",
    "embeddings = embedding_layer.get_weights()[0]  # Shape: (vocab_size, embedding_size)\n",
    "\n",
    "# Print vocabulary size and shape of the embedding matrix\n",
    "print(f\"\\nNumber of words in the vocabulary (including padding): {len(embeddings)}\")\n",
    "print(f\"Shape of embeddings: {embeddings.shape}\")\n",
    "\n",
    "# Print the embedding vectors for all words in the vocabulary\n",
    "print(\"\\nWord Embeddings:\")\n",
    "for word, idx in tokenizer.word_index.items():  # Iterate over word-index pairs\n",
    "    print(f\"Word: '{word}' -> Embedding: {embeddings[idx]}\")\n",
    "\n",
    "# Print the embedding for the padding index (index 0), if applicable\n",
    "if 0 in range(len(embeddings)):\n",
    "    print(f\"\\nEmbedding for padding (index 0): {embeddings[0]}\")\n",
    "\n",
    "# Perform PCA to reduce the dimensionality of the embeddings for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# Visualize the embeddings\n",
    "plt.figure(figsize=(5, 5))\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    x, y = reduced_embeddings[idx]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), fontsize=10)\n",
    "plt.title(\"Word Embeddings Visualized\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JyqjDLNE24WW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
