{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwmhbibPnxdg"
      },
      "source": [
        "## Group No\n",
        "\n",
        "## Group Member Names:\n",
        "1.\n",
        "2.\n",
        "3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQaqju8Nnxdi"
      },
      "source": [
        "1.**Problem statement**:\n",
        "\n",
        "* Develop a reinforcement learning agent using dynamic programming to solve the Treasure Hunt problem in a FrozenLake environment. The agent must learn the optimal policy for navigating the lake while avoiding holes and maximizing its treasure collection.\n",
        "\n",
        "2.**Scenario**:\n",
        "* A treasure hunter is navigating a slippery 5x5 FrozenLake grid. The objective is to navigate through the lake collecting treasures while avoiding holes and ultimately reaching the exit (goal).\n",
        "Grid positions on a 5x5 map with tiles labeled as S, F, H, G, T. The state includes the current position of the agent and whether treasures have been collected.\n",
        "\n",
        "\n",
        "#### Objective\n",
        "* The agent must learn the optimal policy π* using dynamic programming to maximize its cumulative reward while navigating the lake.\n",
        "\n",
        "#### About the environment\n",
        "\n",
        "The environment consists of several types of tiles:\n",
        "* Start (S): The initial position of the agent, safe to step.\n",
        "* Frozen Tiles (F): Frozen surface, safe to step.\n",
        "* Hole (H): Falling into a hole ends the game immediately (die, end).\n",
        "* Goal (G): Exit point; reaching here ends the game successfully (safe, end).\n",
        "* Treasure Tiles (T): Added to the environment. Stepping on these tiles awards +5 reward but does not end the game.\n",
        "\n",
        "After stepping on a treasure tile, it becomes a frozen tile (F).\n",
        "The agent earns rewards as follows:\n",
        "* Reaching the goal (G): +10 reward.\n",
        "* Falling into a hole (H): -10 reward.\n",
        "* Collecting a treasure (T): +5 reward.\n",
        "* Stepping on a frozen tile (F): 0 reward.\n",
        "\n",
        "#### States\n",
        "* Current position of the agent (row, column).\n",
        "* A boolean flag (or equivalent) for whether each treasure has been collected.\n",
        "\n",
        "#### Actions\n",
        "* Four possible moves: up, down, left, right\n",
        "\n",
        "#### Rewards\n",
        "* Goal (G): +10.\n",
        "* Treasure (T): +5 per treasure.\n",
        "* Hole (H): -10.\n",
        "* Frozen tiles (F): 0.\n",
        "\n",
        "#### Environment\n",
        "Modify the FrozenLake environment in OpenAI Gym to include treasures (T) at certain positions. Inherit the original FrozenLakeEnv and modify the reset and step methods accordingly.\n",
        "Example grid:\n",
        "\n",
        "![image.png](attachment:image.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1RvJ1eenxdk"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZqx1IrXnxdk"
      },
      "source": [
        "**Expected Outcomes:**\n",
        "1.\tCreate the custom environment by modifying the existing “FrozenLakeNotSlippery-v0” in OpenAI Gym and Implement the dynamic programming using value iteration and policy improvement to learn the optimal policy for the Treasure Hunt problem.\n",
        "2.\tCalculate the state-value function (V*) for each state on the map after learning the optimal policy.\n",
        "3.\tCompare the agent’s performance with and without treasures, discussing the trade-offs in reward maximization.\n",
        "4.\tVisualize the agent’s direction on the map using the learned policy.\n",
        "5.\tCalculate expected total reward over multiple episodes to evaluate performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k2diXELnxdl"
      },
      "source": [
        "### Import required libraries and Define the custom environment - 2 Marks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxQmzt9ps6Uo",
        "outputId": "d12df751-8f74-48e7-8de4-a901f02aa1d6"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "VwD3_ERwnxdm"
      },
      "outputs": [],
      "source": [
        "# Import statements\n",
        "import gym\n",
        "import numpy as np\n",
        "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "gxz2vb1rnxdn"
      },
      "outputs": [],
      "source": [
        "# Custom environment to create the given grid and respective functions that are required for the problem\n",
        "class FrozenSlipperyLakeWithTreasures(FrozenLakeEnv):\n",
        "\n",
        "    def __init__(self, desc=None, map_name='5x5', render_mode=\"ansi\", is_slippery = True):\n",
        "      super().__init__(desc = desc, map_name = map_name, is_slippery = is_slippery, render_mode = render_mode)\n",
        "      self.treasure_collected = set()\n",
        "\n",
        "    def step(self, action):\n",
        "        state, reward, done, info = super().step(action)  # Call base call method to perform the step\n",
        "\n",
        "        # Determine the current position\n",
        "        row, col = divmod(self.s, self.ncol)  # Convert state index to (row, col)\n",
        "        current_tile = self.desc[row][col].decode(\"utf-8\")  # Get the tile character\n",
        "\n",
        "        # Assign rewards based on tile type\n",
        "        if current_tile == \"G\":  # Goal\n",
        "            reward = 10\n",
        "        elif current_tile == \"H\":  # Hole\n",
        "            reward = -10\n",
        "        elif current_tile == \"T\":  # Treasure\n",
        "            reward = 5\n",
        "            self.treasure_collected.add((row, col))  # Mark treasure as collected\n",
        "            self.desc.flatten()[state] = b'F' #Update the state to F after collecting the treasure\n",
        "        elif current_tile == \"F\":  # Frozen tile\n",
        "            reward = 0\n",
        "\n",
        "        return state, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "      super().reset()\n",
        "      self.treasure_collected = set()\n",
        "      return self.s\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Override render to display the state as a matrix.\"\"\"\n",
        "        # Decode byte strings for human-readable format\n",
        "        decoded_matrix = np.array(self.desc, dtype=str)\n",
        "\n",
        "        # Render the matrix\n",
        "        print(\"\\n\".join([\" \".join(row) for row in decoded_matrix]))\n",
        "\n",
        "    def get_transitionProbability_states(self, state, action):\n",
        "        updated_states = []\n",
        "        states = self.P[state][action]\n",
        "        for prob, s_, r, _ in states:\n",
        "          row, col = divmod(s_, self.ncol)  # Convert state index to (row, col)\n",
        "          current_tile = self.desc[row][col].decode(\"utf-8\")  # Get the tile character\n",
        "          # Assign rewards based on tile type\n",
        "          if current_tile == \"G\":  # Goal\n",
        "              r = 10\n",
        "          elif current_tile == \"H\":  # Hole\n",
        "              r = -10\n",
        "          elif current_tile == \"T\":  # Treasure\n",
        "              if (row, col) not in self.treasure_collected:\n",
        "                  r = 5\n",
        "              else:\n",
        "                  r = 0\n",
        "          elif current_tile == \"F\":  # Frozen tile\n",
        "              r = 0\n",
        "          elif current_tile == \"S\":  # Frozen tile\n",
        "              r = 0\n",
        "          updated_states.append((prob, s_, r, _))\n",
        "        return updated_states\n",
        "\n",
        "\n",
        "#Include functions to take an action, get reward, to check if episode is over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "ybjzZFpqnxdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0229babb-577c-4952-bafb-5e985b88ff0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S H F T F\n",
            "F F F F H\n",
            "T F F F F\n",
            "F F H F T\n",
            "H F F F G\n",
            "State 0:\n",
            "  Action 0: [(1.0, 0, 0, False)]\n",
            "  Action 1: [(1.0, 5, 0, False)]\n",
            "  Action 2: [(1.0, 1, -10, True)]\n",
            "  Action 3: [(1.0, 0, 0, False)]\n",
            "State 1:\n",
            "  Action 0: [(1.0, 1, -10, True)]\n",
            "  Action 1: [(1.0, 1, -10, True)]\n",
            "  Action 2: [(1.0, 1, -10, True)]\n",
            "  Action 3: [(1.0, 1, -10, True)]\n",
            "State 2:\n",
            "  Action 0: [(1.0, 1, -10, True)]\n",
            "  Action 1: [(1.0, 7, 0, False)]\n",
            "  Action 2: [(1.0, 3, 5, False)]\n",
            "  Action 3: [(1.0, 2, 0, False)]\n",
            "State 3:\n",
            "  Action 0: [(1.0, 2, 0, False)]\n",
            "  Action 1: [(1.0, 8, 0, False)]\n",
            "  Action 2: [(1.0, 4, 0, False)]\n",
            "  Action 3: [(1.0, 3, 5, False)]\n",
            "State 4:\n",
            "  Action 0: [(1.0, 3, 5, False)]\n",
            "  Action 1: [(1.0, 9, -10, True)]\n",
            "  Action 2: [(1.0, 4, 0, False)]\n",
            "  Action 3: [(1.0, 4, 0, False)]\n",
            "State 5:\n",
            "  Action 0: [(1.0, 5, 0, False)]\n",
            "  Action 1: [(1.0, 10, 5, False)]\n",
            "  Action 2: [(1.0, 6, 0, False)]\n",
            "  Action 3: [(1.0, 0, 0, False)]\n",
            "State 6:\n",
            "  Action 0: [(1.0, 5, 0, False)]\n",
            "  Action 1: [(1.0, 11, 0, False)]\n",
            "  Action 2: [(1.0, 7, 0, False)]\n",
            "  Action 3: [(1.0, 1, -10, True)]\n",
            "State 7:\n",
            "  Action 0: [(1.0, 6, 0, False)]\n",
            "  Action 1: [(1.0, 12, 0, False)]\n",
            "  Action 2: [(1.0, 8, 0, False)]\n",
            "  Action 3: [(1.0, 2, 0, False)]\n",
            "State 8:\n",
            "  Action 0: [(1.0, 7, 0, False)]\n",
            "  Action 1: [(1.0, 13, 0, False)]\n",
            "  Action 2: [(1.0, 9, -10, True)]\n",
            "  Action 3: [(1.0, 3, 5, False)]\n",
            "State 9:\n",
            "  Action 0: [(1.0, 9, -10, True)]\n",
            "  Action 1: [(1.0, 9, -10, True)]\n",
            "  Action 2: [(1.0, 9, -10, True)]\n",
            "  Action 3: [(1.0, 9, -10, True)]\n",
            "State 10:\n",
            "  Action 0: [(1.0, 10, 5, False)]\n",
            "  Action 1: [(1.0, 15, 0, False)]\n",
            "  Action 2: [(1.0, 11, 0, False)]\n",
            "  Action 3: [(1.0, 5, 0, False)]\n",
            "State 11:\n",
            "  Action 0: [(1.0, 10, 5, False)]\n",
            "  Action 1: [(1.0, 16, 0, False)]\n",
            "  Action 2: [(1.0, 12, 0, False)]\n",
            "  Action 3: [(1.0, 6, 0, False)]\n",
            "State 12:\n",
            "  Action 0: [(1.0, 11, 0, False)]\n",
            "  Action 1: [(1.0, 17, -10, True)]\n",
            "  Action 2: [(1.0, 13, 0, False)]\n",
            "  Action 3: [(1.0, 7, 0, False)]\n",
            "State 13:\n",
            "  Action 0: [(1.0, 12, 0, False)]\n",
            "  Action 1: [(1.0, 18, 0, False)]\n",
            "  Action 2: [(1.0, 14, 0, False)]\n",
            "  Action 3: [(1.0, 8, 0, False)]\n",
            "State 14:\n",
            "  Action 0: [(1.0, 13, 0, False)]\n",
            "  Action 1: [(1.0, 19, 5, False)]\n",
            "  Action 2: [(1.0, 14, 0, False)]\n",
            "  Action 3: [(1.0, 9, -10, True)]\n",
            "State 15:\n",
            "  Action 0: [(1.0, 15, 0, False)]\n",
            "  Action 1: [(1.0, 20, -10, True)]\n",
            "  Action 2: [(1.0, 16, 0, False)]\n",
            "  Action 3: [(1.0, 10, 5, False)]\n",
            "State 16:\n",
            "  Action 0: [(1.0, 15, 0, False)]\n",
            "  Action 1: [(1.0, 21, 0, False)]\n",
            "  Action 2: [(1.0, 17, -10, True)]\n",
            "  Action 3: [(1.0, 11, 0, False)]\n",
            "State 17:\n",
            "  Action 0: [(1.0, 17, -10, True)]\n",
            "  Action 1: [(1.0, 17, -10, True)]\n",
            "  Action 2: [(1.0, 17, -10, True)]\n",
            "  Action 3: [(1.0, 17, -10, True)]\n",
            "State 18:\n",
            "  Action 0: [(1.0, 17, -10, True)]\n",
            "  Action 1: [(1.0, 23, 0, False)]\n",
            "  Action 2: [(1.0, 19, 5, False)]\n",
            "  Action 3: [(1.0, 13, 0, False)]\n",
            "State 19:\n",
            "  Action 0: [(1.0, 18, 0, False)]\n",
            "  Action 1: [(1.0, 24, 10, True)]\n",
            "  Action 2: [(1.0, 19, 5, False)]\n",
            "  Action 3: [(1.0, 14, 0, False)]\n",
            "State 20:\n",
            "  Action 0: [(1.0, 20, -10, True)]\n",
            "  Action 1: [(1.0, 20, -10, True)]\n",
            "  Action 2: [(1.0, 20, -10, True)]\n",
            "  Action 3: [(1.0, 20, -10, True)]\n",
            "State 21:\n",
            "  Action 0: [(1.0, 20, -10, True)]\n",
            "  Action 1: [(1.0, 21, 0, False)]\n",
            "  Action 2: [(1.0, 22, 0, False)]\n",
            "  Action 3: [(1.0, 16, 0, False)]\n",
            "State 22:\n",
            "  Action 0: [(1.0, 21, 0, False)]\n",
            "  Action 1: [(1.0, 22, 0, False)]\n",
            "  Action 2: [(1.0, 23, 0, False)]\n",
            "  Action 3: [(1.0, 17, -10, True)]\n",
            "State 23:\n",
            "  Action 0: [(1.0, 22, 0, False)]\n",
            "  Action 1: [(1.0, 23, 0, False)]\n",
            "  Action 2: [(1.0, 24, 10, True)]\n",
            "  Action 3: [(1.0, 18, 0, False)]\n",
            "State 24:\n",
            "  Action 0: [(1.0, 24, 10, True)]\n",
            "  Action 1: [(1.0, 24, 10, True)]\n",
            "  Action 2: [(1.0, 24, 10, True)]\n",
            "  Action 3: [(1.0, 24, 10, True)]\n",
            "S H F T F\n",
            "F F F F H\n",
            "T F F F F\n",
            "F F H F T\n",
            "H F F F G\n"
          ]
        }
      ],
      "source": [
        "custom_map_5x5 = [\n",
        "    \"SHFTF\",\n",
        "    \"FFFFH\",\n",
        "    \"TFFFF\",\n",
        "    \"FFHFT\",\n",
        "    \"HFFFG\"\n",
        "]\n",
        "env = FrozenSlipperyLakeWithTreasures( desc = custom_map_5x5, map_name = \"5x5\")\n",
        "env.reset()\n",
        "env.render()\n",
        "for state in env.P:\n",
        "    print(f\"State {state}:\")\n",
        "    for action in env.P[state]:\n",
        "      print(f\"  Action {action}: {env.get_transitionProbability_states(state,action)}\")\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_NvEsDMnxdo"
      },
      "source": [
        "### Value Iteration Algorithm - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "tKPclxcdnxdp"
      },
      "outputs": [],
      "source": [
        "def value_iteration(env):\n",
        "  num_iterations = 10000\n",
        "  threshold = 1e-20 #value used to terminate if no changes observed in the new values from the previous values\n",
        "  gamma = 0.9 #discount factor\n",
        "  value_table = np.zeros(env.observation_space.n)\n",
        "  print('Initial Value Table: ', value_table)\n",
        "  for i in range(num_iterations): #iterate through given no of times\n",
        "    updated_value_table = np.copy(value_table)\n",
        "    for s in range(env.observation_space.n): #update for each state\n",
        "      row, col = divmod(s, env.ncol)\n",
        "      if env.desc[row, col] == b'H': #If hole dont update the value keep the reward as it is and return\n",
        "        value_table[s]=-10\n",
        "        continue\n",
        "      if env.desc[row, col] == b'G': #If gold dont update the value keep the reward as it is and return\n",
        "        value_table[s]=10\n",
        "        continue\n",
        "      if env.desc[row, col] == b'T': #If Treasure dont update the value keep the reward as it is and return\n",
        "        value_table[s]=5\n",
        "        env.treasure_collected.add((row, col))\n",
        "        continue\n",
        "      Q_values = []\n",
        "      for a in range(env.action_space.n): #get q values for each possible action\n",
        "        Q_value = 0\n",
        "        for prob, s_, r, _ in env.get_transitionProbability_states(s,a):\n",
        "          Q_value += prob*(r + gamma * updated_value_table[s_])\n",
        "        Q_values.append(Q_value)\n",
        "      value_table[s] = max(Q_values) #pick the maximum q values\n",
        "      #row, col = divmod(s, env.ncol)\n",
        "      #if env.desc[row, col] == b'T':\n",
        "          # Reset the tile to Frozen\n",
        "        #env.desc[row, col] = b'F'\n",
        "      #print(f'Value Table for State {s}', value_table[s])\n",
        "    if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold): # check for divergence\n",
        "      break\n",
        "  for s in range(env.observation_space.n):\n",
        "    print(f'Value Table for State {s}', value_table[s])\n",
        "  return value_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OJwDNisnxdp"
      },
      "source": [
        "### Policy Improvement Function - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "2SS0QqFdnxdp"
      },
      "outputs": [],
      "source": [
        "def extract_policy(value_table):\n",
        "  gamma = 1.0\n",
        "  policy = np.zeros(env.observation_space.n)\n",
        "  for s in range(env.observation_space.n):\n",
        "    Q_values = [sum([prob*(r + gamma * value_table[s_])\n",
        "                for prob, s_, r, _ in env.P[s][a]])\n",
        "                for a in range(env.action_space.n)]\n",
        "    policy[s] = np.argmax(np.array(Q_values))\n",
        "  return policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCLL-m66nxdq"
      },
      "source": [
        "### Print the Optimal Value Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "hlrS9E5Inxdq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnJ9tc4cnxdq"
      },
      "source": [
        "### Visualization of the learned optimal policy - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "P5os3SBCnxdr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sVmjSa8nxdr"
      },
      "source": [
        "### Evaluate the policy - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "zqKkONrNnxdr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYD7gvXsnxdr"
      },
      "source": [
        "### Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "ASXYXR7gnxdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2806a08-9602-40ed-e95c-fe19971dd254",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Value Table:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "Value Table for State 0 9.087641100000004\n",
            "Value Table for State 1 -10.0\n",
            "Value Table for State 2 11.219310000000004\n",
            "Value Table for State 3 5.0\n",
            "Value Table for State 4 4.5\n",
            "Value Table for State 5 10.097379000000004\n",
            "Value Table for State 6 11.219310000000004\n",
            "Value Table for State 7 12.465900000000003\n",
            "Value Table for State 8 13.851000000000003\n",
            "Value Table for State 9 -10.0\n",
            "Value Table for State 10 5.0\n",
            "Value Table for State 11 12.465900000000003\n",
            "Value Table for State 12 13.851000000000003\n",
            "Value Table for State 13 15.390000000000002\n",
            "Value Table for State 14 13.851000000000003\n",
            "Value Table for State 15 12.465900000000003\n",
            "Value Table for State 16 13.851000000000003\n",
            "Value Table for State 17 -10.0\n",
            "Value Table for State 18 17.1\n",
            "Value Table for State 19 5.0\n",
            "Value Table for State 20 -10.0\n",
            "Value Table for State 21 15.390000000000002\n",
            "Value Table for State 22 17.1\n",
            "Value Table for State 23 19.0\n",
            "Value Table for State 24 10.0\n",
            "optimal value function [  9.0876411 -10.         11.21931     5.          4.5        10.097379\n",
            "  11.21931    12.4659     13.851     -10.          5.         12.4659\n",
            "  13.851      15.39       13.851      12.4659     13.851     -10.\n",
            "  17.1         5.        -10.         15.39       17.1        19.\n",
            "  10.       ]\n"
          ]
        }
      ],
      "source": [
        "optimal_value_function = value_iteration(env)\n",
        "print('optimal value function', optimal_value_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1FJ_e8anxdr"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}