{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\madha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\madha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium) (1.23.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\madha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\madha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\madha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\madha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium) (7.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\madha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\madha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\madha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\madha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 25\n",
      "Number of actions: 4\n",
      "Initial state: 0\n",
      "Action taken: 2\n",
      "Next state: 0, Reward: 0, Done: False\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Custom map with treasure (T) and other tiles (F - Frozen, H - Hole, G - Goal)\n",
    "custom_map = [\n",
    "    \"SFFHT\",  # Start position (S), Frozen Tiles (F)\n",
    "    \"FHFFF\",  # Hole (H), Frozen (F), Treasure (T)\n",
    "    \"FFFTF\",  # Frozen (F), Hole (H), Frozen (F)\n",
    "    \"TFHFF\",  # Hole (H), Treasure (T), Frozen (F)\n",
    "    \"FFFFG\",  # Frozen (F), Frozen (F), Hole (H), Goal (G)\n",
    "]\n",
    "\n",
    "class CustomFrozenLakeEnv(gym.Env):\n",
    "    def __init__(self, desc=None, is_slippery=True):\n",
    "        super().__init__()\n",
    "        self.desc = np.array(desc or custom_map, dtype='c')\n",
    "        self.is_slippery = is_slippery\n",
    "        self.reward_range = (-float('inf'), float('inf'))\n",
    "\n",
    "        # Custom action and state space\n",
    "        self.nrow, self.ncol = self.desc.shape\n",
    "        self.nS = self.nrow * self.ncol  # Total number of states (5x5 grid)\n",
    "        self.nA = 4  # 4 actions (up, down, left, right)\n",
    "\n",
    "        # Initialize state to start position 'S'\n",
    "        self.state = 0  # The state represents the index of 'S' in the grid\n",
    "        self.done = False\n",
    "        self.treasures_collected = set()  # Track collected treasures\n",
    "        \n",
    "        # Action space: 4 actions (Up, Down, Left, Right)\n",
    "        self.action_space = gym.spaces.Discrete(self.nA)\n",
    "        \n",
    "        # Observation space: 5x5 grid (flattened to 25 states)\n",
    "        self.observation_space = gym.spaces.Discrete(self.nS)\n",
    "        \n",
    "        # Transition model: store transition probabilities\n",
    "        self.P = {s: {a: [] for a in range(self.nA)} for s in range(self.nS)}\n",
    "\n",
    "        self._build_transition_model()\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self.state, 0, True, {}\n",
    "\n",
    "        row, col = divmod(self.state, self.ncol)\n",
    "\n",
    "        if action == 0:  # Up\n",
    "            new_row, new_col = max(row - 1, 0), col\n",
    "        elif action == 1:  # Down\n",
    "            new_row, new_col = min(row + 1, self.nrow - 1), col\n",
    "        elif action == 2:  # Left\n",
    "            new_row, new_col = row, max(col - 1, 0)\n",
    "        elif action == 3:  # Right\n",
    "            new_row, new_col = row, min(col + 1, self.ncol - 1)\n",
    "        \n",
    "        # Get the new state index\n",
    "        new_state = new_row * self.ncol + new_col\n",
    "        tile = self.desc[new_row, new_col].decode(\"utf-8\")\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if tile == 'H':  # If the agent falls into a hole, game over\n",
    "            done = True\n",
    "            reward = -1\n",
    "        elif tile == 'G':  # If the agent reaches the goal, game over with a success reward\n",
    "            done = True\n",
    "            reward = 1\n",
    "        elif tile == 'T':  # If the agent steps on a treasure, give a +5 reward\n",
    "            reward = 5\n",
    "            # Mark treasure as collected\n",
    "            self.treasures_collected.add(new_state)\n",
    "            # After stepping on a treasure, it becomes frozen (F)\n",
    "            self.desc[new_row, new_col] = b'F'\n",
    "\n",
    "        # Update the state\n",
    "        self.state = new_state\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0  # Reset to the start position (state 0)\n",
    "        self.done = False\n",
    "        self.treasures_collected.clear()  # Reset collected treasures\n",
    "        return self.state\n",
    "\n",
    "    def render(self):\n",
    "        row, col = divmod(self.state, self.ncol)\n",
    "        print(f\"Current state: Position ({row}, {col})\")\n",
    "        print(f\"Treasures collected: {len(self.treasures_collected)}\")\n",
    "        print(f\"Current grid:\")\n",
    "        print(self.desc)\n",
    "\n",
    "    def _build_transition_model(self):\n",
    "        for row in range(self.nrow):\n",
    "            for col in range(self.ncol):\n",
    "                state = row * self.ncol + col  # Position index\n",
    "                for action in range(self.nA):\n",
    "                    next_state, reward, done = self._transition(row, col, action)\n",
    "                    self.P[state][action].append((0.25, next_state, reward, done))\n",
    "\n",
    "    def _transition(self, row, col, action):\n",
    "        if action == 0:  # Up\n",
    "            new_row, new_col = max(row - 1, 0), col\n",
    "        elif action == 1:  # Down\n",
    "            new_row, new_col = min(row + 1, self.nrow - 1), col\n",
    "        elif action == 2:  # Left\n",
    "            new_row, new_col = row, max(col - 1, 0)\n",
    "        elif action == 3:  # Right\n",
    "            new_row, new_col = row, min(col + 1, self.ncol - 1)\n",
    "\n",
    "        new_state = new_row * self.ncol + new_col\n",
    "        tile = self.desc[new_row, new_col].decode(\"utf-8\")\n",
    "\n",
    "        if tile == 'H':  # Hole\n",
    "            reward = -10\n",
    "            done = True\n",
    "        elif tile == 'G':  # Goal\n",
    "            reward = 10\n",
    "            done = True\n",
    "        elif tile == 'T':  # Treasure\n",
    "            reward = 5\n",
    "            done = False\n",
    "        else:  # Frozen Tile\n",
    "            reward = 0\n",
    "            done = False\n",
    "\n",
    "        return new_state, reward, done\n",
    "\n",
    "\n",
    "# Create the custom FrozenLake environment\n",
    "env = CustomFrozenLakeEnv(desc=custom_map, is_slippery=True)\n",
    "\n",
    "# Check the environment properties\n",
    "print(f\"Number of states: {env.observation_space.n}\")  # 25 states for a 5x5 grid\n",
    "print(f\"Number of actions: {env.action_space.n}\")  # 4 actions (up, down, left, right)\n",
    "\n",
    "# Reset the environment to start\n",
    "state = env.reset()\n",
    "print(f\"Initial state: {state}\")\n",
    "\n",
    "# Example interaction: Take a random action\n",
    "action = env.action_space.sample()  # Random action\n",
    "next_state, reward, done, info = env.step(action)\n",
    "\n",
    "print(f\"Action taken: {action}\")\n",
    "print(f\"Next state: {next_state}, Reward: {reward}, Done: {done}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "4\n",
      "State 0:\n",
      "  Action 0: Probability=0.25, Next State=0, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=5, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=0, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=1, Reward=0, Done=False\n",
      "State 1:\n",
      "  Action 0: Probability=0.25, Next State=1, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=6, Reward=-10, Done=True\n",
      "  Action 2: Probability=0.25, Next State=0, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=2, Reward=0, Done=False\n",
      "State 2:\n",
      "  Action 0: Probability=0.25, Next State=2, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=7, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=1, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=3, Reward=-10, Done=True\n",
      "State 3:\n",
      "  Action 0: Probability=0.25, Next State=3, Reward=-10, Done=True\n",
      "  Action 1: Probability=0.25, Next State=8, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=2, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=4, Reward=5, Done=False\n",
      "State 4:\n",
      "  Action 0: Probability=0.25, Next State=4, Reward=5, Done=False\n",
      "  Action 1: Probability=0.25, Next State=9, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=3, Reward=-10, Done=True\n",
      "  Action 3: Probability=0.25, Next State=4, Reward=5, Done=False\n",
      "State 5:\n",
      "  Action 0: Probability=0.25, Next State=0, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=10, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=5, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=6, Reward=-10, Done=True\n",
      "State 6:\n",
      "  Action 0: Probability=0.25, Next State=1, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=11, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=5, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=7, Reward=0, Done=False\n",
      "State 7:\n",
      "  Action 0: Probability=0.25, Next State=2, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=12, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=6, Reward=-10, Done=True\n",
      "  Action 3: Probability=0.25, Next State=8, Reward=0, Done=False\n",
      "State 8:\n",
      "  Action 0: Probability=0.25, Next State=3, Reward=-10, Done=True\n",
      "  Action 1: Probability=0.25, Next State=13, Reward=5, Done=False\n",
      "  Action 2: Probability=0.25, Next State=7, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=9, Reward=0, Done=False\n",
      "State 9:\n",
      "  Action 0: Probability=0.25, Next State=4, Reward=5, Done=False\n",
      "  Action 1: Probability=0.25, Next State=14, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=8, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=9, Reward=0, Done=False\n",
      "State 10:\n",
      "  Action 0: Probability=0.25, Next State=5, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=15, Reward=5, Done=False\n",
      "  Action 2: Probability=0.25, Next State=10, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=11, Reward=0, Done=False\n",
      "State 11:\n",
      "  Action 0: Probability=0.25, Next State=6, Reward=-10, Done=True\n",
      "  Action 1: Probability=0.25, Next State=16, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=10, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=12, Reward=0, Done=False\n",
      "State 12:\n",
      "  Action 0: Probability=0.25, Next State=7, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=17, Reward=-10, Done=True\n",
      "  Action 2: Probability=0.25, Next State=11, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=13, Reward=5, Done=False\n",
      "State 13:\n",
      "  Action 0: Probability=0.25, Next State=8, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=18, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=12, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=14, Reward=0, Done=False\n",
      "State 14:\n",
      "  Action 0: Probability=0.25, Next State=9, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=19, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=13, Reward=5, Done=False\n",
      "  Action 3: Probability=0.25, Next State=14, Reward=0, Done=False\n",
      "State 15:\n",
      "  Action 0: Probability=0.25, Next State=10, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=20, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=15, Reward=5, Done=False\n",
      "  Action 3: Probability=0.25, Next State=16, Reward=0, Done=False\n",
      "State 16:\n",
      "  Action 0: Probability=0.25, Next State=11, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=21, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=15, Reward=5, Done=False\n",
      "  Action 3: Probability=0.25, Next State=17, Reward=-10, Done=True\n",
      "State 17:\n",
      "  Action 0: Probability=0.25, Next State=12, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=22, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=16, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=18, Reward=0, Done=False\n",
      "State 18:\n",
      "  Action 0: Probability=0.25, Next State=13, Reward=5, Done=False\n",
      "  Action 1: Probability=0.25, Next State=23, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=17, Reward=-10, Done=True\n",
      "  Action 3: Probability=0.25, Next State=19, Reward=0, Done=False\n",
      "State 19:\n",
      "  Action 0: Probability=0.25, Next State=14, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=24, Reward=10, Done=True\n",
      "  Action 2: Probability=0.25, Next State=18, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=19, Reward=0, Done=False\n",
      "State 20:\n",
      "  Action 0: Probability=0.25, Next State=15, Reward=5, Done=False\n",
      "  Action 1: Probability=0.25, Next State=20, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=20, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=21, Reward=0, Done=False\n",
      "State 21:\n",
      "  Action 0: Probability=0.25, Next State=16, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=21, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=20, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=22, Reward=0, Done=False\n",
      "State 22:\n",
      "  Action 0: Probability=0.25, Next State=17, Reward=-10, Done=True\n",
      "  Action 1: Probability=0.25, Next State=22, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=21, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=23, Reward=0, Done=False\n",
      "State 23:\n",
      "  Action 0: Probability=0.25, Next State=18, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=23, Reward=0, Done=False\n",
      "  Action 2: Probability=0.25, Next State=22, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=24, Reward=10, Done=True\n",
      "State 24:\n",
      "  Action 0: Probability=0.25, Next State=19, Reward=0, Done=False\n",
      "  Action 1: Probability=0.25, Next State=24, Reward=10, Done=True\n",
      "  Action 2: Probability=0.25, Next State=23, Reward=0, Done=False\n",
      "  Action 3: Probability=0.25, Next State=24, Reward=10, Done=True\n"
     ]
    }
   ],
   "source": [
    "nb_states = env.observation_space.n\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "print(nb_states)\n",
    "print(nb_actions)\n",
    "# Check transition model (env.P)\n",
    "for state in env.P:\n",
    "    print(f\"State {state}:\")\n",
    "    for action in env.P[state]:\n",
    "        for prob, next_state, reward, done in env.P[state][action]:\n",
    "            print(f\"  Action {action}: Probability={prob}, Next State={next_state}, Reward={reward}, Done={done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: Position (0, 0)\n",
      "Treasures collected: 0\n",
      "Current grid:\n",
      "[[b'S' b'F' b'F' b'H' b'T']\n",
      " [b'F' b'H' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'T' b'F']\n",
      " [b'T' b'F' b'H' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'G']]\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Value Table:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "Value Table for State 0 0.0\n",
      "Value Table for State 1 0.0\n",
      "Value Table for State 2 0.0\n",
      "Value Table for State 3 1.25\n",
      "Value Table for State 4 1.25\n",
      "Value Table for State 5 0.0\n",
      "Value Table for State 6 0.0\n",
      "Value Table for State 7 0.0\n",
      "Value Table for State 8 1.25\n",
      "Value Table for State 9 1.25\n",
      "Value Table for State 10 1.25\n",
      "Value Table for State 11 0.0\n",
      "Value Table for State 12 1.25\n",
      "Value Table for State 13 0.0\n",
      "Value Table for State 14 1.25\n",
      "Value Table for State 15 1.25\n",
      "Value Table for State 16 1.25\n",
      "Value Table for State 17 0.0\n",
      "Value Table for State 18 1.25\n",
      "Value Table for State 19 2.5\n",
      "Value Table for State 20 1.25\n",
      "Value Table for State 21 0.0\n",
      "Value Table for State 22 0.0\n",
      "Value Table for State 23 2.5\n",
      "Value Table for State 24 2.5\n",
      "Value Table for State 0 0.0\n",
      "Value Table for State 1 0.0\n",
      "Value Table for State 2 0.0\n",
      "Value Table for State 3 1.4375\n",
      "Value Table for State 4 1.4375\n",
      "Value Table for State 5 0.1875\n",
      "Value Table for State 6 0.0\n",
      "Value Table for State 7 0.1875\n",
      "Value Table for State 8 1.25\n",
      "Value Table for State 9 1.4375\n",
      "Value Table for State 10 1.4375\n",
      "Value Table for State 11 0.1875\n",
      "Value Table for State 12 1.25\n",
      "Value Table for State 13 0.1875\n",
      "Value Table for State 14 1.25\n",
      "Value Table for State 15 1.4375\n",
      "Value Table for State 16 1.4375\n",
      "Value Table for State 17 0.1875\n",
      "Value Table for State 18 1.25\n",
      "Value Table for State 19 2.875\n",
      "Value Table for State 20 1.4375\n",
      "Value Table for State 21 0.1875\n",
      "Value Table for State 22 0.375\n",
      "Value Table for State 23 2.875\n",
      "Value Table for State 24 2.875\n",
      "Value Table for State 0 0.028124999999999997\n",
      "Value Table for State 1 0.0\n",
      "Value Table for State 2 0.028124999999999997\n",
      "Value Table for State 3 1.465625\n",
      "Value Table for State 4 1.465625\n",
      "Value Table for State 5 0.21562499999999998\n",
      "Value Table for State 6 0.028124999999999997\n",
      "Value Table for State 7 0.1875\n",
      "Value Table for State 8 1.278125\n",
      "Value Table for State 9 1.465625\n",
      "Value Table for State 10 1.465625\n",
      "Value Table for State 11 0.21562499999999998\n",
      "Value Table for State 12 1.278125\n",
      "Value Table for State 13 0.1875\n",
      "Value Table for State 14 1.278125\n",
      "Value Table for State 15 1.465625\n",
      "Value Table for State 16 1.465625\n",
      "Value Table for State 17 0.21562499999999998\n",
      "Value Table for State 18 1.278125\n",
      "Value Table for State 19 2.93125\n",
      "Value Table for State 20 1.465625\n",
      "Value Table for State 21 0.21562499999999998\n",
      "Value Table for State 22 0.43124999999999997\n",
      "Value Table for State 23 2.93125\n",
      "Value Table for State 24 2.93125\n",
      "Value Table for State 0 0.03234375\n",
      "Value Table for State 1 0.004218749999999999\n",
      "Value Table for State 2 0.028124999999999997\n",
      "Value Table for State 3 1.46984375\n",
      "Value Table for State 4 1.46984375\n",
      "Value Table for State 5 0.21984374999999998\n",
      "Value Table for State 6 0.03234375\n",
      "Value Table for State 7 0.19171875\n",
      "Value Table for State 8 1.278125\n",
      "Value Table for State 9 1.46984375\n",
      "Value Table for State 10 1.46984375\n",
      "Value Table for State 11 0.21984374999999998\n",
      "Value Table for State 12 1.278125\n",
      "Value Table for State 13 0.19171875\n",
      "Value Table for State 14 1.278125\n",
      "Value Table for State 15 1.46984375\n",
      "Value Table for State 16 1.46984375\n",
      "Value Table for State 17 0.21984374999999998\n",
      "Value Table for State 18 1.278125\n",
      "Value Table for State 19 2.9396875\n",
      "Value Table for State 20 1.46984375\n",
      "Value Table for State 21 0.21984374999999998\n",
      "Value Table for State 22 0.43968749999999995\n",
      "Value Table for State 23 2.9396875\n",
      "Value Table for State 24 2.9396875\n",
      "Value Table for State 0 0.032976562499999994\n",
      "Value Table for State 1 0.004851562499999999\n",
      "Value Table for State 2 0.028757812499999997\n",
      "Value Table for State 3 1.4704765625\n",
      "Value Table for State 4 1.4704765625\n",
      "Value Table for State 5 0.22047656249999997\n",
      "Value Table for State 6 0.032976562499999994\n",
      "Value Table for State 7 0.19171875\n",
      "Value Table for State 8 1.2787578125\n",
      "Value Table for State 9 1.4704765625\n",
      "Value Table for State 10 1.4704765625\n",
      "Value Table for State 11 0.22047656249999997\n",
      "Value Table for State 12 1.2787578125\n",
      "Value Table for State 13 0.19171875\n",
      "Value Table for State 14 1.2787578125\n",
      "Value Table for State 15 1.4704765625\n",
      "Value Table for State 16 1.4704765625\n",
      "Value Table for State 17 0.22047656249999997\n",
      "Value Table for State 18 1.2787578125\n",
      "Value Table for State 19 2.940953125\n",
      "Value Table for State 20 1.4704765625\n",
      "Value Table for State 21 0.22047656249999997\n",
      "Value Table for State 22 0.44095312499999995\n",
      "Value Table for State 23 2.940953125\n",
      "Value Table for State 24 2.940953125\n",
      "Value Table for State 0 0.033071484374999995\n",
      "Value Table for State 1 0.004946484374999999\n",
      "Value Table for State 2 0.028757812499999997\n",
      "Value Table for State 3 1.470571484375\n",
      "Value Table for State 4 1.470571484375\n",
      "Value Table for State 5 0.220571484375\n",
      "Value Table for State 6 0.033071484374999995\n",
      "Value Table for State 7 0.191813671875\n",
      "Value Table for State 8 1.2787578125\n",
      "Value Table for State 9 1.470571484375\n",
      "Value Table for State 10 1.470571484375\n",
      "Value Table for State 11 0.220571484375\n",
      "Value Table for State 12 1.2787578125\n",
      "Value Table for State 13 0.191813671875\n",
      "Value Table for State 14 1.2787578125\n",
      "Value Table for State 15 1.470571484375\n",
      "Value Table for State 16 1.470571484375\n",
      "Value Table for State 17 0.220571484375\n",
      "Value Table for State 18 1.2787578125\n",
      "Value Table for State 19 2.94114296875\n",
      "Value Table for State 20 1.470571484375\n",
      "Value Table for State 21 0.220571484375\n",
      "Value Table for State 22 0.44114296875\n",
      "Value Table for State 23 2.94114296875\n",
      "Value Table for State 24 2.94114296875\n",
      "Value Table for State 0 0.03308572265625\n",
      "Value Table for State 1 0.004960722656249999\n",
      "Value Table for State 2 0.02877205078125\n",
      "Value Table for State 3 1.47058572265625\n",
      "Value Table for State 4 1.47058572265625\n",
      "Value Table for State 5 0.22058572265624998\n",
      "Value Table for State 6 0.03308572265625\n",
      "Value Table for State 7 0.191813671875\n",
      "Value Table for State 8 1.27877205078125\n",
      "Value Table for State 9 1.47058572265625\n",
      "Value Table for State 10 1.47058572265625\n",
      "Value Table for State 11 0.22058572265624998\n",
      "Value Table for State 12 1.27877205078125\n",
      "Value Table for State 13 0.191813671875\n",
      "Value Table for State 14 1.27877205078125\n",
      "Value Table for State 15 1.47058572265625\n",
      "Value Table for State 16 1.47058572265625\n",
      "Value Table for State 17 0.22058572265624998\n",
      "Value Table for State 18 1.27877205078125\n",
      "Value Table for State 19 2.9411714453125\n",
      "Value Table for State 20 1.47058572265625\n",
      "Value Table for State 21 0.22058572265624998\n",
      "Value Table for State 22 0.44117144531249997\n",
      "Value Table for State 23 2.9411714453125\n",
      "Value Table for State 24 2.9411714453125\n",
      "Value Table for State 0 0.033087858398437496\n",
      "Value Table for State 1 0.0049628583984375\n",
      "Value Table for State 2 0.02877205078125\n",
      "Value Table for State 3 1.4705878583984375\n",
      "Value Table for State 4 1.4705878583984375\n",
      "Value Table for State 5 0.2205878583984375\n",
      "Value Table for State 6 0.033087858398437496\n",
      "Value Table for State 7 0.19181580761718747\n",
      "Value Table for State 8 1.27877205078125\n",
      "Value Table for State 9 1.4705878583984375\n",
      "Value Table for State 10 1.4705878583984375\n",
      "Value Table for State 11 0.2205878583984375\n",
      "Value Table for State 12 1.27877205078125\n",
      "Value Table for State 13 0.19181580761718747\n",
      "Value Table for State 14 1.27877205078125\n",
      "Value Table for State 15 1.4705878583984375\n",
      "Value Table for State 16 1.4705878583984375\n",
      "Value Table for State 17 0.2205878583984375\n",
      "Value Table for State 18 1.27877205078125\n",
      "Value Table for State 19 2.941175716796875\n",
      "Value Table for State 20 1.4705878583984375\n",
      "Value Table for State 21 0.2205878583984375\n",
      "Value Table for State 22 0.441175716796875\n",
      "Value Table for State 23 2.941175716796875\n",
      "Value Table for State 24 2.941175716796875\n",
      "Value Table for State 0 0.03308817875976562\n",
      "Value Table for State 1 0.004963178759765624\n",
      "Value Table for State 2 0.02877237114257812\n",
      "Value Table for State 3 1.4705881787597657\n",
      "Value Table for State 4 1.4705881787597657\n",
      "Value Table for State 5 0.22058817875976564\n",
      "Value Table for State 6 0.03308817875976562\n",
      "Value Table for State 7 0.19181580761718747\n",
      "Value Table for State 8 1.278772371142578\n",
      "Value Table for State 9 1.4705881787597657\n",
      "Value Table for State 10 1.4705881787597657\n",
      "Value Table for State 11 0.22058817875976564\n",
      "Value Table for State 12 1.278772371142578\n",
      "Value Table for State 13 0.19181580761718747\n",
      "Value Table for State 14 1.278772371142578\n",
      "Value Table for State 15 1.4705881787597657\n",
      "Value Table for State 16 1.4705881787597657\n",
      "Value Table for State 17 0.22058817875976564\n",
      "Value Table for State 18 1.278772371142578\n",
      "Value Table for State 19 2.9411763575195313\n",
      "Value Table for State 20 1.4705881787597657\n",
      "Value Table for State 21 0.22058817875976564\n",
      "Value Table for State 22 0.44117635751953127\n",
      "Value Table for State 23 2.9411763575195313\n",
      "Value Table for State 24 2.9411763575195313\n",
      "Value Table for State 0 0.033088226813964845\n",
      "Value Table for State 1 0.004963226813964843\n",
      "Value Table for State 2 0.02877237114257812\n",
      "Value Table for State 3 1.4705882268139647\n",
      "Value Table for State 4 1.4705882268139647\n",
      "Value Table for State 5 0.22058822681396484\n",
      "Value Table for State 6 0.033088226813964845\n",
      "Value Table for State 7 0.1918158556713867\n",
      "Value Table for State 8 1.278772371142578\n",
      "Value Table for State 9 1.4705882268139647\n",
      "Value Table for State 10 1.4705882268139647\n",
      "Value Table for State 11 0.22058822681396484\n",
      "Value Table for State 12 1.278772371142578\n",
      "Value Table for State 13 0.1918158556713867\n",
      "Value Table for State 14 1.278772371142578\n",
      "Value Table for State 15 1.4705882268139647\n",
      "Value Table for State 16 1.4705882268139647\n",
      "Value Table for State 17 0.22058822681396484\n",
      "Value Table for State 18 1.278772371142578\n",
      "Value Table for State 19 2.9411764536279295\n",
      "Value Table for State 20 1.4705882268139647\n",
      "Value Table for State 21 0.22058822681396484\n",
      "Value Table for State 22 0.4411764536279297\n",
      "Value Table for State 23 2.9411764536279295\n",
      "Value Table for State 24 2.9411764536279295\n",
      "Value Table for State 0 0.03308823402209472\n",
      "Value Table for State 1 0.004963234022094727\n",
      "Value Table for State 2 0.028772378350708006\n",
      "Value Table for State 3 1.4705882340220948\n",
      "Value Table for State 4 1.4705882340220948\n",
      "Value Table for State 5 0.2205882340220947\n",
      "Value Table for State 6 0.03308823402209472\n",
      "Value Table for State 7 0.1918158556713867\n",
      "Value Table for State 8 1.2787723783507081\n",
      "Value Table for State 9 1.4705882340220948\n",
      "Value Table for State 10 1.4705882340220948\n",
      "Value Table for State 11 0.2205882340220947\n",
      "Value Table for State 12 1.2787723783507081\n",
      "Value Table for State 13 0.1918158556713867\n",
      "Value Table for State 14 1.2787723783507081\n",
      "Value Table for State 15 1.4705882340220948\n",
      "Value Table for State 16 1.4705882340220948\n",
      "Value Table for State 17 0.2205882340220947\n",
      "Value Table for State 18 1.2787723783507081\n",
      "Value Table for State 19 2.9411764680441896\n",
      "Value Table for State 20 1.4705882340220948\n",
      "Value Table for State 21 0.2205882340220947\n",
      "Value Table for State 22 0.4411764680441894\n",
      "Value Table for State 23 2.9411764680441896\n",
      "Value Table for State 24 2.9411764680441896\n",
      "Value Table for State 0 0.033088235103314205\n",
      "Value Table for State 1 0.004963235103314209\n",
      "Value Table for State 2 0.028772378350708006\n",
      "Value Table for State 3 1.4705882351033142\n",
      "Value Table for State 4 1.4705882351033142\n",
      "Value Table for State 5 0.2205882351033142\n",
      "Value Table for State 6 0.033088235103314205\n",
      "Value Table for State 7 0.1918158567526062\n",
      "Value Table for State 8 1.2787723783507081\n",
      "Value Table for State 9 1.4705882351033142\n",
      "Value Table for State 10 1.4705882351033142\n",
      "Value Table for State 11 0.2205882351033142\n",
      "Value Table for State 12 1.2787723783507081\n",
      "Value Table for State 13 0.1918158567526062\n",
      "Value Table for State 14 1.2787723783507081\n",
      "Value Table for State 15 1.4705882351033142\n",
      "Value Table for State 16 1.4705882351033142\n",
      "Value Table for State 17 0.2205882351033142\n",
      "Value Table for State 18 1.2787723783507081\n",
      "Value Table for State 19 2.9411764702066283\n",
      "Value Table for State 20 1.4705882351033142\n",
      "Value Table for State 21 0.2205882351033142\n",
      "Value Table for State 22 0.4411764702066284\n",
      "Value Table for State 23 2.9411764702066283\n",
      "Value Table for State 24 2.9411764702066283\n",
      "Value Table for State 0 0.03308823526549713\n",
      "Value Table for State 1 0.00496323526549713\n",
      "Value Table for State 2 0.02877237851289093\n",
      "Value Table for State 3 1.470588235265497\n",
      "Value Table for State 4 1.470588235265497\n",
      "Value Table for State 5 0.22058823526549712\n",
      "Value Table for State 6 0.03308823526549713\n",
      "Value Table for State 7 0.1918158567526062\n",
      "Value Table for State 8 1.2787723785128908\n",
      "Value Table for State 9 1.470588235265497\n",
      "Value Table for State 10 1.470588235265497\n",
      "Value Table for State 11 0.22058823526549712\n",
      "Value Table for State 12 1.2787723785128908\n",
      "Value Table for State 13 0.1918158567526062\n",
      "Value Table for State 14 1.2787723785128908\n",
      "Value Table for State 15 1.470588235265497\n",
      "Value Table for State 16 1.470588235265497\n",
      "Value Table for State 17 0.22058823526549712\n",
      "Value Table for State 18 1.2787723785128908\n",
      "Value Table for State 19 2.941176470530994\n",
      "Value Table for State 20 1.470588235265497\n",
      "Value Table for State 21 0.22058823526549712\n",
      "Value Table for State 22 0.44117647053099424\n",
      "Value Table for State 23 2.941176470530994\n",
      "Value Table for State 24 2.941176470530994\n",
      "Value Table for State 0 0.03308823528982457\n",
      "Value Table for State 1 0.00496323528982457\n",
      "Value Table for State 2 0.02877237851289093\n",
      "Value Table for State 3 1.4705882352898245\n",
      "Value Table for State 4 1.4705882352898245\n",
      "Value Table for State 5 0.22058823528982455\n",
      "Value Table for State 6 0.03308823528982457\n",
      "Value Table for State 7 0.1918158567769336\n",
      "Value Table for State 8 1.2787723785128908\n",
      "Value Table for State 9 1.4705882352898245\n",
      "Value Table for State 10 1.4705882352898245\n",
      "Value Table for State 11 0.22058823528982455\n",
      "Value Table for State 12 1.2787723785128908\n",
      "Value Table for State 13 0.1918158567769336\n",
      "Value Table for State 14 1.2787723785128908\n",
      "Value Table for State 15 1.4705882352898245\n",
      "Value Table for State 16 1.4705882352898245\n",
      "Value Table for State 17 0.22058823528982455\n",
      "Value Table for State 18 1.2787723785128908\n",
      "Value Table for State 19 2.941176470579649\n",
      "Value Table for State 20 1.4705882352898245\n",
      "Value Table for State 21 0.22058823528982455\n",
      "Value Table for State 22 0.4411764705796491\n",
      "Value Table for State 23 2.941176470579649\n",
      "Value Table for State 24 2.941176470579649\n",
      "Value Table for State 0 0.03308823529347368\n",
      "Value Table for State 1 0.004963235293473685\n",
      "Value Table for State 2 0.02877237851654004\n",
      "Value Table for State 3 1.4705882352934736\n",
      "Value Table for State 4 1.4705882352934736\n",
      "Value Table for State 5 0.22058823529347368\n",
      "Value Table for State 6 0.03308823529347368\n",
      "Value Table for State 7 0.1918158567769336\n",
      "Value Table for State 8 1.2787723785165401\n",
      "Value Table for State 9 1.4705882352934736\n",
      "Value Table for State 10 1.4705882352934736\n",
      "Value Table for State 11 0.22058823529347368\n",
      "Value Table for State 12 1.2787723785165401\n",
      "Value Table for State 13 0.1918158567769336\n",
      "Value Table for State 14 1.2787723785165401\n",
      "Value Table for State 15 1.4705882352934736\n",
      "Value Table for State 16 1.4705882352934736\n",
      "Value Table for State 17 0.22058823529347368\n",
      "Value Table for State 18 1.2787723785165401\n",
      "Value Table for State 19 2.941176470586947\n",
      "Value Table for State 20 1.4705882352934736\n",
      "Value Table for State 21 0.22058823529347368\n",
      "Value Table for State 22 0.44117647058694737\n",
      "Value Table for State 23 2.941176470586947\n",
      "Value Table for State 24 2.941176470586947\n",
      "Value Table for State 0 0.03308823529402105\n",
      "Value Table for State 1 0.004963235294021052\n",
      "Value Table for State 2 0.02877237851654004\n",
      "Value Table for State 3 1.470588235294021\n",
      "Value Table for State 4 1.470588235294021\n",
      "Value Table for State 5 0.22058823529402102\n",
      "Value Table for State 6 0.03308823529402105\n",
      "Value Table for State 7 0.191815856777481\n",
      "Value Table for State 8 1.2787723785165401\n",
      "Value Table for State 9 1.470588235294021\n",
      "Value Table for State 10 1.470588235294021\n",
      "Value Table for State 11 0.22058823529402102\n",
      "Value Table for State 12 1.2787723785165401\n",
      "Value Table for State 13 0.191815856777481\n",
      "Value Table for State 14 1.2787723785165401\n",
      "Value Table for State 15 1.470588235294021\n",
      "Value Table for State 16 1.470588235294021\n",
      "Value Table for State 17 0.22058823529402102\n",
      "Value Table for State 18 1.2787723785165401\n",
      "Value Table for State 19 2.941176470588042\n",
      "Value Table for State 20 1.470588235294021\n",
      "Value Table for State 21 0.22058823529402102\n",
      "Value Table for State 22 0.44117647058804205\n",
      "Value Table for State 23 2.941176470588042\n",
      "Value Table for State 24 2.941176470588042\n",
      "Value Table for State 0 0.03308823529410315\n",
      "Value Table for State 1 0.0049632352941031575\n",
      "Value Table for State 2 0.02877237851662215\n",
      "Value Table for State 3 1.470588235294103\n",
      "Value Table for State 4 1.470588235294103\n",
      "Value Table for State 5 0.22058823529410312\n",
      "Value Table for State 6 0.03308823529410315\n",
      "Value Table for State 7 0.191815856777481\n",
      "Value Table for State 8 1.278772378516622\n",
      "Value Table for State 9 1.470588235294103\n",
      "Value Table for State 10 1.470588235294103\n",
      "Value Table for State 11 0.22058823529410312\n",
      "Value Table for State 12 1.278772378516622\n",
      "Value Table for State 13 0.191815856777481\n",
      "Value Table for State 14 1.278772378516622\n",
      "Value Table for State 15 1.470588235294103\n",
      "Value Table for State 16 1.470588235294103\n",
      "Value Table for State 17 0.22058823529410312\n",
      "Value Table for State 18 1.278772378516622\n",
      "Value Table for State 19 2.941176470588206\n",
      "Value Table for State 20 1.470588235294103\n",
      "Value Table for State 21 0.22058823529410312\n",
      "Value Table for State 22 0.44117647058820625\n",
      "Value Table for State 23 2.941176470588206\n",
      "Value Table for State 24 2.941176470588206\n",
      "Value Table for State 0 0.03308823529411547\n",
      "Value Table for State 1 0.004963235294115472\n",
      "Value Table for State 2 0.02877237851662215\n",
      "Value Table for State 3 1.4705882352941155\n",
      "Value Table for State 4 1.4705882352941155\n",
      "Value Table for State 5 0.22058823529411545\n",
      "Value Table for State 6 0.03308823529411547\n",
      "Value Table for State 7 0.1918158567774933\n",
      "Value Table for State 8 1.278772378516622\n",
      "Value Table for State 9 1.4705882352941155\n",
      "Value Table for State 10 1.4705882352941155\n",
      "Value Table for State 11 0.22058823529411545\n",
      "Value Table for State 12 1.278772378516622\n",
      "Value Table for State 13 0.1918158567774933\n",
      "Value Table for State 14 1.278772378516622\n",
      "Value Table for State 15 1.4705882352941155\n",
      "Value Table for State 16 1.4705882352941155\n",
      "Value Table for State 17 0.22058823529411545\n",
      "Value Table for State 18 1.278772378516622\n",
      "Value Table for State 19 2.941176470588231\n",
      "Value Table for State 20 1.4705882352941155\n",
      "Value Table for State 21 0.22058823529411545\n",
      "Value Table for State 22 0.4411764705882309\n",
      "Value Table for State 23 2.941176470588231\n",
      "Value Table for State 24 2.941176470588231\n",
      "Value Table for State 0 0.033088235294117314\n",
      "Value Table for State 1 0.00496323529411732\n",
      "Value Table for State 2 0.028772378516623995\n",
      "Value Table for State 3 1.4705882352941173\n",
      "Value Table for State 4 1.4705882352941173\n",
      "Value Table for State 5 0.22058823529411734\n",
      "Value Table for State 6 0.033088235294117314\n",
      "Value Table for State 7 0.1918158567774933\n",
      "Value Table for State 8 1.278772378516624\n",
      "Value Table for State 9 1.4705882352941173\n",
      "Value Table for State 10 1.4705882352941173\n",
      "Value Table for State 11 0.22058823529411734\n",
      "Value Table for State 12 1.278772378516624\n",
      "Value Table for State 13 0.1918158567774933\n",
      "Value Table for State 14 1.278772378516624\n",
      "Value Table for State 15 1.4705882352941173\n",
      "Value Table for State 16 1.4705882352941173\n",
      "Value Table for State 17 0.22058823529411734\n",
      "Value Table for State 18 1.278772378516624\n",
      "Value Table for State 19 2.9411764705882346\n",
      "Value Table for State 20 1.4705882352941173\n",
      "Value Table for State 21 0.22058823529411734\n",
      "Value Table for State 22 0.44117647058823467\n",
      "Value Table for State 23 2.9411764705882346\n",
      "Value Table for State 24 2.9411764705882346\n",
      "Value Table for State 0 0.0330882352941176\n",
      "Value Table for State 1 0.004963235294117597\n",
      "Value Table for State 2 0.028772378516623995\n",
      "Value Table for State 3 1.4705882352941175\n",
      "Value Table for State 4 1.4705882352941175\n",
      "Value Table for State 5 0.22058823529411759\n",
      "Value Table for State 6 0.0330882352941176\n",
      "Value Table for State 7 0.1918158567774936\n",
      "Value Table for State 8 1.278772378516624\n",
      "Value Table for State 9 1.4705882352941175\n",
      "Value Table for State 10 1.4705882352941175\n",
      "Value Table for State 11 0.22058823529411759\n",
      "Value Table for State 12 1.278772378516624\n",
      "Value Table for State 13 0.1918158567774936\n",
      "Value Table for State 14 1.278772378516624\n",
      "Value Table for State 15 1.4705882352941175\n",
      "Value Table for State 16 1.4705882352941175\n",
      "Value Table for State 17 0.22058823529411759\n",
      "Value Table for State 18 1.278772378516624\n",
      "Value Table for State 19 2.941176470588235\n",
      "Value Table for State 20 1.4705882352941175\n",
      "Value Table for State 21 0.22058823529411759\n",
      "Value Table for State 22 0.44117647058823517\n",
      "Value Table for State 23 2.941176470588235\n",
      "Value Table for State 24 2.941176470588235\n",
      "Value Table for State 0 0.033088235294117634\n",
      "Value Table for State 1 0.00496323529411764\n",
      "Value Table for State 2 0.02877237851662404\n",
      "Value Table for State 3 1.4705882352941175\n",
      "Value Table for State 4 1.4705882352941175\n",
      "Value Table for State 5 0.2205882352941176\n",
      "Value Table for State 6 0.033088235294117634\n",
      "Value Table for State 7 0.1918158567774936\n",
      "Value Table for State 8 1.278772378516624\n",
      "Value Table for State 9 1.4705882352941175\n",
      "Value Table for State 10 1.4705882352941175\n",
      "Value Table for State 11 0.2205882352941176\n",
      "Value Table for State 12 1.278772378516624\n",
      "Value Table for State 13 0.1918158567774936\n",
      "Value Table for State 14 1.278772378516624\n",
      "Value Table for State 15 1.4705882352941175\n",
      "Value Table for State 16 1.4705882352941175\n",
      "Value Table for State 17 0.2205882352941176\n",
      "Value Table for State 18 1.278772378516624\n",
      "Value Table for State 19 2.941176470588235\n",
      "Value Table for State 20 1.4705882352941175\n",
      "Value Table for State 21 0.2205882352941176\n",
      "Value Table for State 22 0.4411764705882352\n",
      "Value Table for State 23 2.941176470588235\n",
      "Value Table for State 24 2.941176470588235\n",
      "Value Table for State 0 0.03308823529411764\n",
      "Value Table for State 1 0.004963235294117645\n",
      "Value Table for State 2 0.02877237851662404\n",
      "Value Table for State 3 1.4705882352941175\n",
      "Value Table for State 4 1.4705882352941175\n",
      "Value Table for State 5 0.2205882352941176\n",
      "Value Table for State 6 0.03308823529411764\n",
      "Value Table for State 7 0.1918158567774936\n",
      "Value Table for State 8 1.278772378516624\n",
      "Value Table for State 9 1.4705882352941175\n",
      "Value Table for State 10 1.4705882352941175\n",
      "Value Table for State 11 0.2205882352941176\n",
      "Value Table for State 12 1.278772378516624\n",
      "Value Table for State 13 0.1918158567774936\n",
      "Value Table for State 14 1.278772378516624\n",
      "Value Table for State 15 1.4705882352941175\n",
      "Value Table for State 16 1.4705882352941175\n",
      "Value Table for State 17 0.2205882352941176\n",
      "Value Table for State 18 1.278772378516624\n",
      "Value Table for State 19 2.941176470588235\n",
      "Value Table for State 20 1.4705882352941175\n",
      "Value Table for State 21 0.2205882352941176\n",
      "Value Table for State 22 0.4411764705882352\n",
      "Value Table for State 23 2.941176470588235\n",
      "Value Table for State 24 2.941176470588235\n",
      "Value Table for State 0 0.03308823529411764\n",
      "Value Table for State 1 0.004963235294117646\n",
      "Value Table for State 2 0.02877237851662404\n",
      "Value Table for State 3 1.4705882352941175\n",
      "Value Table for State 4 1.4705882352941175\n",
      "Value Table for State 5 0.2205882352941176\n",
      "Value Table for State 6 0.03308823529411764\n",
      "Value Table for State 7 0.1918158567774936\n",
      "Value Table for State 8 1.278772378516624\n",
      "Value Table for State 9 1.4705882352941175\n",
      "Value Table for State 10 1.4705882352941175\n",
      "Value Table for State 11 0.2205882352941176\n",
      "Value Table for State 12 1.278772378516624\n",
      "Value Table for State 13 0.1918158567774936\n",
      "Value Table for State 14 1.278772378516624\n",
      "Value Table for State 15 1.4705882352941175\n",
      "Value Table for State 16 1.4705882352941175\n",
      "Value Table for State 17 0.2205882352941176\n",
      "Value Table for State 18 1.278772378516624\n",
      "Value Table for State 19 2.941176470588235\n",
      "Value Table for State 20 1.4705882352941175\n",
      "Value Table for State 21 0.2205882352941176\n",
      "Value Table for State 22 0.4411764705882352\n",
      "Value Table for State 23 2.941176470588235\n",
      "Value Table for State 24 2.941176470588235\n",
      "Value Table for State 0 0.03308823529411764\n",
      "Value Table for State 1 0.004963235294117646\n",
      "Value Table for State 2 0.02877237851662404\n",
      "Value Table for State 3 1.4705882352941175\n",
      "Value Table for State 4 1.4705882352941175\n",
      "Value Table for State 5 0.2205882352941176\n",
      "Value Table for State 6 0.03308823529411764\n",
      "Value Table for State 7 0.1918158567774936\n",
      "Value Table for State 8 1.278772378516624\n",
      "Value Table for State 9 1.4705882352941175\n",
      "Value Table for State 10 1.4705882352941175\n",
      "Value Table for State 11 0.2205882352941176\n",
      "Value Table for State 12 1.278772378516624\n",
      "Value Table for State 13 0.1918158567774936\n",
      "Value Table for State 14 1.278772378516624\n",
      "Value Table for State 15 1.4705882352941175\n",
      "Value Table for State 16 1.4705882352941175\n",
      "Value Table for State 17 0.2205882352941176\n",
      "Value Table for State 18 1.278772378516624\n",
      "Value Table for State 19 2.941176470588235\n",
      "Value Table for State 20 1.4705882352941175\n",
      "Value Table for State 21 0.2205882352941176\n",
      "Value Table for State 22 0.4411764705882352\n",
      "Value Table for State 23 2.941176470588235\n",
      "Value Table for State 24 2.941176470588235\n",
      "optimal value function [0.03308824 0.00496324 0.02877238 1.47058824 1.47058824 0.22058824\n",
      " 0.03308824 0.19181586 1.27877238 1.47058824 1.47058824 0.22058824\n",
      " 1.27877238 0.19181586 1.27877238 1.47058824 1.47058824 0.22058824\n",
      " 1.27877238 2.94117647 1.47058824 0.22058824 0.44117647 2.94117647\n",
      " 2.94117647]\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(env):\n",
    "  num_iterations = 100\n",
    "  threshold = 1e-20 #value used to terminate if no changes observed in the new values from the previous values\n",
    "  gamma = .6 #discount factor\n",
    "  value_table = np.zeros(env.observation_space.n)\n",
    "  print('Initial Value Table: ', value_table)\n",
    "  for i in range(num_iterations):\n",
    "    updated_value_table = np.copy(value_table)\n",
    "    for s in range(env.observation_space.n):\n",
    "      Q_values = []\n",
    "      # Loop over all possible actions\n",
    "      for a in range(env.action_space.n):\n",
    "       action_value = 0\n",
    "       for prob, s_, r, done in env.P[s][a]:\n",
    "           action_value += prob * (r + gamma * updated_value_table[s_])\n",
    "           Q_values.append(action_value)\n",
    "       value_table[s] = max(Q_values)\n",
    "      print(f'Value Table for State {s}', value_table[s])\n",
    "    if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
    "      break\n",
    "  return value_table\n",
    "optimal_value_function = value_iteration(env)\n",
    "print('optimal value function', optimal_value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 1. 3. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 1. 3. 0. 2.]\n",
      " [2. 2. 2. 0. 1.]\n",
      " [0. 0. 3. 3. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def extract_policy(value_table):\n",
    "  gamma = 1.0\n",
    "  policy = np.zeros(env.observation_space.n)\n",
    "  for s in range(env.observation_space.n):\n",
    "    Q_values = [sum([prob*(r + gamma * value_table[s_])\n",
    "                for prob, s_, r, _ in env.P[s][a]])\n",
    "                for a in range(env.action_space.n)]\n",
    "    policy[s] = np.argmax(np.array(Q_values))\n",
    "  return policy\n",
    "optimal_policy = extract_policy(optimal_value_function)\n",
    "\n",
    "print(optimal_policy.reshape(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomFrozenLakeEnv' object has no attribute 'P'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m V, policy\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Calculate the state-value function (V*)\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m V, policy \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Print the optimal value function (V*)\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal Value Function (V*):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[31], line 88\u001b[0m, in \u001b[0;36mvalue_iteration\u001b[1;34m(env, gamma, theta)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn):\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# Initialize variables for calculating the expected reward\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     expected_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prob, next_state, reward, done \u001b[38;5;129;01min\u001b[39;00m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP\u001b[49m[s][a]:\n\u001b[0;32m     89\u001b[0m         expected_value \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prob \u001b[38;5;241m*\u001b[39m (reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m V[next_state])\n\u001b[0;32m     90\u001b[0m     q_values\u001b[38;5;241m.\u001b[39mappend(expected_value)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CustomFrozenLakeEnv' object has no attribute 'P'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the custom environment class\n",
    "class CustomFrozenLakeEnv(gym.Env):\n",
    "    def __init__(self, desc=None, is_slippery=False):\n",
    "        # Set the custom grid for the environment\n",
    "        self.desc = np.array(desc, dtype=\"c\")\n",
    "        self.nrow, self.ncol = self.desc.shape\n",
    "        self.nS = self.nrow * self.ncol  # Total number of states (5x5 grid)\n",
    "        self.nA = 4  # 4 actions (up, down, left, right)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(self.nA)\n",
    "        self.observation_space = gym.spaces.Discrete(self.nS)\n",
    "\n",
    "        # Initialize agent position\n",
    "        self.state = 0  # Start at position 'S'\n",
    "        self.done = False\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = 0  # Reset to the start position\n",
    "        self.done = False\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self.state, 0, True, {}\n",
    "\n",
    "        row, col = divmod(self.state, self.ncol)\n",
    "\n",
    "        if action == 0:  # Up\n",
    "            new_row, new_col = max(row - 1, 0), col\n",
    "        elif action == 1:  # Down\n",
    "            new_row, new_col = min(row + 1, self.nrow - 1), col\n",
    "        elif action == 2:  # Left\n",
    "            new_row, new_col = row, max(col - 1, 0)\n",
    "        elif action == 3:  # Right\n",
    "            new_row, new_col = row, min(col + 1, self.ncol - 1)\n",
    "        \n",
    "        new_state = new_row * self.ncol + new_col\n",
    "        tile = self.desc[new_row, new_col].decode(\"utf-8\")\n",
    "\n",
    "        reward = 0\n",
    "        if tile == 'H':  # If the agent falls into a hole, game over\n",
    "            self.done = True\n",
    "            reward = -10\n",
    "        elif tile == 'G':  # If the agent reaches the goal, game over with a success reward\n",
    "            self.done = True\n",
    "            reward = 10\n",
    "        elif tile == 'T':  # If the agent steps on a treasure, reward +5\n",
    "            reward = 5\n",
    "            # After stepping on a treasure, it becomes frozen (F)\n",
    "            self.desc[new_row, new_col] = b'F'\n",
    "\n",
    "        # Update the state\n",
    "        self.state = new_state\n",
    "        return self.state, reward, self.done, {}\n",
    "\n",
    "# Custom 5x5 grid with treasures (T), holes (H), goal (G), frozen tiles (F), and start (S)\n",
    "custom_map = [\n",
    "    \"SFFHT\",  # Start position (S), Frozen Tiles (F)\n",
    "    \"FHFFF\",  # Hole (H), Frozen (F)\n",
    "    \"FFFTF\",  # Frozen (F), Hole (H), Frozen (F)\n",
    "    \"TFHFF\",  # Treasure (T), Hole (H), Frozen (F)\n",
    "    \"FFFFG\",  # Frozen (F), Goal (G)\n",
    "]\n",
    "\n",
    "# Create the custom FrozenLake environment\n",
    "env = CustomFrozenLakeEnv(desc=custom_map, is_slippery=False)\n",
    "\n",
    "# Value Iteration to calculate the optimal value function and policy\n",
    "def value_iteration(env, gamma=0.9, theta=1e-6):\n",
    "    V = np.zeros(env.observation_space.n)  # Initialize the value function\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)  # Initialize the policy\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # Iterate through all states\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = V[s]\n",
    "            q_values = []\n",
    "            # Iterate through all possible actions\n",
    "            for a in range(env.action_space.n):\n",
    "                # Initialize variables for calculating the expected reward\n",
    "                expected_value = 0\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    expected_value += prob * (reward + gamma * V[next_state])\n",
    "                q_values.append(expected_value)\n",
    "\n",
    "            # Update the value function for the state\n",
    "            V[s] = max(q_values)\n",
    "            policy[s] = np.argmax(q_values)\n",
    "\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "\n",
    "        # If the value function has converged, break\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V, policy\n",
    "\n",
    "# Calculate the state-value function (V*)\n",
    "V, policy = value_iteration(env)\n",
    "\n",
    "# Print the optimal value function (V*)\n",
    "print(\"Optimal Value Function (V*):\")\n",
    "print(V.reshape(env.nrow, env.ncol))\n",
    "\n",
    "# Visualize the optimal policy (*) for each state\n",
    "policy_map = np.array([['', '', '', ''][a] for a in policy]).reshape(env.nrow, env.ncol)\n",
    "print(\"Optimal Policy (*):\")\n",
    "print(policy_map)\n",
    "\n",
    "# Plot the agents path on the map using the learned policy\n",
    "def visualize_policy(env, policy):\n",
    "    policy_map = np.array([['', '', '', ''][a] for a in policy]).reshape(env.nrow, env.ncol)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.matshow(np.zeros_like(policy_map), cmap=\"Blues\", alpha=0.3)\n",
    "    \n",
    "    for i in range(env.nrow):\n",
    "        for j in range(env.ncol):\n",
    "            ax.text(j, i, policy_map[i][j], ha='center', va='center', fontsize=16)\n",
    "    \n",
    "    plt.title(\"Learned Policy (*)\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_policy(env, policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
