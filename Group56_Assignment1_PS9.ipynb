{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 56  \n",
    "## Problem Statement Assignment - 1 Problem Set - 9\n",
    "\n",
    "\n",
    "## Contributors\n",
    "| **Name**                | **BITS ID**      | **Contributions**  |\n",
    "|-------------------------|-----------------|-------------------|\n",
    "| SUBHASIS CHAKRABORTY    | 2023 AC 05309   | 100 %             |\n",
    "| LALITHA SHREE V         | 2023 AC 05278   | 100 %             |\n",
    "| RAMYA S                 | 2023 AC 05705   | 100 %             |\n",
    "| REVATHI P               | 2023 AD 05044   | 100 %             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\subhasis\\miniconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\subhasis\\miniconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\subhasis\\miniconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\subhasis\\miniconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\subhasis\\miniconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\subhasis\\miniconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\subhasis\\miniconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\subhasis\\miniconda3\\lib\\site-packages (from openpyxl) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "# Installation of nltk\n",
    "# In Jupyter, the console commands can be executed by the ‘!’ sign before the command within the cell\n",
    "!pip install nltk\n",
    "!pip install openpyxl\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop an application to preprocess a document collection for effective Boolean query retrieval. Implement a function that takes a dictionary of documents (where document IDs serve as keys and text content as values) and constructs an inverted index from the pre-processed text. Compute and display the number of tokens at various stages of preprocessing:\n",
    "-   A.    Before preprocessing\n",
    "-   B.    After stopword removal\n",
    "-   C.    After normalization\n",
    "-   D.    After stemming/lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Nature_1.docx\n",
      "Tokens before preprocessing: 167\n",
      "Tokens after stopword removal: 110\n",
      "Tokens after normalization: 84\n",
      "Tokens after lemmatization: 84\n",
      "\n",
      "Document: Nature_10.pdf\n",
      "Tokens before preprocessing: 155\n",
      "Tokens after stopword removal: 99\n",
      "Tokens after normalization: 79\n",
      "Tokens after lemmatization: 79\n",
      "\n",
      "Document: Nature_2.pdf\n",
      "Tokens before preprocessing: 197\n",
      "Tokens after stopword removal: 114\n",
      "Tokens after normalization: 85\n",
      "Tokens after lemmatization: 85\n",
      "\n",
      "Document: Nature_3.txt\n",
      "Tokens before preprocessing: 144\n",
      "Tokens after stopword removal: 96\n",
      "Tokens after normalization: 63\n",
      "Tokens after lemmatization: 63\n",
      "\n",
      "Document: Nature_4.xlsx\n",
      "Tokens before preprocessing: 120\n",
      "Tokens after stopword removal: 75\n",
      "Tokens after normalization: 62\n",
      "Tokens after lemmatization: 62\n",
      "\n",
      "Document: Nature_5.docx\n",
      "Tokens before preprocessing: 85\n",
      "Tokens after stopword removal: 57\n",
      "Tokens after normalization: 46\n",
      "Tokens after lemmatization: 46\n",
      "\n",
      "Document: Nature_6.pdf\n",
      "Tokens before preprocessing: 171\n",
      "Tokens after stopword removal: 103\n",
      "Tokens after normalization: 81\n",
      "Tokens after lemmatization: 81\n",
      "\n",
      "Document: Nature_7.txt\n",
      "Tokens before preprocessing: 100\n",
      "Tokens after stopword removal: 62\n",
      "Tokens after normalization: 51\n",
      "Tokens after lemmatization: 51\n",
      "\n",
      "Document: Nature_8.xlsx\n",
      "Tokens before preprocessing: 148\n",
      "Tokens after stopword removal: 103\n",
      "Tokens after normalization: 77\n",
      "Tokens after lemmatization: 77\n",
      "\n",
      "Document: Nature_9.docx\n",
      "Tokens before preprocessing: 176\n",
      "Tokens after stopword removal: 121\n",
      "Tokens after normalization: 98\n",
      "Tokens after lemmatization: 98\n",
      "\n",
      "Total Token Counts:\n",
      "Tokens before preprocessing: 1463\n",
      "Tokens after stopword removal: 940\n",
      "Tokens after normalization: 726\n",
      "Tokens after lemmatization: 726\n",
      "\n",
      "Inverted Index:\n",
      "100: ['Nature_10.pdf']\n",
      "180: ['Nature_9.docx']\n",
      "20: ['Nature_8.xlsx']\n",
      "300: ['Nature_10.pdf']\n",
      "4: ['Nature_8.xlsx']\n",
      "540: ['Nature_9.docx']\n",
      "66: ['Nature_9.docx']\n",
      "750: ['Nature_9.docx']\n",
      "abil: ['Nature_10.pdf']\n",
      "academ: ['Nature_6.pdf']\n",
      "accord: ['Nature_2.pdf']\n",
      "action: ['Nature_5.docx']\n",
      "activ: ['Nature_1.docx', 'Nature_5.docx', 'Nature_8.xlsx']\n",
      "actual: ['Nature_6.pdf']\n",
      "advent: ['Nature_1.docx', 'Nature_10.pdf']\n",
      "affect: ['Nature_10.pdf']\n",
      "african: ['Nature_10.pdf']\n",
      "ago: ['Nature_10.pdf', 'Nature_8.xlsx', 'Nature_9.docx']\n",
      "agricultur: ['Nature_10.pdf']\n",
      "alga: ['Nature_10.pdf']\n",
      "allow: ['Nature_10.pdf']\n",
      "along: ['Nature_8.xlsx']\n",
      "also: ['Nature_2.pdf', 'Nature_6.pdf']\n",
      "alter: ['Nature_2.pdf', 'Nature_6.pdf']\n",
      "american: ['Nature_1.docx']\n",
      "ancient: ['Nature_3.txt']\n",
      "anim: ['Nature_2.pdf', 'Nature_3.txt', 'Nature_9.docx']\n",
      "apart: ['Nature_9.docx']\n",
      "ape: ['Nature_10.pdf']\n",
      "applic: ['Nature_3.txt']\n",
      "area: ['Nature_7.txt']\n",
      "around: ['Nature_8.xlsx']\n",
      "artifici: ['Nature_2.pdf']\n",
      "ash: ['Nature_7.txt']\n",
      "associ: ['Nature_2.pdf']\n",
      "atmospher: ['Nature_6.pdf', 'Nature_8.xlsx']\n",
      "averag: ['Nature_6.pdf']\n",
      "balanc: ['Nature_6.pdf']\n",
      "batholith: ['Nature_7.txt']\n",
      "becam: ['Nature_1.docx']\n",
      "began: ['Nature_3.txt', 'Nature_9.docx']\n",
      "believ: ['Nature_8.xlsx']\n",
      "billion: ['Nature_8.xlsx']\n",
      "biolog: ['Nature_5.docx']\n",
      "biologist: ['Nature_10.pdf']\n",
      "biospher: ['Nature_10.pdf']\n",
      "blanket: ['Nature_7.txt']\n",
      "borrow: ['Nature_3.txt']\n",
      "boundari: ['Nature_8.xlsx']\n",
      "break: ['Nature_9.docx']\n",
      "broadli: ['Nature_8.xlsx']\n",
      "broke: ['Nature_9.docx']\n",
      "brought: ['Nature_2.pdf']\n",
      "calcul: ['Nature_10.pdf']\n",
      "cambrian: ['Nature_9.docx']\n",
      "came: ['Nature_8.xlsx']\n",
      "case: ['Nature_2.pdf']\n",
      "caus: ['Nature_10.pdf']\n",
      "centuri: ['Nature_1.docx']\n",
      "certain: ['Nature_3.txt']\n",
      "chang: ['Nature_2.pdf', 'Nature_7.txt']\n",
      "charact: ['Nature_1.docx']\n",
      "characterist: ['Nature_3.txt']\n",
      "charl: ['Nature_1.docx']\n",
      "chemistri: ['Nature_8.xlsx']\n",
      "civil: ['Nature_10.pdf']\n",
      "classifi: ['Nature_10.pdf']\n",
      "climat: ['Nature_4.xlsx', 'Nature_6.pdf']\n",
      "closer: ['Nature_1.docx']\n",
      "closest: ['Nature_4.xlsx']\n",
      "collis: ['Nature_9.docx']\n",
      "combin: ['Nature_9.docx']\n",
      "comet: ['Nature_8.xlsx']\n",
      "comparison: ['Nature_10.pdf']\n",
      "compos: ['Nature_5.docx']\n",
      "composit: ['Nature_6.pdf']\n",
      "concept: ['Nature_2.pdf', 'Nature_3.txt']\n",
      "condens: ['Nature_8.xlsx']\n",
      "condit: ['Nature_6.pdf']\n",
      "conscious: ['Nature_2.pdf']\n",
      "consid: ['Nature_1.docx', 'Nature_2.pdf']\n",
      "consist: ['Nature_4.xlsx']\n",
      "constitut: ['Nature_1.docx', 'Nature_6.pdf']\n",
      "content: ['Nature_4.xlsx', 'Nature_8.xlsx']\n",
      "context: ['Nature_2.pdf']\n",
      "contin: ['Nature_4.xlsx', 'Nature_9.docx']\n",
      "convect: ['Nature_5.docx']\n",
      "converg: ['Nature_8.xlsx']\n",
      "cool: ['Nature_8.xlsx']\n",
      "core: ['Nature_3.txt', 'Nature_5.docx']\n",
      "could: ['Nature_10.pdf']\n",
      "cover: ['Nature_4.xlsx', 'Nature_9.docx']\n",
      "creat: ['Nature_6.pdf']\n",
      "crystal: ['Nature_7.txt']\n",
      "currenc: ['Nature_3.txt']\n",
      "current: ['Nature_10.pdf', 'Nature_5.docx']\n",
      "darwin: ['Nature_1.docx']\n",
      "debat: ['Nature_10.pdf']\n",
      "decorum: ['Nature_1.docx']\n",
      "deform: ['Nature_7.txt', 'Nature_8.xlsx']\n",
      "degre: ['Nature_6.pdf']\n",
      "deliv: ['Nature_8.xlsx']\n",
      "deposit: ['Nature_7.txt', 'Nature_8.xlsx']\n",
      "depriv: ['Nature_1.docx']\n",
      "deriv: ['Nature_3.txt']\n",
      "describ: ['Nature_1.docx']\n",
      "despit: ['Nature_2.pdf']\n",
      "destruct: ['Nature_10.pdf']\n",
      "develop: ['Nature_10.pdf', 'Nature_3.txt']\n",
      "dike: ['Nature_7.txt']\n",
      "dimens: ['Nature_3.txt']\n",
      "dinosaur: ['Nature_9.docx']\n",
      "disciplin: ['Nature_6.pdf']\n",
      "disposit: ['Nature_3.txt']\n",
      "distinct: ['Nature_2.pdf']\n",
      "distinctli: ['Nature_9.docx']\n",
      "distinguish: ['Nature_2.pdf']\n",
      "diverg: ['Nature_8.xlsx']\n",
      "diversifi: ['Nature_9.docx']\n",
      "divid: ['Nature_5.docx']\n",
      "divin: ['Nature_1.docx']\n",
      "dynam: ['Nature_3.txt', 'Nature_6.pdf']\n",
      "dynamo: ['Nature_5.docx']\n",
      "earliest: ['Nature_9.docx']\n",
      "earth: ['Nature_10.pdf', 'Nature_4.xlsx', 'Nature_5.docx', 'Nature_6.pdf', 'Nature_7.txt', 'Nature_8.xlsx', 'Nature_9.docx']\n",
      "ecolog: ['Nature_6.pdf']\n",
      "ecospher: ['Nature_1.docx']\n",
      "effect: ['Nature_6.pdf']\n",
      "either: ['Nature_7.txt']\n",
      "electr: ['Nature_5.docx']\n",
      "element: ['Nature_1.docx']\n",
      "emplac: ['Nature_7.txt']\n",
      "encompass: ['Nature_6.pdf']\n",
      "energet: ['Nature_8.xlsx']\n",
      "engin: ['Nature_6.pdf']\n",
      "environ: ['Nature_2.pdf', 'Nature_6.pdf']\n",
      "equatori: ['Nature_4.xlsx']\n",
      "era: ['Nature_10.pdf', 'Nature_9.docx']\n",
      "especi: ['Nature_1.docx', 'Nature_3.txt']\n",
      "estim: ['Nature_8.xlsx']\n",
      "even: ['Nature_1.docx']\n",
      "event: ['Nature_10.pdf']\n",
      "ever: ['Nature_10.pdf', 'Nature_3.txt']\n",
      "evolv: ['Nature_5.docx', 'Nature_7.txt']\n",
      "exampl: ['Nature_2.pdf']\n",
      "exist: ['Nature_2.pdf']\n",
      "expans: ['Nature_3.txt']\n",
      "explos: ['Nature_9.docx']\n",
      "extens: ['Nature_8.xlsx']\n",
      "extent: ['Nature_10.pdf']\n",
      "extinct: ['Nature_10.pdf', 'Nature_9.docx']\n",
      "extract: ['Nature_6.pdf']\n",
      "factor: ['Nature_6.pdf']\n",
      "fastest: ['Nature_10.pdf']\n",
      "featur: ['Nature_3.txt', 'Nature_4.xlsx']\n",
      "field: ['Nature_4.xlsx', 'Nature_5.docx', 'Nature_6.pdf']\n",
      "fifth: ['Nature_4.xlsx']\n",
      "final: ['Nature_9.docx']\n",
      "first: ['Nature_7.txt']\n",
      "five: ['Nature_9.docx']\n",
      "flow: ['Nature_7.txt']\n",
      "fluid: ['Nature_5.docx']\n",
      "forest: ['Nature_2.pdf']\n",
      "form: ['Nature_10.pdf', 'Nature_6.pdf', 'Nature_8.xlsx', 'Nature_9.docx']\n",
      "found: ['Nature_2.pdf']\n",
      "freez: ['Nature_9.docx']\n",
      "french: ['Nature_3.txt']\n",
      "gain: ['Nature_10.pdf', 'Nature_3.txt']\n",
      "gener: ['Nature_1.docx', 'Nature_2.pdf', 'Nature_5.docx']\n",
      "geograph: ['Nature_6.pdf']\n",
      "geographi: ['Nature_6.pdf']\n",
      "geolog: ['Nature_2.pdf', 'Nature_5.docx', 'Nature_6.pdf', 'Nature_7.txt']\n",
      "geomagnet: ['Nature_5.docx']\n",
      "geotechn: ['Nature_6.pdf']\n",
      "glacier: ['Nature_9.docx']\n",
      "global: ['Nature_10.pdf', 'Nature_6.pdf']\n",
      "got: ['Nature_1.docx']\n",
      "gradual: ['Nature_5.docx']\n",
      "great: ['Nature_10.pdf']\n",
      "greek: ['Nature_3.txt']\n",
      "harvard: ['Nature_10.pdf']\n",
      "hazard: ['Nature_6.pdf']\n",
      "hegel: ['Nature_1.docx']\n",
      "hemispher: ['Nature_4.xlsx']\n",
      "henc: ['Nature_1.docx']\n",
      "heraclitu: ['Nature_3.txt']\n",
      "highli: ['Nature_8.xlsx']\n",
      "histor: ['Nature_6.pdf']\n",
      "histori: ['Nature_1.docx', 'Nature_6.pdf']\n",
      "holocen: ['Nature_10.pdf']\n",
      "horizont: ['Nature_8.xlsx']\n",
      "human: ['Nature_1.docx', 'Nature_10.pdf', 'Nature_2.pdf']\n",
      "hundr: ['Nature_9.docx']\n",
      "hydrocarbon: ['Nature_6.pdf']\n",
      "hypothesi: ['Nature_9.docx']\n",
      "ice: ['Nature_8.xlsx', 'Nature_9.docx']\n",
      "identifi: ['Nature_9.docx']\n",
      "igneou: ['Nature_7.txt']\n",
      "impli: ['Nature_2.pdf']\n",
      "import: ['Nature_6.pdf']\n",
      "inanim: ['Nature_2.pdf']\n",
      "includ: ['Nature_1.docx']\n",
      "increasingli: ['Nature_1.docx']\n",
      "industri: ['Nature_1.docx']\n",
      "inhabit: ['Nature_4.xlsx']\n",
      "inher: ['Nature_1.docx']\n",
      "initi: ['Nature_8.xlsx']\n",
      "innat: ['Nature_3.txt']\n",
      "inner: ['Nature_5.docx']\n",
      "insert: ['Nature_7.txt']\n",
      "intent: ['Nature_1.docx']\n",
      "interact: ['Nature_2.pdf']\n",
      "interest: ['Nature_9.docx']\n",
      "interglaci: ['Nature_6.pdf']\n",
      "interior: ['Nature_5.docx']\n",
      "intervent: ['Nature_1.docx', 'Nature_2.pdf']\n",
      "intrins: ['Nature_3.txt']\n",
      "intrud: ['Nature_7.txt']\n",
      "intrus: ['Nature_7.txt']\n",
      "iron: ['Nature_5.docx']\n",
      "island: ['Nature_4.xlsx']\n",
      "knowledg: ['Nature_6.pdf']\n",
      "known: ['Nature_4.xlsx', 'Nature_9.docx']\n",
      "laccolith: ['Nature_7.txt']\n",
      "land: ['Nature_4.xlsx']\n",
      "larg: ['Nature_4.xlsx', 'Nature_9.docx']\n",
      "largest: ['Nature_4.xlsx']\n",
      "last: ['Nature_1.docx', 'Nature_9.docx']\n",
      "later: ['Nature_7.txt', 'Nature_8.xlsx', 'Nature_9.docx']\n",
      "latin: ['Nature_3.txt']\n",
      "latitud: ['Nature_6.pdf']\n",
      "lava: ['Nature_7.txt']\n",
      "law: ['Nature_1.docx']\n",
      "layer: ['Nature_5.docx', 'Nature_8.xlsx']\n",
      "left: ['Nature_5.docx']\n",
      "less: ['Nature_4.xlsx']\n",
      "life: ['Nature_10.pdf', 'Nature_4.xlsx', 'Nature_9.docx']\n",
      "liquid: ['Nature_6.pdf']\n",
      "liter: ['Nature_3.txt']\n",
      "lithifi: ['Nature_7.txt']\n",
      "live: ['Nature_2.pdf']\n",
      "locat: ['Nature_4.xlsx']\n",
      "magnet: ['Nature_5.docx']\n",
      "major: ['Nature_6.pdf']\n",
      "make: ['Nature_9.docx']\n",
      "mammal: ['Nature_9.docx']\n",
      "mammalian: ['Nature_9.docx']\n",
      "mani: ['Nature_4.xlsx']\n",
      "mantl: ['Nature_5.docx']\n",
      "manufactur: ['Nature_2.pdf']\n",
      "marx: ['Nature_1.docx']\n",
      "mass: ['Nature_10.pdf', 'Nature_9.docx']\n",
      "materi: ['Nature_6.pdf', 'Nature_7.txt']\n",
      "matter: ['Nature_6.pdf']\n",
      "mean: ['Nature_2.pdf']\n",
      "meant: ['Nature_3.txt']\n",
      "mere: ['Nature_1.docx']\n",
      "meteorit: ['Nature_9.docx']\n",
      "method: ['Nature_1.docx']\n",
      "metr: ['Nature_4.xlsx']\n",
      "might: ['Nature_2.pdf']\n",
      "migrat: ['Nature_5.docx']\n",
      "million: ['Nature_10.pdf', 'Nature_8.xlsx', 'Nature_9.docx']\n",
      "miner: ['Nature_6.pdf']\n",
      "mitig: ['Nature_6.pdf']\n",
      "modern: ['Nature_1.docx']\n",
      "molecul: ['Nature_8.xlsx']\n",
      "molten: ['Nature_8.xlsx']\n",
      "moon: ['Nature_8.xlsx']\n",
      "mostli: ['Nature_3.txt']\n",
      "motion: ['Nature_5.docx']\n",
      "move: ['Nature_1.docx', 'Nature_6.pdf']\n",
      "much: ['Nature_9.docx']\n",
      "multicellular: ['Nature_9.docx']\n",
      "narrow: ['Nature_4.xlsx']\n",
      "natur: ['Nature_1.docx', 'Nature_10.pdf', 'Nature_2.pdf', 'Nature_3.txt', 'Nature_4.xlsx', 'Nature_6.pdf']\n",
      "natura: ['Nature_3.txt']\n",
      "nebula: ['Nature_8.xlsx']\n",
      "neoproterozo: ['Nature_9.docx']\n",
      "next: ['Nature_10.pdf']\n",
      "northern: ['Nature_4.xlsx']\n",
      "notion: ['Nature_3.txt']\n",
      "object: ['Nature_2.pdf']\n",
      "occasion: ['Nature_9.docx']\n",
      "occur: ['Nature_7.txt', 'Nature_8.xlsx', 'Nature_9.docx']\n",
      "ocean: ['Nature_4.xlsx', 'Nature_8.xlsx']\n",
      "odd: ['Nature_1.docx']\n",
      "often: ['Nature_1.docx', 'Nature_2.pdf']\n",
      "old: ['Nature_3.txt']\n",
      "one: ['Nature_1.docx', 'Nature_3.txt']\n",
      "onto: ['Nature_7.txt']\n",
      "organ: ['Nature_1.docx', 'Nature_10.pdf']\n",
      "origin: ['Nature_3.txt', 'Nature_5.docx', 'Nature_6.pdf']\n",
      "outer: ['Nature_5.docx', 'Nature_8.xlsx']\n",
      "outright: ['Nature_1.docx']\n",
      "overli: ['Nature_7.txt']\n",
      "oxygen: ['Nature_10.pdf']\n",
      "pangaea: ['Nature_9.docx']\n",
      "pannotia: ['Nature_9.docx']\n",
      "part: ['Nature_1.docx', 'Nature_10.pdf', 'Nature_2.pdf']\n",
      "particular: ['Nature_2.pdf', 'Nature_9.docx']\n",
      "particularli: ['Nature_1.docx']\n",
      "passiv: ['Nature_1.docx']\n",
      "past: ['Nature_6.pdf', 'Nature_9.docx']\n",
      "per: ['Nature_4.xlsx']\n",
      "percent: ['Nature_4.xlsx']\n",
      "period: ['Nature_10.pdf', 'Nature_6.pdf']\n",
      "persist: ['Nature_2.pdf']\n",
      "phase: ['Nature_5.docx']\n",
      "phenomena: ['Nature_1.docx']\n",
      "philosoph: ['Nature_3.txt']\n",
      "philosophi: ['Nature_3.txt']\n",
      "physi: ['Nature_3.txt']\n",
      "physic: ['Nature_1.docx', 'Nature_3.txt', 'Nature_6.pdf']\n",
      "planet: ['Nature_4.xlsx']\n",
      "plant: ['Nature_2.pdf', 'Nature_3.txt']\n",
      "plastic: ['Nature_5.docx']\n",
      "polar: ['Nature_4.xlsx']\n",
      "preced: ['Nature_9.docx']\n",
      "predict: ['Nature_10.pdf']\n",
      "presenc: ['Nature_6.pdf']\n",
      "present: ['Nature_10.pdf']\n",
      "previou: ['Nature_10.pdf']\n",
      "primordi: ['Nature_8.xlsx']\n",
      "probabl: ['Nature_9.docx']\n",
      "process: ['Nature_2.pdf', 'Nature_5.docx', 'Nature_6.pdf', 'Nature_7.txt']\n",
      "produc: ['Nature_10.pdf', 'Nature_8.xlsx']\n",
      "prolifer: ['Nature_10.pdf', 'Nature_9.docx']\n",
      "promin: ['Nature_4.xlsx']\n",
      "properti: ['Nature_6.pdf']\n",
      "provid: ['Nature_1.docx']\n",
      "push: ['Nature_7.txt']\n",
      "qualifi: ['Nature_2.pdf']\n",
      "qualiti: ['Nature_3.txt']\n",
      "quantiti: ['Nature_10.pdf']\n",
      "quit: ['Nature_6.pdf']\n",
      "rapidli: ['Nature_10.pdf']\n",
      "realiti: ['Nature_1.docx']\n",
      "realm: ['Nature_2.pdf']\n",
      "reborn: ['Nature_1.docx']\n",
      "recombin: ['Nature_9.docx']\n",
      "refer: ['Nature_1.docx', 'Nature_2.pdf']\n",
      "reform: ['Nature_9.docx']\n",
      "regim: ['Nature_8.xlsx']\n",
      "region: ['Nature_4.xlsx', 'Nature_6.pdf']\n",
      "rel: ['Nature_4.xlsx']\n",
      "relat: ['Nature_3.txt', 'Nature_8.xlsx']\n",
      "remain: ['Nature_5.docx']\n",
      "remaind: ['Nature_4.xlsx']\n",
      "reptil: ['Nature_9.docx']\n",
      "requir: ['Nature_10.pdf']\n",
      "research: ['Nature_10.pdf', 'Nature_4.xlsx']\n",
      "reshap: ['Nature_9.docx']\n",
      "respect: ['Nature_8.xlsx']\n",
      "result: ['Nature_8.xlsx']\n",
      "revolut: ['Nature_1.docx']\n",
      "rock: ['Nature_2.pdf', 'Nature_7.txt', 'Nature_8.xlsx']\n",
      "rodinia: ['Nature_9.docx']\n",
      "roughli: ['Nature_8.xlsx']\n",
      "rousseau: ['Nature_1.docx']\n",
      "sacr: ['Nature_1.docx']\n",
      "scienc: ['Nature_6.pdf']\n",
      "scientif: ['Nature_1.docx', 'Nature_4.xlsx']\n",
      "sediment: ['Nature_7.txt']\n",
      "sedimentari: ['Nature_7.txt']\n",
      "seen: ['Nature_1.docx']\n",
      "sens: ['Nature_1.docx']\n",
      "separ: ['Nature_1.docx']\n",
      "sequenc: ['Nature_8.xlsx']\n",
      "settl: ['Nature_7.txt']\n",
      "sever: ['Nature_1.docx', 'Nature_10.pdf', 'Nature_3.txt', 'Nature_4.xlsx', 'Nature_5.docx']\n",
      "shape: ['Nature_7.txt']\n",
      "shorten: ['Nature_8.xlsx']\n",
      "siderian: ['Nature_10.pdf']\n",
      "significantli: ['Nature_6.pdf']\n",
      "sill: ['Nature_7.txt']\n",
      "sinc: ['Nature_3.txt']\n",
      "small: ['Nature_10.pdf', 'Nature_9.docx']\n",
      "solar: ['Nature_4.xlsx', 'Nature_8.xlsx']\n",
      "solid: ['Nature_5.docx', 'Nature_6.pdf', 'Nature_8.xlsx']\n",
      "spare: ['Nature_9.docx']\n",
      "speci: ['Nature_10.pdf']\n",
      "stabil: ['Nature_6.pdf']\n",
      "stabl: ['Nature_6.pdf']\n",
      "stand: ['Nature_10.pdf']\n",
      "steadili: ['Nature_3.txt']\n",
      "still: ['Nature_10.pdf', 'Nature_2.pdf']\n",
      "structur: ['Nature_6.pdf', 'Nature_8.xlsx']\n",
      "studi: ['Nature_6.pdf']\n",
      "subject: ['Nature_4.xlsx']\n",
      "subsequ: ['Nature_10.pdf']\n",
      "substanti: ['Nature_2.pdf']\n",
      "subtrop: ['Nature_4.xlsx']\n",
      "sun: ['Nature_4.xlsx', 'Nature_8.xlsx']\n",
      "supercontin: ['Nature_9.docx']\n",
      "superior: ['Nature_1.docx']\n",
      "supernatur: ['Nature_2.pdf']\n",
      "support: ['Nature_4.xlsx']\n",
      "surfac: ['Nature_4.xlsx', 'Nature_5.docx', 'Nature_6.pdf', 'Nature_7.txt', 'Nature_9.docx']\n",
      "system: ['Nature_4.xlsx']\n",
      "taken: ['Nature_2.pdf']\n",
      "tecton: ['Nature_5.docx', 'Nature_8.xlsx']\n",
      "temper: ['Nature_4.xlsx']\n",
      "temperatur: ['Nature_6.pdf', 'Nature_9.docx']\n",
      "term: ['Nature_2.pdf', 'Nature_9.docx']\n",
      "terrestri: ['Nature_4.xlsx']\n",
      "thick: ['Nature_5.docx']\n",
      "thing: ['Nature_2.pdf']\n",
      "third: ['Nature_4.xlsx']\n",
      "though: ['Nature_3.txt']\n",
      "time: ['Nature_1.docx', 'Nature_3.txt', 'Nature_7.txt']\n",
      "today: ['Nature_2.pdf']\n",
      "trace: ['Nature_5.docx']\n",
      "tradit: ['Nature_1.docx', 'Nature_2.pdf']\n",
      "transcendent: ['Nature_1.docx']\n",
      "transform: ['Nature_8.xlsx']\n",
      "translat: ['Nature_3.txt']\n",
      "trigger: ['Nature_9.docx']\n",
      "tropic: ['Nature_4.xlsx']\n",
      "turn: ['Nature_5.docx']\n",
      "two: ['Nature_4.xlsx', 'Nature_6.pdf']\n",
      "type: ['Nature_2.pdf']\n",
      "typic: ['Nature_8.xlsx']\n",
      "understand: ['Nature_6.pdf']\n",
      "understood: ['Nature_2.pdf']\n",
      "unit: ['Nature_7.txt', 'Nature_8.xlsx']\n",
      "univers: ['Nature_1.docx', 'Nature_10.pdf', 'Nature_3.txt']\n",
      "unless: ['Nature_2.pdf']\n",
      "unnatur: ['Nature_2.pdf']\n",
      "upward: ['Nature_7.txt']\n",
      "use: ['Nature_2.pdf', 'Nature_3.txt']\n",
      "vapor: ['Nature_8.xlsx']\n",
      "vari: ['Nature_4.xlsx']\n",
      "variat: ['Nature_6.pdf']\n",
      "variou: ['Nature_2.pdf']\n",
      "vision: ['Nature_1.docx']\n",
      "vitalist: ['Nature_1.docx']\n",
      "volcan: ['Nature_7.txt', 'Nature_8.xlsx']\n",
      "water: ['Nature_4.xlsx', 'Nature_8.xlsx']\n",
      "way: ['Nature_2.pdf']\n",
      "weather: ['Nature_2.pdf']\n",
      "well: ['Nature_10.pdf']\n",
      "whole: ['Nature_1.docx', 'Nature_2.pdf', 'Nature_3.txt']\n",
      "wide: ['Nature_4.xlsx', 'Nature_6.pdf']\n",
      "wild: ['Nature_2.pdf']\n",
      "wilder: ['Nature_2.pdf']\n",
      "wilson: ['Nature_10.pdf']\n",
      "within: ['Nature_2.pdf', 'Nature_4.xlsx']\n",
      "word: ['Nature_2.pdf', 'Nature_3.txt']\n",
      "world: ['Nature_1.docx', 'Nature_3.txt']\n",
      "year: ['Nature_10.pdf', 'Nature_4.xlsx', 'Nature_8.xlsx', 'Nature_9.docx']\n",
      "zone: ['Nature_4.xlsx']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Subhasis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Subhasis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Subhasis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load and process all files from a dataset directory\n",
    "def load_documents_from_folder(folder_path):\n",
    "    \"\"\"Load text, docx, pdf, and xlsx files from a folder into a dictionary.\"\"\"\n",
    "    documents = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                documents[filename] = file.read()\n",
    "        elif filename.endswith('.docx'):\n",
    "            doc = Document(file_path)\n",
    "            documents[filename] = '\\n'.join([para.text for para in doc.paragraphs])\n",
    "        elif filename.endswith('.pdf'):\n",
    "            with open(file_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                documents[filename] = '\\n'.join(page.extract_text() for page in reader.pages)\n",
    "        elif filename.endswith('.xlsx'):\n",
    "            xls = pd.ExcelFile(file_path)\n",
    "            documents[filename] = '\\n'.join(xls.parse(sheet_name).to_string(index=False) for sheet_name in xls.sheet_names)\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text, use_stemming=True):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    token_count_before = len(tokens)\n",
    "\n",
    "    # Stopword Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    token_count_stopwords_removed = len(tokens)\n",
    "\n",
    "    # Normalization (lowercasing, removing punctuation)\n",
    "    # not removing numbers\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "    token_count_normalized = len(tokens)\n",
    "\n",
    "    # Stemming/Lemmatization\n",
    "    if use_stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens =  [stemmer.stem(token) for token in tokens]\n",
    "    else:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    token_count_lemmatized = len(tokens)\n",
    "\n",
    "    return tokens, token_count_before, token_count_stopwords_removed, token_count_normalized, token_count_lemmatized\n",
    "\n",
    "# Build inverted index\n",
    "def build_inverted_index(documents):\n",
    "    inverted_index = defaultdict(set)\n",
    "    token_counts_per_doc = {}\n",
    "\n",
    "    for doc_id, text in documents.items():\n",
    "        tokens, count_before, count_stopwords_removed, count_normalized, count_lemmatized = preprocess_text(text)\n",
    "        for token in tokens:\n",
    "            inverted_index[token].add(doc_id)\n",
    "\n",
    "        # Store token counts for the document\n",
    "        token_counts_per_doc[doc_id] = {\n",
    "            \"before\": count_before,\n",
    "            \"stopwords_removed\": count_stopwords_removed,\n",
    "            \"normalized\": count_normalized,\n",
    "            \"lemmatized\": count_lemmatized,\n",
    "        }\n",
    "\n",
    "    return inverted_index, token_counts_per_doc\n",
    "\n",
    "# executor Program\n",
    "def preprocess_executor():\n",
    "    dataset_path = \"Dataset\"  \n",
    "    documents = load_documents_from_folder(dataset_path)\n",
    "    inverted_index, token_counts_per_doc = build_inverted_index(documents)\n",
    "\n",
    "    total_counts = {\"before\": 0, \"stopwords_removed\": 0, \"normalized\": 0, \"lemmatized\": 0}\n",
    "    # Display token counts for each document\n",
    "    for doc_id, counts in token_counts_per_doc.items():\n",
    "        print(f\"Document: {doc_id}\")\n",
    "        print(f\"Tokens before preprocessing: {counts['before']}\")\n",
    "        print(f\"Tokens after stopword removal: {counts['stopwords_removed']}\")\n",
    "        print(f\"Tokens after normalization: {counts['normalized']}\")\n",
    "        print(f\"Tokens after lemmatization: {counts['lemmatized']}\\n\")\n",
    "\n",
    "        # Accumulate totals\n",
    "        total_counts[\"before\"] += counts[\"before\"]\n",
    "        total_counts[\"stopwords_removed\"] += counts[\"stopwords_removed\"]\n",
    "        total_counts[\"normalized\"] += counts[\"normalized\"]\n",
    "        total_counts[\"lemmatized\"] += counts[\"lemmatized\"]\n",
    "\n",
    "    # Display total token counts\n",
    "    print(\"Total Token Counts:\")\n",
    "    print(f\"Tokens before preprocessing: {total_counts['before']}\")\n",
    "    print(f\"Tokens after stopword removal: {total_counts['stopwords_removed']}\")\n",
    "    print(f\"Tokens after normalization: {total_counts['normalized']}\")\n",
    "    print(f\"Tokens after lemmatization: {total_counts['lemmatized']}\\n\")\n",
    "\n",
    "    # Display inverted index\n",
    "    print(\"Inverted Index:\")\n",
    "    for term, doc_ids in sorted(inverted_index.items()):\n",
    "        print(f\"{term}: {sorted(doc_ids)}\")\n",
    "\n",
    "\n",
    "preprocess_executor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.     Implement dictionary compression techniques to estimate space savings and compare their efficiency. Techniques to be implemented include:\n",
    "·       Without blocking\n",
    "·       With blocking\n",
    "·       Blocking combined with front coding. (for both dictionary and posting lists)\n",
    "Analyze the space utilization of the compressed dictionary for each method and present the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space Utilization Analysis:\n",
      "Original Dictionary Size: 120960 bytes\n",
      "Compressed Without Blocking: 33936 bytes\n",
      "Compressed With Blocking: 4250 bytes\n",
      "Compressed With Front Coding: 3192 bytes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Compression without blocking\n",
    "def compress_without_blocking(dictionary):\n",
    "    compressed_dict = {}  # Store term and offset\n",
    "    current_offset = 0\n",
    "    for term in sorted(dictionary):\n",
    "        compressed_dict[term] = current_offset\n",
    "        current_offset += len(term) + 1  # +1 for null terminator\n",
    "    return compressed_dict\n",
    "\n",
    "# Compression with blocking\n",
    "def compress_with_blocking(dictionary, block_size):\n",
    "    compressed_dict = {}\n",
    "    current_offset = 0\n",
    "    block = []\n",
    "    block_start = None\n",
    "\n",
    "    for idx, term in enumerate(sorted(dictionary)):\n",
    "        if idx % block_size == 0:\n",
    "            block_start = term\n",
    "            block = []\n",
    "            compressed_dict[block_start] = current_offset\n",
    "        block.append(term)\n",
    "        current_offset += len(term) + 1\n",
    "\n",
    "    return compressed_dict\n",
    "\n",
    "# Compression with blocking and front coding\n",
    "def compress_with_front_coding_legacy(dictionary, block_size):\n",
    "    compressed_dict = {}\n",
    "    current_offset = 0\n",
    "    block = []\n",
    "    block_start = None\n",
    "\n",
    "    for idx, term in enumerate(sorted(dictionary)):\n",
    "        if idx % block_size == 0:\n",
    "            block_start = term\n",
    "            block = []\n",
    "            compressed_dict[block_start] = current_offset\n",
    "            prev_term = term\n",
    "            current_offset += len(term) + 1\n",
    "        else:\n",
    "            prefix_length = 0\n",
    "            while (prefix_length < len(prev_term) and prefix_length < len(term) and prev_term[prefix_length] == term[prefix_length]):\n",
    "                prefix_length += 1\n",
    "            compressed_dict[f\"{prefix_length}|{term[prefix_length:]}\"] = current_offset\n",
    "            current_offset += len(term) - prefix_length + 1\n",
    "            prev_term = term\n",
    "\n",
    "    return compressed_dict\n",
    "\n",
    "def compress_with_front_coding(dictionary, block_size):\n",
    "    \"\"\"Compress dictionary and posting lists using blocking combined with front coding.\"\"\"\n",
    "    compressed_dict = []\n",
    "    keys = sorted(dictionary.keys())\n",
    "\n",
    "    for i in range(0, len(keys), block_size):\n",
    "        block = keys[i:i + block_size]\n",
    "        base = block[0]\n",
    "        # Compress terms in the block using front coding\n",
    "        encoded_block = [base] + [key[len(base):] for key in block[1:]]\n",
    "        # Compress posting lists (convert sets to lists and apply front coding)\n",
    "        postings = [sorted(list(dictionary[key])) for key in block]\n",
    "        compressed_postings = compress_posting_lists_with_front_coding(postings)\n",
    "        compressed_dict.append((encoded_block, compressed_postings))\n",
    "\n",
    "    return compressed_dict\n",
    "\n",
    "def compress_posting_lists_with_front_coding(postings):\n",
    "    \"\"\"Compress posting lists using front coding for strings.\"\"\"\n",
    "    compressed_postings = []\n",
    "    for block in postings:\n",
    "        if not block:\n",
    "            compressed_postings.append([])\n",
    "            continue\n",
    "        base = block[0]\n",
    "        # For strings, store suffixes relative to the base\n",
    "        encoded_block = [base] + [doc[len(base):] for doc in block[1:] if doc.startswith(base)]\n",
    "        compressed_postings.append(encoded_block)\n",
    "    return compressed_postings\n",
    "\n",
    "\n",
    "# Calculate dictionary size\n",
    "def calculate_size(dictionary):\n",
    "    return sum(sys.getsizeof(key) + sys.getsizeof(value) for key, value in dictionary.items())\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"Dataset\" \n",
    "    documents = load_documents_from_folder(dataset_path)\n",
    "    inverted_index, token_counts_per_doc = build_inverted_index(documents)\n",
    "\n",
    "    # Compress the dictionary\n",
    "    dictionary = {term: postings for term, postings in inverted_index.items()}\n",
    "\n",
    "    compressed_without_blocking = compress_without_blocking(dictionary)\n",
    "    compressed_with_blocking = compress_with_blocking(dictionary, block_size=8)\n",
    "    compressed_with_front_coding = compress_with_front_coding(dictionary, block_size=8)\n",
    "\n",
    "    # Analyze space savings\n",
    "    original_size = calculate_size(dictionary)\n",
    "    size_without_blocking = calculate_size(compressed_without_blocking)\n",
    "    size_with_blocking = calculate_size(compressed_with_blocking)\n",
    "    size_with_front_coding = sum(sys.getsizeof(block) for block in compressed_with_front_coding)\n",
    "\n",
    "\n",
    "    print(\"Space Utilization Analysis:\")\n",
    "    print(f\"Original Dictionary Size: {original_size} bytes\")\n",
    "    print(f\"Compressed Without Blocking: {size_without_blocking} bytes\")\n",
    "    print(f\"Compressed With Blocking: {size_with_blocking} bytes\")\n",
    "    print(f\"Compressed With Front Coding: {size_with_front_coding} bytes\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
