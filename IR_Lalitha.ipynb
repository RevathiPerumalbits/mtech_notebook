{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Contributors\n",
    "\n",
    "1. xx\n",
    "2. xx\n",
    "3. xx\n",
    "4. xx\n",
    "5. xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lashreev\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\lashreev\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\lashreev\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lashreev\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lashreev\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lashreev\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\lashreev\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\lashreev\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\lashreev\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openpyxl) (2.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\lashreev\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installation of nltk\n",
    "# In Jupyter, the console commands can be executed by the ‘!’ sign before the command within the cell\n",
    "!pip install nltk\n",
    "!pip install openpyxl\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop an application to preprocess a document collection for effective Boolean query retrieval. Implement a function that takes a dictionary of documents (where document IDs serve as keys and text content as values) and constructs an inverted index from the pre-processed text. Compute and display the number of tokens at various stages of preprocessing:\n",
    "A.    Before preprocessing\n",
    "B.    After stopword removal\n",
    "C.    After normalization\n",
    "D.    After stemming/lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Nature_1.docx\n",
      "Tokens before preprocessing: 167\n",
      "Tokens after stopword removal: 110\n",
      "Tokens after normalization: 84\n",
      "Tokens after lemmatization: 84\n",
      "\n",
      "Document: Nature_10.pdf\n",
      "Tokens before preprocessing: 155\n",
      "Tokens after stopword removal: 99\n",
      "Tokens after normalization: 79\n",
      "Tokens after lemmatization: 79\n",
      "\n",
      "Document: Nature_2.pdf\n",
      "Tokens before preprocessing: 197\n",
      "Tokens after stopword removal: 114\n",
      "Tokens after normalization: 85\n",
      "Tokens after lemmatization: 85\n",
      "\n",
      "Document: Nature_3.txt\n",
      "Tokens before preprocessing: 144\n",
      "Tokens after stopword removal: 96\n",
      "Tokens after normalization: 63\n",
      "Tokens after lemmatization: 63\n",
      "\n",
      "Document: Nature_4.xlsx\n",
      "Tokens before preprocessing: 119\n",
      "Tokens after stopword removal: 74\n",
      "Tokens after normalization: 61\n",
      "Tokens after lemmatization: 61\n",
      "\n",
      "Document: Nature_5.docx\n",
      "Tokens before preprocessing: 85\n",
      "Tokens after stopword removal: 57\n",
      "Tokens after normalization: 46\n",
      "Tokens after lemmatization: 46\n",
      "\n",
      "Document: Nature_6.pdf\n",
      "Tokens before preprocessing: 171\n",
      "Tokens after stopword removal: 103\n",
      "Tokens after normalization: 81\n",
      "Tokens after lemmatization: 81\n",
      "\n",
      "Document: Nature_7.txt\n",
      "Tokens before preprocessing: 100\n",
      "Tokens after stopword removal: 62\n",
      "Tokens after normalization: 51\n",
      "Tokens after lemmatization: 51\n",
      "\n",
      "Document: Nature_8.xlsx\n",
      "Tokens before preprocessing: 147\n",
      "Tokens after stopword removal: 102\n",
      "Tokens after normalization: 76\n",
      "Tokens after lemmatization: 76\n",
      "\n",
      "Document: Nature_9.docx\n",
      "Tokens before preprocessing: 176\n",
      "Tokens after stopword removal: 121\n",
      "Tokens after normalization: 98\n",
      "Tokens after lemmatization: 98\n",
      "\n",
      "Sample of Inverted Index:\n",
      "nature: {'Nature_1.docx', 'Nature_3.txt', 'Nature_2.pdf', 'Nature_10.pdf'}\n",
      "inherent: {'Nature_1.docx'}\n",
      "character: {'Nature_1.docx'}\n",
      "constitution: {'Nature_1.docx'}\n",
      "particularly: {'Nature_1.docx'}\n",
      "ecosphere: {'Nature_1.docx'}\n",
      "universe: {'Nature_1.docx', 'Nature_3.txt'}\n",
      "whole: {'Nature_1.docx', 'Nature_3.txt', 'Nature_2.pdf'}\n",
      "general: {'Nature_1.docx', 'Nature_2.pdf'}\n",
      "sense: {'Nature_1.docx'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lashreev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lashreev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lashreev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define text extraction functions for various formats\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return \" \".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() if page.extract_text() else \"\"\n",
    "    return text\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_xlsx(file_path):\n",
    "    excel_data = pd.ExcelFile(file_path)\n",
    "    all_sheets_text = []\n",
    "    for sheet_name in excel_data.sheet_names:\n",
    "        df = excel_data.parse(sheet_name)\n",
    "        all_sheets_text.append(\" \".join(df.astype(str).stack()))\n",
    "    return \" \".join(all_sheets_text)\n",
    "\n",
    "# Load and process all files from a dataset directory\n",
    "def load_documents(dataset_path):\n",
    "    document_collection = {}\n",
    "    for root, _, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if file.endswith('.docx'):\n",
    "                document_collection[file] = read_docx(file_path)\n",
    "            elif file.endswith('.pdf'):\n",
    "                document_collection[file] = read_pdf(file_path)\n",
    "            elif file.endswith('.txt'):\n",
    "                document_collection[file] = read_txt(file_path)\n",
    "            elif file.endswith('.xlsx'):\n",
    "                document_collection[file] = read_xlsx(file_path)\n",
    "    return document_collection\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    token_count_before = len(tokens)\n",
    "\n",
    "    # Stopword Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    token_count_stopwords_removed = len(tokens)\n",
    "\n",
    "    # Normalization (lowercasing, removing punctuation)\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "    token_count_normalized = len(tokens)\n",
    "\n",
    "    # Stemming/Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    token_count_lemmatized = len(tokens)\n",
    "\n",
    "    return tokens, token_count_before, token_count_stopwords_removed, token_count_normalized, token_count_lemmatized\n",
    "\n",
    "# Build inverted index\n",
    "def build_inverted_index(documents):\n",
    "    inverted_index = defaultdict(set)\n",
    "    for doc_id, text in documents.items():\n",
    "        tokens, count_before, count_stopwords_removed, count_normalized, count_lemmatized = preprocess_text(text)\n",
    "        for token in tokens:\n",
    "            inverted_index[token].add(doc_id)\n",
    "        \n",
    "        # Display token counts for the document\n",
    "        print(f\"Document: {doc_id}\")\n",
    "        print(f\"Tokens before preprocessing: {count_before}\")\n",
    "        print(f\"Tokens after stopword removal: {count_stopwords_removed}\")\n",
    "        print(f\"Tokens after normalization: {count_normalized}\")\n",
    "        print(f\"Tokens after lemmatization: {count_lemmatized}\\n\")\n",
    "    return inverted_index\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"C:\\\\Users\\\\lashreev\\\\Downloads\\\\Dataset\"  \n",
    "    documents = load_documents(dataset_path)\n",
    "    inverted_index = build_inverted_index(documents)\n",
    "\n",
    "    # Display a sample of the inverted index\n",
    "    print(\"Sample of Inverted Index:\")\n",
    "    for term, doc_ids in list(inverted_index.items())[:10]:\n",
    "        print(f\"{term}: {doc_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.     Implement dictionary compression techniques to estimate space savings and compare their efficiency. Techniques to be implemented include:\n",
    "·       Without blocking\n",
    "·       With blocking\n",
    "·       Blocking combined with front coding. (for both dictionary and posting lists)\n",
    "Analyze the space utilization of the compressed dictionary for each method and present the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lashreev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lashreev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lashreev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Nature_1.docx\n",
      "Tokens before preprocessing: 167\n",
      "Tokens after stopword removal: 110\n",
      "Tokens after normalization: 84\n",
      "Tokens after lemmatization: 84\n",
      "\n",
      "Document: Nature_10.pdf\n",
      "Tokens before preprocessing: 155\n",
      "Tokens after stopword removal: 99\n",
      "Tokens after normalization: 79\n",
      "Tokens after lemmatization: 79\n",
      "\n",
      "Document: Nature_2.pdf\n",
      "Tokens before preprocessing: 197\n",
      "Tokens after stopword removal: 114\n",
      "Tokens after normalization: 85\n",
      "Tokens after lemmatization: 85\n",
      "\n",
      "Document: Nature_3.txt\n",
      "Tokens before preprocessing: 144\n",
      "Tokens after stopword removal: 96\n",
      "Tokens after normalization: 63\n",
      "Tokens after lemmatization: 63\n",
      "\n",
      "Document: Nature_4.xlsx\n",
      "Tokens before preprocessing: 119\n",
      "Tokens after stopword removal: 74\n",
      "Tokens after normalization: 61\n",
      "Tokens after lemmatization: 61\n",
      "\n",
      "Document: Nature_5.docx\n",
      "Tokens before preprocessing: 85\n",
      "Tokens after stopword removal: 57\n",
      "Tokens after normalization: 46\n",
      "Tokens after lemmatization: 46\n",
      "\n",
      "Document: Nature_6.pdf\n",
      "Tokens before preprocessing: 171\n",
      "Tokens after stopword removal: 103\n",
      "Tokens after normalization: 81\n",
      "Tokens after lemmatization: 81\n",
      "\n",
      "Document: Nature_7.txt\n",
      "Tokens before preprocessing: 100\n",
      "Tokens after stopword removal: 62\n",
      "Tokens after normalization: 51\n",
      "Tokens after lemmatization: 51\n",
      "\n",
      "Document: Nature_8.xlsx\n",
      "Tokens before preprocessing: 147\n",
      "Tokens after stopword removal: 102\n",
      "Tokens after normalization: 76\n",
      "Tokens after lemmatization: 76\n",
      "\n",
      "Document: Nature_9.docx\n",
      "Tokens before preprocessing: 176\n",
      "Tokens after stopword removal: 121\n",
      "Tokens after normalization: 98\n",
      "Tokens after lemmatization: 98\n",
      "\n",
      "Space Utilization Analysis:\n",
      "Original Dictionary Size: 40696 bytes\n",
      "Compressed Without Blocking: 40696 bytes\n",
      "Compressed With Blocking: 5122 bytes\n",
      "Compressed With Front Coding: 39251 bytes\n",
      "\n",
      "Sample of Compressed Dictionary:\n",
      "100: 0\n",
      "1|80: 4\n",
      "0|20: 7\n",
      "0|300: 10\n",
      "0|4: 14\n",
      "0|540: 16\n",
      "0|66: 20\n",
      "0|750: 23\n",
      "ability: 27\n",
      "1|cademic: 35\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import struct\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define text extraction functions for various formats\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return \" \".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() if page.extract_text() else \"\"\n",
    "    return text\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_xlsx(file_path):\n",
    "    excel_data = pd.ExcelFile(file_path)\n",
    "    all_sheets_text = []\n",
    "    for sheet_name in excel_data.sheet_names:\n",
    "        df = excel_data.parse(sheet_name)\n",
    "        all_sheets_text.append(\" \".join(df.astype(str).stack()))\n",
    "    return \" \".join(all_sheets_text)\n",
    "\n",
    "# Load and process all files from a dataset directory\n",
    "def load_documents(dataset_path):\n",
    "    document_collection = {}\n",
    "    for root, _, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if file.endswith('.docx'):\n",
    "                document_collection[file] = read_docx(file_path)\n",
    "            elif file.endswith('.pdf'):\n",
    "                document_collection[file] = read_pdf(file_path)\n",
    "            elif file.endswith('.txt'):\n",
    "                document_collection[file] = read_txt(file_path)\n",
    "            elif file.endswith('.xlsx'):\n",
    "                document_collection[file] = read_xlsx(file_path)\n",
    "    return document_collection\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    token_count_before = len(tokens)\n",
    "\n",
    "    # Stopword Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    token_count_stopwords_removed = len(tokens)\n",
    "\n",
    "    # Normalization (lowercasing, removing punctuation)\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "    token_count_normalized = len(tokens)\n",
    "\n",
    "    # Stemming/Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    token_count_lemmatized = len(tokens)\n",
    "\n",
    "    return tokens, token_count_before, token_count_stopwords_removed, token_count_normalized, token_count_lemmatized\n",
    "\n",
    "# Build inverted index\n",
    "def build_inverted_index(documents):\n",
    "    inverted_index = defaultdict(set)\n",
    "    for doc_id, text in documents.items():\n",
    "        tokens, count_before, count_stopwords_removed, count_normalized, count_lemmatized = preprocess_text(text)\n",
    "        for token in tokens:\n",
    "            inverted_index[token].add(doc_id)\n",
    "        \n",
    "        # Display token counts for the document\n",
    "        print(f\"Document: {doc_id}\")\n",
    "        print(f\"Tokens before preprocessing: {count_before}\")\n",
    "        print(f\"Tokens after stopword removal: {count_stopwords_removed}\")\n",
    "        print(f\"Tokens after normalization: {count_normalized}\")\n",
    "        print(f\"Tokens after lemmatization: {count_lemmatized}\\n\")\n",
    "    return inverted_index\n",
    "\n",
    "# Compression without blocking\n",
    "def compress_without_blocking(dictionary):\n",
    "    compressed_dict = {}  # Store term and offset\n",
    "    current_offset = 0\n",
    "    for term in sorted(dictionary):\n",
    "        compressed_dict[term] = current_offset\n",
    "        current_offset += len(term) + 1  # +1 for null terminator\n",
    "    return compressed_dict\n",
    "\n",
    "# Compression with blocking\n",
    "def compress_with_blocking(dictionary, block_size):\n",
    "    compressed_dict = {}\n",
    "    current_offset = 0\n",
    "    block = []\n",
    "    block_start = None\n",
    "\n",
    "    for idx, term in enumerate(sorted(dictionary)):\n",
    "        if idx % block_size == 0:\n",
    "            block_start = term\n",
    "            block = []\n",
    "            compressed_dict[block_start] = current_offset\n",
    "        block.append(term)\n",
    "        current_offset += len(term) + 1\n",
    "\n",
    "    return compressed_dict\n",
    "\n",
    "# Compression with blocking and front coding\n",
    "def compress_with_front_coding(dictionary, block_size):\n",
    "    compressed_dict = {}\n",
    "    current_offset = 0\n",
    "    block = []\n",
    "    block_start = None\n",
    "\n",
    "    for idx, term in enumerate(sorted(dictionary)):\n",
    "        if idx % block_size == 0:\n",
    "            block_start = term\n",
    "            block = []\n",
    "            compressed_dict[block_start] = current_offset\n",
    "            prev_term = term\n",
    "            current_offset += len(term) + 1\n",
    "        else:\n",
    "            prefix_length = 0\n",
    "            while (prefix_length < len(prev_term) and prefix_length < len(term) and prev_term[prefix_length] == term[prefix_length]):\n",
    "                prefix_length += 1\n",
    "            compressed_dict[f\"{prefix_length}|{term[prefix_length:]}\"] = current_offset\n",
    "            current_offset += len(term) - prefix_length + 1\n",
    "            prev_term = term\n",
    "\n",
    "    return compressed_dict\n",
    "\n",
    "# Calculate dictionary size\n",
    "def calculate_size(dictionary):\n",
    "    return sum(sys.getsizeof(key) + sys.getsizeof(value) for key, value in dictionary.items())\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"C:\\\\Users\\\\lashreev\\\\Downloads\\\\Dataset\" \n",
    "    documents = load_documents(dataset_path)\n",
    "    inverted_index = build_inverted_index(documents)\n",
    "\n",
    "    # Compress the dictionary\n",
    "    dictionary = {term: idx for idx, term in enumerate(sorted(inverted_index.keys()))}\n",
    "\n",
    "    compressed_without_blocking = compress_without_blocking(dictionary)\n",
    "    compressed_with_blocking = compress_with_blocking(dictionary, block_size=8)\n",
    "    compressed_with_front_coding = compress_with_front_coding(dictionary, block_size=8)\n",
    "\n",
    "    # Analyze space savings\n",
    "    original_size = calculate_size(dictionary)\n",
    "    size_without_blocking = calculate_size(compressed_without_blocking)\n",
    "    size_with_blocking = calculate_size(compressed_with_blocking)\n",
    "    size_with_front_coding = calculate_size(compressed_with_front_coding)\n",
    "\n",
    "    print(\"Space Utilization Analysis:\")\n",
    "    print(f\"Original Dictionary Size: {original_size} bytes\")\n",
    "    print(f\"Compressed Without Blocking: {size_without_blocking} bytes\")\n",
    "    print(f\"Compressed With Blocking: {size_with_blocking} bytes\")\n",
    "    print(f\"Compressed With Front Coding: {size_with_front_coding} bytes\")\n",
    "\n",
    "    # Display a sample of the compressed dictionary\n",
    "    print(\"\\nSample of Compressed Dictionary:\")\n",
    "    for term, offset in list(compressed_with_front_coding.items())[:10]:\n",
    "        print(f\"{term}: {offset}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
